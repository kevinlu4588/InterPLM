{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get key from environment\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Initialize client\n",
    "client = Anthropic(api_key=api_key)\n",
    "\n",
    "# Send a simple prompt\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-7-sonnet-20250219\",\n",
    "    max_tokens=200,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Say hello\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, torch, os, gc\n",
    "from interplm.sae.inference import load_sae_from_hf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DEVICE=\"cuda\"\n",
    "DTYPE=torch.float16\n",
    "\n",
    "DATA_DIR = Path(\"esm_sae_results\"); DATA_DIR.mkdir(exist_ok=True)\n",
    "SEQUENCES_DIR = Path(\"/home/ec2-user/SageMaker/InterPLM/data/uniprot/subset_25k.csv\")\n",
    "# ANNOTATIONS_DIR = Path(\"uniprotkb_swissprot_annotations.tsv.gz\")\n",
    "ANNOTATIONS_DIR = Path(\"/home/ec2-user/SageMaker/InterPLM/uniprotkb_swissprot_annotations.tsv.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", do_lower_case=False)\n",
    "model     = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\",\n",
    "                                        output_hidden_states=True).to(DEVICE).eval()\n",
    "\n",
    "# Make sure the SAE you load matches the *plm_model* and *plm_layer* you want to use\n",
    "plm_model = \"esm2-650m\"   # matches your checkpoint naming\n",
    "plm_layer = 24            # <= MUST match esm_layer_sel\n",
    "sae = load_sae_from_hf(plm_model=plm_model, plm_layer=plm_layer).to(DEVICE).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "features_all = pd.read_pickle(\"features_all.pkl\")\n",
    "features_all.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>length</th>\n",
       "      <th>features</th>\n",
       "      <th>max_activation</th>\n",
       "      <th>n_active_features</th>\n",
       "      <th>reconstruction_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q9GL23</td>\n",
       "      <td>50</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002...</td>\n",
       "      <td>1.265625</td>\n",
       "      <td>1876</td>\n",
       "      <td>45.198380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6GZU6</td>\n",
       "      <td>50</td>\n",
       "      <td>[0.00023197175, 0.0, 0.0, 0.0, 0.0013056946, 0...</td>\n",
       "      <td>0.843262</td>\n",
       "      <td>2168</td>\n",
       "      <td>13.467114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P9WJG6</td>\n",
       "      <td>50</td>\n",
       "      <td>[0.0, 0.00057144166, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.935059</td>\n",
       "      <td>1740</td>\n",
       "      <td>12.720748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P18924</td>\n",
       "      <td>51</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...</td>\n",
       "      <td>0.956543</td>\n",
       "      <td>1799</td>\n",
       "      <td>11.394856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q08076</td>\n",
       "      <td>52</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...</td>\n",
       "      <td>1.139648</td>\n",
       "      <td>1772</td>\n",
       "      <td>24.694654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_id  length                                           features  \\\n",
       "0     Q9GL23      50  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002...   \n",
       "1     Q6GZU6      50  [0.00023197175, 0.0, 0.0, 0.0, 0.0013056946, 0...   \n",
       "2     P9WJG6      50  [0.0, 0.00057144166, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     P18924      51  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...   \n",
       "4     Q08076      52  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...   \n",
       "\n",
       "   max_activation  n_active_features  reconstruction_mse  \n",
       "0        1.265625               1876           45.198380  \n",
       "1        0.843262               2168           13.467114  \n",
       "2        0.935059               1740           12.720748  \n",
       "3        0.956543               1799           11.394856  \n",
       "4        1.139648               1772           24.694654  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.read_csv(ANNOTATIONS_DIR, sep=\"\\t\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Reviewed</th>\n",
       "      <th>Protein names</th>\n",
       "      <th>Length</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>EC number</th>\n",
       "      <th>Active site</th>\n",
       "      <th>Binding site</th>\n",
       "      <th>Cofactor</th>\n",
       "      <th>Disulfide bond</th>\n",
       "      <th>...</th>\n",
       "      <th>Helix</th>\n",
       "      <th>Turn</th>\n",
       "      <th>Beta strand</th>\n",
       "      <th>Coiled coil</th>\n",
       "      <th>Domain [CC]</th>\n",
       "      <th>Compositional bias</th>\n",
       "      <th>Domain [FT]</th>\n",
       "      <th>Motif</th>\n",
       "      <th>Region</th>\n",
       "      <th>Zinc finger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A009IHW8</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>2' cyclic ADP-D-ribose synthase AbTIR (2'cADPR...</td>\n",
       "      <td>269</td>\n",
       "      <td>MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...</td>\n",
       "      <td>3.2.2.-; 3.2.2.6</td>\n",
       "      <td>ACT_SITE 208; /evidence=\"ECO:0000255|PROSITE-P...</td>\n",
       "      <td>BINDING 143; /ligand=\"NAD(+)\"; /ligand_id=\"ChE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>HELIX 143..145; /evidence=\"ECO:0007829|PDB:7UW...</td>\n",
       "      <td>TURN 146..149; /evidence=\"ECO:0007829|PDB:7UWG...</td>\n",
       "      <td>STRAND 135..142; /evidence=\"ECO:0007829|PDB:7U...</td>\n",
       "      <td>COILED 31..99; /evidence=\"ECO:0000255\"</td>\n",
       "      <td>DOMAIN: The TIR domain mediates NAD(+) hydrola...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOMAIN 133..266; /note=\"TIR\"; /evidence=\"ECO:0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A023I7E1</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3...</td>\n",
       "      <td>796</td>\n",
       "      <td>MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...</td>\n",
       "      <td>3.2.1.39</td>\n",
       "      <td>ACT_SITE 500; /evidence=\"ECO:0000255|PROSITE-P...</td>\n",
       "      <td>BINDING 504; /ligand=\"(1,3-beta-D-glucosyl)n\";...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>HELIX 42..44; /evidence=\"ECO:0007829|PDB:4K35\"...</td>\n",
       "      <td>TURN 287..289; /evidence=\"ECO:0007829|PDB:4K35...</td>\n",
       "      <td>STRAND 56..58; /evidence=\"ECO:0007829|PDB:4K35...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOMAIN 31..759; /note=\"GH81\"; /evidence=\"ECO:0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGION 31..276; /note=\"beta-sandwich subdomain...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A024B7W1</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Genome polyprotein [Cleaved into: Capsid prote...</td>\n",
       "      <td>3423</td>\n",
       "      <td>MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...</td>\n",
       "      <td>2.1.1.56; 2.1.1.57; 2.7.7.48; 3.4.21.91; 3.6.1...</td>\n",
       "      <td>ACT_SITE 1553; /note=\"Charge relay system; for...</td>\n",
       "      <td>BINDING 1696..1703; /ligand=\"ATP\"; /ligand_id=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DISULFID 350..406; /evidence=\"ECO:0000250|UniP...</td>\n",
       "      <td>...</td>\n",
       "      <td>HELIX 222..225; /evidence=\"ECO:0007829|PDB:6CO...</td>\n",
       "      <td>TURN 237..241; /evidence=\"ECO:0007829|PDB:6CO8...</td>\n",
       "      <td>STRAND 234..236; /evidence=\"ECO:0007829|PDB:6C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOMAIN: [Small envelope protein M]: The transm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOMAIN 1503..1680; /note=\"Peptidase S7\"; /evid...</td>\n",
       "      <td>MOTIF 1787..1790; /note=\"DEAH box\"; /evidence=...</td>\n",
       "      <td>REGION 1..25; /note=\"Disordered\"; /evidence=\"E...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A024RXP8</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Exoglucanase 1 (EC 3.2.1.91) (1,4-beta-cellobi...</td>\n",
       "      <td>514</td>\n",
       "      <td>MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...</td>\n",
       "      <td>3.2.1.91</td>\n",
       "      <td>ACT_SITE 229; /note=\"Nucleophile\"; /evidence=\"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DISULFID 21..89; /evidence=\"ECO:0000250|UniPro...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOMAIN: The enzyme consists of two functional ...</td>\n",
       "      <td>COMPBIAS 401..437; /note=\"Polar residues\"; /ev...</td>\n",
       "      <td>DOMAIN 478..514; /note=\"CBM1\"; /evidence=\"ECO:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGION 18..453; /note=\"Catalytic\"; /evidence=\"...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A024SC78</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>Cutinase (EC 3.1.1.74)</td>\n",
       "      <td>248</td>\n",
       "      <td>MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...</td>\n",
       "      <td>3.1.1.74</td>\n",
       "      <td>ACT_SITE 164; /note=\"Nucleophile\"; /evidence=\"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DISULFID 55..91; /evidence=\"ECO:0000269|PubMed...</td>\n",
       "      <td>...</td>\n",
       "      <td>HELIX 51..69; /evidence=\"ECO:0007829|PDB:4PSC\"...</td>\n",
       "      <td>TURN 94..100; /evidence=\"ECO:0007829|PDB:4PSC\"...</td>\n",
       "      <td>STRAND 48..50; /evidence=\"ECO:0007829|PDB:4PSC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOMAIN: In contract to classical cutinases, po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGION 31..70; /note=\"Lid covering the active ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry  Reviewed                                      Protein names  \\\n",
       "0  A0A009IHW8  reviewed  2' cyclic ADP-D-ribose synthase AbTIR (2'cADPR...   \n",
       "1  A0A023I7E1  reviewed  Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3...   \n",
       "2  A0A024B7W1  reviewed  Genome polyprotein [Cleaved into: Capsid prote...   \n",
       "3  A0A024RXP8  reviewed  Exoglucanase 1 (EC 3.2.1.91) (1,4-beta-cellobi...   \n",
       "4  A0A024SC78  reviewed                             Cutinase (EC 3.1.1.74)   \n",
       "\n",
       "   Length                                           Sequence  \\\n",
       "0     269  MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...   \n",
       "1     796  MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...   \n",
       "2    3423  MKNPKKKSGGFRIVNMLKRGVARVSPFGGLKRLPAGLLLGHGPIRM...   \n",
       "3     514  MYRKLAVISAFLATARAQSACTLQSETHPPLTWQKCSSGGTCTQQT...   \n",
       "4     248  MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...   \n",
       "\n",
       "                                           EC number  \\\n",
       "0                                   3.2.2.-; 3.2.2.6   \n",
       "1                                           3.2.1.39   \n",
       "2  2.1.1.56; 2.1.1.57; 2.7.7.48; 3.4.21.91; 3.6.1...   \n",
       "3                                           3.2.1.91   \n",
       "4                                           3.1.1.74   \n",
       "\n",
       "                                         Active site  \\\n",
       "0  ACT_SITE 208; /evidence=\"ECO:0000255|PROSITE-P...   \n",
       "1  ACT_SITE 500; /evidence=\"ECO:0000255|PROSITE-P...   \n",
       "2  ACT_SITE 1553; /note=\"Charge relay system; for...   \n",
       "3  ACT_SITE 229; /note=\"Nucleophile\"; /evidence=\"...   \n",
       "4  ACT_SITE 164; /note=\"Nucleophile\"; /evidence=\"...   \n",
       "\n",
       "                                        Binding site Cofactor  \\\n",
       "0  BINDING 143; /ligand=\"NAD(+)\"; /ligand_id=\"ChE...      NaN   \n",
       "1  BINDING 504; /ligand=\"(1,3-beta-D-glucosyl)n\";...      NaN   \n",
       "2  BINDING 1696..1703; /ligand=\"ATP\"; /ligand_id=...      NaN   \n",
       "3                                                NaN      NaN   \n",
       "4                                                NaN      NaN   \n",
       "\n",
       "                                      Disulfide bond  ...  \\\n",
       "0                                                NaN  ...   \n",
       "1                                                NaN  ...   \n",
       "2  DISULFID 350..406; /evidence=\"ECO:0000250|UniP...  ...   \n",
       "3  DISULFID 21..89; /evidence=\"ECO:0000250|UniPro...  ...   \n",
       "4  DISULFID 55..91; /evidence=\"ECO:0000269|PubMed...  ...   \n",
       "\n",
       "                                               Helix  \\\n",
       "0  HELIX 143..145; /evidence=\"ECO:0007829|PDB:7UW...   \n",
       "1  HELIX 42..44; /evidence=\"ECO:0007829|PDB:4K35\"...   \n",
       "2  HELIX 222..225; /evidence=\"ECO:0007829|PDB:6CO...   \n",
       "3                                                NaN   \n",
       "4  HELIX 51..69; /evidence=\"ECO:0007829|PDB:4PSC\"...   \n",
       "\n",
       "                                                Turn  \\\n",
       "0  TURN 146..149; /evidence=\"ECO:0007829|PDB:7UWG...   \n",
       "1  TURN 287..289; /evidence=\"ECO:0007829|PDB:4K35...   \n",
       "2  TURN 237..241; /evidence=\"ECO:0007829|PDB:6CO8...   \n",
       "3                                                NaN   \n",
       "4  TURN 94..100; /evidence=\"ECO:0007829|PDB:4PSC\"...   \n",
       "\n",
       "                                         Beta strand  \\\n",
       "0  STRAND 135..142; /evidence=\"ECO:0007829|PDB:7U...   \n",
       "1  STRAND 56..58; /evidence=\"ECO:0007829|PDB:4K35...   \n",
       "2  STRAND 234..236; /evidence=\"ECO:0007829|PDB:6C...   \n",
       "3                                                NaN   \n",
       "4  STRAND 48..50; /evidence=\"ECO:0007829|PDB:4PSC...   \n",
       "\n",
       "                              Coiled coil  \\\n",
       "0  COILED 31..99; /evidence=\"ECO:0000255\"   \n",
       "1                                     NaN   \n",
       "2                                     NaN   \n",
       "3                                     NaN   \n",
       "4                                     NaN   \n",
       "\n",
       "                                         Domain [CC]  \\\n",
       "0  DOMAIN: The TIR domain mediates NAD(+) hydrola...   \n",
       "1                                                NaN   \n",
       "2  DOMAIN: [Small envelope protein M]: The transm...   \n",
       "3  DOMAIN: The enzyme consists of two functional ...   \n",
       "4  DOMAIN: In contract to classical cutinases, po...   \n",
       "\n",
       "                                  Compositional bias  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  COMPBIAS 401..437; /note=\"Polar residues\"; /ev...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         Domain [FT]  \\\n",
       "0  DOMAIN 133..266; /note=\"TIR\"; /evidence=\"ECO:0...   \n",
       "1  DOMAIN 31..759; /note=\"GH81\"; /evidence=\"ECO:0...   \n",
       "2  DOMAIN 1503..1680; /note=\"Peptidase S7\"; /evid...   \n",
       "3  DOMAIN 478..514; /note=\"CBM1\"; /evidence=\"ECO:...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               Motif  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  MOTIF 1787..1790; /note=\"DEAH box\"; /evidence=...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              Region Zinc finger  \n",
       "0                                                NaN         NaN  \n",
       "1  REGION 31..276; /note=\"beta-sandwich subdomain...         NaN  \n",
       "2  REGION 1..25; /note=\"Disordered\"; /evidence=\"E...         NaN  \n",
       "3  REGION 18..453; /note=\"Catalytic\"; /evidence=\"...         NaN  \n",
       "4  REGION 31..70; /note=\"Lid covering the active ...         NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import random\n",
    "\n",
    "# # Parameters\n",
    "# N_FEATURES = 1200\n",
    "# BINS = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "# # Randomly select feature ids\n",
    "# all_feature_ids = list(range(len(features_all.iloc[0].features)))\n",
    "# print(\"num features\", len(all_feature_ids))\n",
    "# selected_features = random.sample(all_feature_ids, N_FEATURES)\n",
    "\n",
    "# print(f\"Selected {len(selected_features)} features out of {len(all_feature_ids)}\")\n",
    "\n",
    "# # Build dataset for each feature\n",
    "# feature_datasets = {}\n",
    "\n",
    "# # Predefine bin labels\n",
    "# bin_labels = [f\"{BINS[i]:.1f}-{BINS[i+1]:.1f}\" for i in range(len(BINS)-1)]\n",
    "\n",
    "# for fid in selected_features:\n",
    "#     # Extract activations for this feature\n",
    "#     activations = [f[fid] for f in features_all[\"features\"]]\n",
    "#     df = pd.DataFrame({\n",
    "#         \"uniprot_id\": features_all[\"uniprot_id\"],\n",
    "#         \"activation\": activations\n",
    "#     })\n",
    "\n",
    "#     # Assign bins\n",
    "#     df[\"bin\"] = pd.cut(df[\"activation\"], bins=BINS, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "#     sampled = []\n",
    "\n",
    "#     # Sample proteins per bin\n",
    "#     for b in df[\"bin\"].dropna().unique():\n",
    "#         bin_df = df[df[\"bin\"] == b]\n",
    "#         n = 10 if b == \"0.9-1.0\" else 2\n",
    "#         sampled.extend(bin_df.sample(min(len(bin_df), n), random_state=42).to_dict(orient=\"records\"))\n",
    "\n",
    "#     # Add 10 random zero-activation proteins \n",
    "#     zero_df = df[df[\"activation\"] == 0.0]\n",
    "#     if len(zero_df) > 0:\n",
    "#         sampled.extend(zero_df.sample(min(len(zero_df), 10), random_state=42).to_dict(orient=\"records\"))\n",
    "\n",
    "#     # Merge with metadata from annotations_df\n",
    "#     sampled_df = pd.DataFrame(sampled)\n",
    "#     merged = sampled_df.merge(annotations_df, left_on=\"uniprot_id\", right_on=\"Entry\", how=\"left\")\n",
    "\n",
    "#     feature_datasets[fid] = merged\n",
    "\n",
    "# # Example feature dataset\n",
    "# example_fid = selected_features[0]\n",
    "# feature_datasets[example_fid].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original max activation (feature 0): 0.02611415\n",
      "Normalized max activation (feature 0): 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Stakc into [num_proteins, num_features]\n",
    "X = np.vstack(features_all['features'].values) #Shape (N, F)\n",
    "\n",
    "#Max activation per feature across all proteins\n",
    "max_per_feature = X.max(axis=0) # shape: (F,)\n",
    "eps = 1e-12\n",
    "max_safe = np.where(max_per_feature > 0, max_per_feature, eps)\n",
    "#Normalize\n",
    "X_norm = X / max_safe\n",
    "\n",
    "#Save back\n",
    "features_all = features_all.copy()\n",
    "features_all[\"features_norm\"] = [row for row in X_norm]\n",
    "print(\"Original max activation (feature 0):\", X[:,0].max())\n",
    "print(\"Normalized max activation (feature 0):\", X_norm[:,0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "N_FEATURES = 10240\n",
    "BINS = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "# Randomly select feature ids\n",
    "# all_feature_ids = list(range(len(features_all.iloc[0].features)))\n",
    "# print(\"num features\", len(all_feature_ids))\n",
    "# selected_features = random.sample(all_feature_ids, N_FEATURES)\n",
    "\n",
    "# print(f\"Selected {len(selected_features)} features out of {len(all_feature_ids)}\")\n",
    "\n",
    "# # Build dataset for each feature\n",
    "# feature_datasets = {}\n",
    "\n",
    "# # Predefine bin labels\n",
    "# bin_labels = [f\"{BINS[i]:.1f}-{BINS[i+1]:.1f}\" for i in range(len(BINS)-1)]\n",
    "\n",
    "# for fid in selected_features:\n",
    "#     # Extract activations for this feature\n",
    "#     activations = [f[fid] for f in features_all[\"features_norm\"]]\n",
    "#     df = pd.DataFrame({\n",
    "#         \"uniprot_id\": features_all[\"uniprot_id\"],\n",
    "#         \"activation\": activations\n",
    "#     })\n",
    "\n",
    "#     # Assign bins\n",
    "#     df[\"bin\"] = pd.cut(df[\"activation\"], bins=BINS, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "#     sampled = []\n",
    "\n",
    "#     # Sample proteins per bin\n",
    "#     for b in df[\"bin\"].dropna().unique():\n",
    "#         bin_df = df[df[\"bin\"] == b]\n",
    "#         n = 10 if b == \"0.9-1.0\" else 2\n",
    "#         sampled.extend(bin_df.sample(min(len(bin_df), n), random_state=42).to_dict(orient=\"records\"))\n",
    "\n",
    "#     # Add 10 random zero-activation proteins \n",
    "#     zero_df = df[df[\"activation\"] == 0.0]\n",
    "#     if len(zero_df) > 0:\n",
    "#         sampled.extend(zero_df.sample(min(len(zero_df), 10), random_state=42).to_dict(orient=\"records\"))\n",
    "\n",
    "#     # Merge with metadata from annotations_df\n",
    "#     sampled_df = pd.DataFrame(sampled)\n",
    "#     merged = sampled_df.merge(annotations_df, left_on=\"uniprot_id\", right_on=\"Entry\", how=\"left\")\n",
    "\n",
    "#     feature_datasets[fid] = merged\n",
    "\n",
    "# # Example feature dataset\n",
    "# example_fid = selected_features[0]\n",
    "# feature_datasets[example_fid].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Suppose feature_datasets is your dict of DataFrames\n",
    "# with open(\"feature_datasets.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(feature_datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"feature_datasets.pkl\", \"rb\") as f:\n",
    "    feature_datasets = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Amino Acid Indices and activations for better LLM annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import extract_esm_features_batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_sae_features(hidden_states: torch.Tensor, sae):\n",
    "    \"\"\"\n",
    "    Pass ESM hidden states through the Sparse Autoencoder (SAE).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    hidden_states : torch.Tensor\n",
    "        Shape [B, L, d] or [L, d].\n",
    "        - B = batch size (optional if unsqueezed)\n",
    "        - L = sequence length\n",
    "        - d = ESM embedding dimension (e.g., 1280 for esm2_t33_650M)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sae_features : torch.Tensor\n",
    "        Shape [B, L, F]\n",
    "        Sparse latent features per residue.\n",
    "        F = number of SAE dictionary atoms / features.\n",
    "\n",
    "    recon : torch.Tensor\n",
    "        Shape [B, L, d]\n",
    "        Reconstructed embeddings in token space.\n",
    "\n",
    "    error : torch.Tensor\n",
    "        Shape [B, L, d]\n",
    "        Residual = hidden_states - recon\n",
    "    \"\"\"\n",
    "    if hidden_states.dim() == 2:          # [L, d]\n",
    "        hidden_states = hidden_states.unsqueeze(0)  # → [1, L, d]\n",
    "    x = hidden_states.to(torch.float32)      # <- ensure fp32 for SAE\n",
    "\n",
    "    # SAE should have encode() and decode() that operate on last dimension\n",
    "    sae_features = sae.encode(x)     # [B, L, F]\n",
    "    recon        = sae.decode(sae_features)      # [B, L, d]\n",
    "    error        = hidden_states - recon         # [B, L, d]\n",
    "\n",
    "    return sae_features, recon, error\n",
    "\n",
    "#Config for extracting activated positions\n",
    "\n",
    "TOP_K = 8 # How many positions to record per protein\n",
    "MIN_ACT = 0.0 #Only consider positions with activation > MIN_ACT\n",
    "BATCH_SIZE = 16 #For ESM/SAE Inference\n",
    "\n",
    "\n",
    "def _batched(iterable, n):\n",
    "    \"\"\"Yield Successive n-sized chunks from iterable\n",
    "    \"\"\"\n",
    "    it = list(iterable)\n",
    "    for i in range(0, len(it), n):\n",
    "        yield it[i:i+n]\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Tuple, Optional, Literal\n",
    "\n",
    "TOP_K = 8\n",
    "MIN_ACT = 0.0\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def _batched(iterable, n):\n",
    "    it = list(iterable)\n",
    "    for i in range(0, len(it), n):\n",
    "        yield it[i:i+n]\n",
    "\n",
    "def _normalize_1d(\n",
    "    x: np.ndarray,\n",
    "    mode: Literal[\"seq_max\",\"feature_global_max\",\"zscore\",\"none\"] = \"seq_max\",\n",
    "    global_max: Optional[float] = None,\n",
    "    eps: float = 1e-8\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize a 1D activation vector x (valid positions only).\n",
    "    \"\"\"\n",
    "    if mode == \"none\":\n",
    "        return x.copy()\n",
    "\n",
    "    if mode == \"feature_global_max\":\n",
    "        if global_max is None or global_max <= eps:\n",
    "            # fallback to seq_max if global not available/safe\n",
    "            mode = \"seq_max\"\n",
    "        else:\n",
    "            return x / (global_max + eps)\n",
    "\n",
    "    if mode == \"seq_max\":\n",
    "        m = np.max(x) if x.size else 0.0\n",
    "        return x / (m + eps)\n",
    "\n",
    "    if mode == \"zscore\":\n",
    "        mu = float(np.mean(x)) if x.size else 0.0\n",
    "        sd = float(np.std(x)) if x.size else 0.0\n",
    "        return (x - mu) / (sd + eps)\n",
    "\n",
    "    # Fallback (shouldn't hit)\n",
    "    return x.copy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_activated_positions_for_feature(\n",
    "    fid: int,\n",
    "    seqs: List[str],\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    max_per_feature: Optional[np.ndarray] = None,\n",
    "    norm_mode: Literal[\"seq_max\",\"feature_global_max\",\"zscore\",\"none\"] = \"seq_max\",\n",
    "    top_k: int = TOP_K,\n",
    "    min_act: float = MIN_ACT,\n",
    "    device: str = \"cuda\"\n",
    ") -> Tuple[List[List[int]], List[List[str]], List[List[float]], List[List[float]]]:\n",
    "    \"\"\"\n",
    "    For a list of sequences and a single SAE feature id (fid),\n",
    "    return per-sequence top-K activated residue indices, AA identities,\n",
    "    normalized scores (per selected position), and raw scores.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_indices : List[List[int]]\n",
    "        Per-sequence Top-K indices (0-based).\n",
    "    all_aas : List[List[str]]\n",
    "        Per-sequence amino acids at those indices.\n",
    "    norm_vals : List[List[float]]\n",
    "        Per-sequence normalized activations aligned with Top-K order.\n",
    "    raw_vals : List[List[float]]\n",
    "        Per-sequence raw activations aligned with Top-K order.\n",
    "    \"\"\"\n",
    "    all_indices: List[List[int]] = []\n",
    "    all_aas: List[List[str]] = []\n",
    "    norm_vals: List[List[float]] = []\n",
    "    raw_vals: List[List[float]] = []\n",
    "\n",
    "    # Optional global max for this feature (for 'feature_global_max' mode)\n",
    "    global_max = None\n",
    "    if norm_mode == \"feature_global_max\" and max_per_feature is not None:\n",
    "        if 0 <= fid < len(max_per_feature):\n",
    "            gm = float(max_per_feature[fid])\n",
    "            global_max = gm if np.isfinite(gm) else None\n",
    "\n",
    "    for chunk in _batched(seqs, batch_size):\n",
    "        # ESM -> SAE: token representations and mask\n",
    "        token_reps, attn_mask = extract_esm_features_batch(\n",
    "            chunk, layer_sel=plm_layer, device=device, model=model, tokenizer=tokenizer\n",
    "        )  # token_reps: [B, L, H], attn_mask: [B, L] bool\n",
    "        sae_feats, _, _ = extract_sae_features(token_reps, sae)  # [B, L, F]\n",
    "\n",
    "        # Select feature channel -> [B, L] on CPU\n",
    "        feat_act = sae_feats[..., fid].float().cpu()\n",
    "        mask = attn_mask.cpu()  # [B, L] bool\n",
    "\n",
    "        for seq, act_row, m in zip(chunk, feat_act, mask):\n",
    "            L = int(m.sum().item())  # valid residues\n",
    "            act_valid = act_row[:L].numpy() if L > 0 else np.array([], dtype=np.float32)\n",
    "\n",
    "            # positions above threshold\n",
    "            valid_idx = np.where(act_valid > min_act)[0]\n",
    "            if valid_idx.size == 0:\n",
    "                all_indices.append([])\n",
    "                all_aas.append([])\n",
    "                norm_vals.append([])\n",
    "                raw_vals.append([])\n",
    "                continue\n",
    "\n",
    "            # sort by activation desc within valid subset and take top_k\n",
    "            order = np.argsort(-act_valid[valid_idx])\n",
    "            chosen_local = valid_idx[order[:top_k]].tolist()\n",
    "\n",
    "            # raw values for the chosen positions\n",
    "            chosen_raw = act_valid[chosen_local]\n",
    "\n",
    "            # normalization (done on the full valid slice, then gather chosen)\n",
    "            norm_full = _normalize_1d(\n",
    "                act_valid, mode=norm_mode, global_max=global_max\n",
    "            )\n",
    "            chosen_norm = norm_full[chosen_local]\n",
    "\n",
    "            # map to AAs (defensive indexing)\n",
    "            aas = [seq[i] if i < len(seq) else \"X\" for i in chosen_local]\n",
    "\n",
    "            all_indices.append(chosen_local)\n",
    "            all_aas.append(aas)\n",
    "            norm_vals.append([float(v) for v in chosen_norm])\n",
    "            raw_vals.append([float(v) for v in chosen_raw])\n",
    "\n",
    "        # free tensors early\n",
    "        del token_reps, sae_feats, feat_act, attn_mask\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return all_indices, all_aas, norm_vals, raw_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ray so this doesn't take an hour and a half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib, utils\n",
    "import ray_based_processing\n",
    "importlib.reload(utils)\n",
    "importlib.reload(ray_based_processing)\n",
    "\n",
    "from utils import _batched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate with feature_datasets dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-13 19:55:43,593\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid option keyword max_retries for actors. Valid ones are ['label_selector', 'accelerator_type', 'memory', 'name', 'num_cpus', 'num_gpus', 'object_store_memory', 'placement_group', 'placement_group_bundle_index', 'placement_group_capture_child_tasks', 'resources', 'runtime_env', 'scheduling_strategy', '_metadata', 'enable_task_events', '_labels', 'concurrency_groups', 'enable_tensor_transport', 'lifetime', 'max_concurrency', 'max_restarts', 'max_task_retries', 'max_pending_calls', 'namespace', 'get_if_exists', 'allow_out_of_order_execution'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 137\u001b[39m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_indices, all_aas, norm_vals, raw_vals\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Ray worker (Actor) to reuse model/tokenizer/sae across many features\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;129;43m@ray\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpus\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mFeatureWorker\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp_dtype\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplm_layer\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfrom\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mtqdm\u001b[39;49;00m\u001b[34;43;01m.\u001b[39;49;00m\u001b[34;43;01mauto\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# local import OK\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/ray/_private/worker.py:3290\u001b[39m, in \u001b[36m_make_remote\u001b[39m\u001b[34m(function_or_class, options)\u001b[39m\n\u001b[32m   3282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ray.remote_function.RemoteFunction(\n\u001b[32m   3283\u001b[39m         Language.PYTHON,\n\u001b[32m   3284\u001b[39m         function_or_class,\n\u001b[32m   3285\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3286\u001b[39m         options,\n\u001b[32m   3287\u001b[39m     )\n\u001b[32m   3289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.isclass(function_or_class):\n\u001b[32m-> \u001b[39m\u001b[32m3290\u001b[39m     \u001b[43mray_option_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_actor_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ray.actor._make_actor(function_or_class, options)\n\u001b[32m   3293\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3294\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe @ray.remote decorator must be applied to either a function or a class.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3295\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/ray/_common/ray_option_utils.py:346\u001b[39m, in \u001b[36mvalidate_actor_options\u001b[39m\u001b[34m(options, in_options)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m options.items():\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m actor_options:\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    347\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid option keyword \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for actors. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    348\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid ones are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(actor_options)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    349\u001b[39m         )\n\u001b[32m    350\u001b[39m     actor_options[k].validate(k, v)\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_options \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mconcurrency_groups\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m options:\n",
      "\u001b[31mValueError\u001b[39m: Invalid option keyword max_retries for actors. Valid ones are ['label_selector', 'accelerator_type', 'memory', 'name', 'num_cpus', 'num_gpus', 'object_store_memory', 'placement_group', 'placement_group_bundle_index', 'placement_group_capture_child_tasks', 'resources', 'runtime_env', 'scheduling_strategy', '_metadata', 'enable_task_events', '_labels', 'concurrency_groups', 'enable_tensor_transport', 'lifetime', 'max_concurrency', 'max_restarts', 'max_task_retries', 'max_pending_calls', 'namespace', 'get_if_exists', 'allow_out_of_order_execution']."
     ]
    }
   ],
   "source": [
    "# ray_feature_activation.py\n",
    "import os\n",
    "import ray\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Literal, Tuple\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from interplm.sae.inference import load_sae_from_hf\n",
    "\n",
    "# === Your existing helpers (import these from your utils if already defined) ===\n",
    "from utils import extract_esm_features_batch, _batched, _normalize_1d\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "DATA_DIR = Path(\"esm_sae_results\"); DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "os.environ.setdefault(\"HF_HOME\", str(DATA_DIR / \"hf_cache\"))\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", str(DATA_DIR / \"hf_cache\"))\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
    "\n",
    "# Ray init\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# GPU world size\n",
    "N_GPUS = int(ray.available_resources().get(\"GPU\", 0))\n",
    "WORLD_SIZE = max(1, min(8, N_GPUS))\n",
    "\n",
    "# ----------------------------\n",
    "# Save your feature_datasets to disk to avoid shipping a giant object\n",
    "# feature_datasets: Dict[int, pd.DataFrame] must already exist in memory\n",
    "# ----------------------------\n",
    "# Example:\n",
    "#   feature_datasets: Dict[int, pd.DataFrame]  # keys=fids; each df has at least a 'Sequence' column\n",
    "#   max_safe: Optional[np.ndarray]             # global max per feature if using \"feature_global_max\"\n",
    "#   BATCH_SIZE, TOP_K, MIN_ACT, NORM_MODE, ESM_LAYER_SEL defined\n",
    "\n",
    "FEATURE_PICKLE = DATA_DIR / \"feature_datasets.pkl\"\n",
    "META_PICKLE    = DATA_DIR / \"feature_meta.pkl\"\n",
    "ESM_LAYER_SEL=24\n",
    "# Write once here in the driver\n",
    "# (We store (fids list, shapes) separately if you’d like; for now we just persist the dict)\n",
    "pd.to_pickle(feature_datasets, FEATURE_PICKLE)\n",
    "pd.to_pickle(\n",
    "    dict(\n",
    "        all_fids=list(feature_datasets.keys()),\n",
    "        norm_mode=NORM_MODE,\n",
    "        top_k=TOP_K,\n",
    "        min_act=MIN_ACT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        plm_layer=(ESM_LAYER_SEL if isinstance(ESM_LAYER_SEL, int) else 24),\n",
    "        max_per_feature=np.asarray(max_safe) if max_safe is not None else None,\n",
    "    ),\n",
    "    META_PICKLE,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Core compute function (uses passed-in model/tokenizer/sae) — same logic you posted\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def _compute_activated_positions_for_feature_wrapped(\n",
    "    fid: int,\n",
    "    seqs: List[str],\n",
    "    *,\n",
    "    batch_size: int,\n",
    "    max_per_feature: Optional[np.ndarray] = None,\n",
    "    norm_mode: Literal[\"seq_max\", \"feature_global_max\", \"zscore\", \"none\"] = \"seq_max\",\n",
    "    top_k: int = 5,\n",
    "    min_act: float = 0.0,\n",
    "    device: str = \"cuda:0\",\n",
    "    plm_layer: int = 24,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    sae=None,\n",
    ") -> Tuple[List[List[int]], List[List[str]], List[List[float]], List[List[float]]]:\n",
    "    all_indices: List[List[int]] = []\n",
    "    all_aas: List[List[str]] = []\n",
    "    norm_vals: List[List[float]] = []\n",
    "    raw_vals: List[List[float]] = []\n",
    "\n",
    "    global_max = None\n",
    "    if norm_mode == \"feature_global_max\" and max_per_feature is not None:\n",
    "        if 0 <= fid < len(max_per_feature):\n",
    "            gm = float(max_per_feature[fid])\n",
    "            global_max = gm if np.isfinite(gm) else None\n",
    "\n",
    "    for chunk in _batched(seqs, batch_size):\n",
    "        token_reps, attn_mask = extract_esm_features_batch(\n",
    "            chunk,\n",
    "            layer_sel=plm_layer,\n",
    "            device=device,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        sae_feats, _, _ = extract_sae_features(token_reps, sae)  # [B, L, F]\n",
    "        feat_act = sae_feats[..., fid].float().cpu()\n",
    "        mask = attn_mask.cpu()\n",
    "\n",
    "        for seq, act_row, m in zip(chunk, feat_act, mask):\n",
    "            L = int(m.sum().item())\n",
    "            act_valid = act_row[:L].numpy() if L > 0 else np.array([], dtype=np.float32)\n",
    "\n",
    "            valid_idx = np.where(act_valid > min_act)[0]\n",
    "            if valid_idx.size == 0:\n",
    "                all_indices.append([])\n",
    "                all_aas.append([])\n",
    "                norm_vals.append([])\n",
    "                raw_vals.append([])\n",
    "                continue\n",
    "\n",
    "            order = np.argsort(-act_valid[valid_idx])\n",
    "            chosen_local = valid_idx[order[:top_k]].tolist()\n",
    "\n",
    "            chosen_raw = act_valid[chosen_local]\n",
    "            norm_full = _normalize_1d(act_valid, mode=norm_mode, global_max=global_max)\n",
    "            chosen_norm = norm_full[chosen_local]\n",
    "\n",
    "            aas = [seq[i] if i < len(seq) else \"X\" for i in chosen_local]\n",
    "\n",
    "            all_indices.append(chosen_local)\n",
    "            all_aas.append(aas)\n",
    "            norm_vals.append([float(v) for v in chosen_norm])\n",
    "            raw_vals.append([float(v) for v in chosen_raw])\n",
    "\n",
    "        # clean up\n",
    "        del token_reps, sae_feats, feat_act, attn_mask\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return all_indices, all_aas, norm_vals, raw_vals\n",
    "\n",
    "# ----------------------------\n",
    "# Ray worker (Actor) to reuse model/tokenizer/sae across many features\n",
    "# ----------------------------\n",
    "@ray.remote(num_gpus=1, num_cpus=0, max_retries=1)\n",
    "class FeatureWorker:\n",
    "    def __init__(self, amp_dtype: torch.dtype = torch.float16, plm_layer: int = 24):\n",
    "        from tqdm.auto import tqdm  # local import OK\n",
    "        self.tqdm = tqdm\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        self.dtype  = amp_dtype\n",
    "        self.plm_layer = plm_layer\n",
    "\n",
    "        # Load ESM\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", do_lower_case=False)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\",\n",
    "            output_hidden_states=True,\n",
    "            torch_dtype=self.dtype,\n",
    "            low_cpu_mem_usage=True,\n",
    "        ).to(self.device).eval()\n",
    "\n",
    "        # Load SAE\n",
    "        self.sae = load_sae_from_hf(plm_model=\"esm2-650m\", plm_layer=self.plm_layer).to(self.device).eval()\n",
    "\n",
    "    def process_shard(\n",
    "        self,\n",
    "        rank: int,\n",
    "        world_size: int,\n",
    "        feature_pickle_path: str,\n",
    "        meta_pickle_path: str,\n",
    "        out_prefix: str = \"feature_acts\",\n",
    "    ) -> List[str]:\n",
    "        # Load inputs\n",
    "        fd: Dict[int, pd.DataFrame] = pd.read_pickle(feature_pickle_path)\n",
    "        meta = pd.read_pickle(meta_pickle_path)\n",
    "\n",
    "        all_fids: List[int] = meta[\"all_fids\"]\n",
    "        norm_mode: str      = meta[\"norm_mode\"]\n",
    "        top_k: int          = meta[\"top_k\"]\n",
    "        min_act: float      = meta[\"min_act\"]\n",
    "        batch_size: int     = meta[\"batch_size\"]\n",
    "        plm_layer: int      = meta[\"plm_layer\"]\n",
    "        max_per_feature     = meta[\"max_per_feature\"]\n",
    "\n",
    "        # This worker’s slice of features\n",
    "        shard_fids = all_fids[rank::world_size]\n",
    "\n",
    "        paths = []\n",
    "        pbar = self.tqdm(total=len(shard_fids), desc=f\"GPU{rank}\", position=rank, leave=True)\n",
    "\n",
    "        for fid in shard_fids:\n",
    "            df = fd[fid].copy()\n",
    "            if \"Sequence\" not in df.columns:\n",
    "                raise KeyError(\"Expected a 'Sequence' column in feature_datasets[fid].\")\n",
    "\n",
    "            seqs = df[\"Sequence\"].astype(str).fillna(\"\").tolist()\n",
    "\n",
    "            idx_lists, aa_lists, norm_vals, raw_vals = _compute_activated_positions_for_feature_wrapped(\n",
    "                fid=fid,\n",
    "                seqs=seqs,\n",
    "                batch_size=batch_size,\n",
    "                max_per_feature=max_per_feature,\n",
    "                norm_mode=norm_mode,\n",
    "                top_k=top_k,\n",
    "                min_act=min_act,\n",
    "                device=str(self.device),\n",
    "                plm_layer=plm_layer,\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                sae=self.sae,\n",
    "            )\n",
    "\n",
    "            # Attach outputs\n",
    "            df[\"activated_indices\"] = idx_lists\n",
    "            df[\"activated_aas\"] = aa_lists\n",
    "            df[\"seq_max_activation_norm\"] = norm_vals\n",
    "            df[\"activated_indices_str\"] = df[\"activated_indices\"].apply(lambda xs: \",\".join(map(str, xs)) if xs else \"\")\n",
    "            df[\"activated_aas_str\"] = df[\"activated_aas\"].apply(lambda xs: \",\".join(xs) if xs else \"\")\n",
    "            df[\"seq_max_activation_norm_str\"] = df[\"seq_max_activation_norm\"].apply(\n",
    "                lambda xs: \",\".join(map(str, xs)) if xs else \"\"\n",
    "            )\n",
    "\n",
    "            # Persist per-fid (nice for resuming/merging)\n",
    "            out_path = DATA_DIR / f\"{out_prefix}_fid{fid}_rank{rank}.pkl\"\n",
    "            df.to_pickle(out_path)\n",
    "            paths.append(str(out_path))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # finish this worker shard bundle as well (optional consolidated file)\n",
    "        shard_bundle = DATA_DIR / f\"{out_prefix}_rank{rank}.final.pkl\"\n",
    "        pd.to_pickle({\"rank\": rank, \"paths\": paths}, shard_bundle)\n",
    "\n",
    "        pbar.close()\n",
    "        return [str(p) for p in paths] + [str(shard_bundle)]\n",
    "\n",
    "# ----------------------------\n",
    "# Kick off workers\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "futs = []\n",
    "for r in range(WORLD_SIZE):\n",
    "    actor = FeatureWorker.remote(amp_dtype=torch.float16, plm_layer=(ESM_LAYER_SEL if isinstance(ESM_LAYER_SEL, int) else 24))\n",
    "    futs.append(actor.process_shard.remote(\n",
    "        rank=r,\n",
    "        world_size=WORLD_SIZE,\n",
    "        feature_pickle_path=str(FEATURE_PICKLE),\n",
    "        meta_pickle_path=str(META_PICKLE),\n",
    "        out_prefix=\"feature_acts\",\n",
    "    ))\n",
    "\n",
    "out_paths_per_rank = ray.get(futs)\n",
    "flat_paths = [p for sub in out_paths_per_rank for p in sub]\n",
    "print(f\"[done] wrote {len(flat_paths)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose feature_datasets is your dict of DataFrames\n",
    "with open(\"feature_datasets_with_amino_acids.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# --- Setup (once per notebook) ---\n",
    "# pip install anthropic python-dotenv\n",
    "from anthropic import Anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os, time, json, math, textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "# Load .env (expects ANTHROPIC_API_KEY=...)\n",
    "load_dotenv()\n",
    "client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# --- Config ---\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20240620\"\n",
    "MAX_TOKENS = 800  # enough for description + summary\n",
    "TEMPERATURE = 0.0 # deterministic\n",
    "CHECKPOINT_EVERY = 50\n",
    "OUTPUT_PATH = \"claude_feature_annotations.csv\"\n",
    "\n",
    "# Columns to show Claude (customize as you like)\n",
    "# We'll include what exists; missing columns are auto-dropped\n",
    "PREFERRED_COLS = [\n",
    "    # keys/ids\n",
    "    \"uniprot_id\", \"Entry\", \"Protein names\",\n",
    "    # size/sequence shape\n",
    "    \"Length\",\n",
    "    # functional annotations\n",
    "    \"EC number\", \"Active site\", \"Binding site\", \"Cofactor\", \"Disulfide bond\",\n",
    "    \"Helix\", \"Turn\", \"Beta strand\", \"Coiled coil\",\n",
    "    \"Domain [CC]\", \"Compositional bias\", \"Domain [FT]\", \"Motif\", \"Region\", \"Zinc finger\",\n",
    "    # your per-feature fields\n",
    "    \"activation\", \"bin\",\n",
    "    # optional (only used if present)\n",
    "    \"activated_indices\", \"activated_aas\"\n",
    "]\n",
    "\n",
    "# Limit rows/cols so the table fits comfortably in context\n",
    "MAX_ROWS = 80   # you can raise/lower if you hit token limits\n",
    "TRUNCATE_STR_LEN = 120  # truncate long text fields so tables tay compact\n",
    "\n",
    "\n",
    "def _coerce_and_trim_cols(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Select existing columns, stringify, and truncate long strings so the table is compact.\"\"\"\n",
    "    use_cols = [c for c in cols if c in df.columns]\n",
    "    if not use_cols:\n",
    "        # Fallback: show whatever is available\n",
    "        use_cols = list(df.columns)\n",
    "\n",
    "    out = df[use_cols].copy()\n",
    "\n",
    "    # Coerce to string and truncate long values\n",
    "    for c in use_cols:\n",
    "        out[c] = out[c].astype(str).str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        out[c] = out[c].apply(lambda s: s[:TRUNCATE_STR_LEN] + \"…\" if len(s) > TRUNCATE_STR_LEN else s)\n",
    "\n",
    "    # Keep only first MAX_ROWS to control token usage\n",
    "    return out.head(MAX_ROWS)\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Generate description and summary\n",
    "Analyze this protein dataset to determine what predicts the ’Maximum activation value’ and ‘Amino acids of\n",
    "highest activated indices in protein’ columns. This description should be as concise as possible but sufficient to\n",
    "predict these two columns on held-out data given only the description and the rest of the protein metadata\n",
    "provided. The feature could be specific to a protein family, a structural motif, a sequence motif, a functional\n",
    "role, etc. These WILL be used to predict how much unseen proteins are activated by the feature so only\n",
    "highlight relevant factors for this.\n",
    "\n",
    "Focus on:\n",
    "• Properties of proteins from the metadata that are associated with high vs medium vs low activation.\n",
    "• Where in the protein sequence activation occurs (in relation to the protein sequence, length, structure,\n",
    "  or other properties)\n",
    "• What functional annotations (binding sites, domains, etc.) and amino acids are present at or near the\n",
    "  activated positions\n",
    "• This description that will be used to help predict missing activation values should start with:\n",
    "  “The activation patterns are characterized by:”\n",
    "\n",
    "Then, in 1 sentence, summarize what biological feature or pattern this neural network activation is detecting.\n",
    "This concise summary should start with “The feature activates on”.\n",
    "\n",
    "Protein record:\n",
    "{TABLE}\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(table_df: pd.DataFrame) -> str:\n",
    "    table_md = table_df.to_markdown(index=False)\n",
    "    return PROMPT_TEMPLATE.replace(\"{TABLE}\", table_md)\n",
    "\n",
    "# Configure once\n",
    "BEDROCK_REGION = \"us-east-1\"\n",
    "MODEL_ID = \"us.anthropic.claude-3-opus-20240229-v1:0\"  # change if you prefer another Claude on Bedrock\n",
    "\n",
    "_bedrock = boto3.client(\"bedrock-runtime\", region_name=BEDROCK_REGION)\n",
    "\n",
    "\n",
    "def call_claude(prompt: str) -> str:\n",
    "    \"\"\"Call Claude, return raw text.\"\"\"\n",
    "\n",
    "    #Build Bedrock/Anthropic messages pyalod\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\":\"text\", \"text\": prompt}]\n",
    "    }\n",
    "    ]\n",
    "\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"temperature\" : TEMPERATURE,\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    resp = _bedrock.invoke_model(\n",
    "        modelId=MODEL_ID,\n",
    "        body=json.dumps(body),\n",
    "        contentType = \"application/json\",\n",
    "        accept=\"application/json\",\n",
    "    )\n",
    "    payload = json.loads(resp[\"body\"].read())\n",
    "\n",
    "    #Concatenate all text content blocks(CLaude may return multiple)\n",
    "    text_parts = []\n",
    "    for part in payload.get(\"content\", []):\n",
    "        if part.get(\"type\") == \"text\":\n",
    "            text-parts.append(part.get(\"text\", \"\"))\n",
    "    return \"\".join(text_parts)\n",
    "\n",
    "def parse_description_and_summary(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Best-effort parse: extract the long description (must start with the required phrase)\n",
    "    and the one-sentence summary (starts with 'The feature activates on').\n",
    "    Falls back to raw if patterns aren’t found.\n",
    "    \"\"\"\n",
    "    desc = \"\"\n",
    "    summ = \"\"\n",
    "    lines = [l.strip() for l in text.splitlines() if l.strip()]\n",
    "\n",
    "    # Find the description block\n",
    "    start_idx = None\n",
    "    for i, l in enumerate(lines):\n",
    "        if l.lower().startswith(\"the activation patterns are characterized by:\"):\n",
    "            start_idx = i\n",
    "            break\n",
    "    if start_idx is not None:\n",
    "        # collect until we hit the summary or end\n",
    "        buff = []\n",
    "        for j in range(start_idx, len(lines)):\n",
    "            if lines[j].lower().startswith(\"the feature activates on\"):\n",
    "                break\n",
    "            buff.append(lines[j])\n",
    "        desc = \"\\n\".join(buff).strip()\n",
    "\n",
    "    # Find the one-sentence summary\n",
    "    for l in lines:\n",
    "        if l.lower().startswith(\"the feature activates on\"):\n",
    "            # keep first sentence\n",
    "            summ = l.split(\"\\n\")[0].strip()\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"description\": desc or \"\",\n",
    "        \"summary\": summ or \"\",\n",
    "        \"raw\": text.strip()\n",
    "    }\n",
    "\n",
    "# --- Main loop over feature datasets ---\n",
    "# Expects: feature_datasets: Dict[int, pd.DataFrame]\n",
    "results_rows = []\n",
    "\n",
    "processed = 0\n",
    "for fid, df in list(feature_datasets.items())[:3]:\n",
    "    # Build a compact table for the model\n",
    "    view = _coerce_and_trim_cols(df, PREFERRED_COLS)\n",
    "    prompt = build_prompt(view)\n",
    "\n",
    "    try:\n",
    "        text = call_claude(prompt)\n",
    "        parsed = parse_description_and_summary(text)\n",
    "    except Exception as e:\n",
    "        parsed = {\"description\": \"\", \"summary\": \"\", \"raw\": f\"[ERROR] {e}\"}\n",
    "\n",
    "    results_rows.append({\n",
    "        \"feature_id\": fid,\n",
    "        \"n_rows_shown\": len(view),\n",
    "        \"description\": parsed[\"description\"],\n",
    "        \"summary\": parsed[\"summary\"],\n",
    "        \"raw_response\": parsed[\"raw\"],\n",
    "    })\n",
    "\n",
    "    processed += 1\n",
    "    if processed % CHECKPOINT_EVERY == 0:\n",
    "        pd.DataFrame(results_rows).to_parquet(OUTPUT_PATH, index=False)\n",
    "        print(f\"[checkpoint] saved {processed} → {OUTPUT_PATH}\")\n",
    "\n",
    "# Final save\n",
    "df_results = pd.DataFrame(results_rows)\n",
    "df_results.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"[done] {len(df_results)} features → {OUTPUT_PATH}\")\n",
    "df_results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_coerce_and_trim_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m fid = \u001b[32m2167\u001b[39m\n\u001b[32m      5\u001b[39m df  = feature_datasets[fid]          \u001b[38;5;66;03m# <- this is a DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m view   = \u001b[43m_coerce_and_trim_cols\u001b[49m(df, PREFERRED_COLS)\n\u001b[32m      8\u001b[39m prompt = build_prompt(view)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name '_coerce_and_trim_cols' is not defined"
     ]
    }
   ],
   "source": [
    "results_rows = []\n",
    "processed = 0\n",
    "\n",
    "fid = 2167\n",
    "df  = feature_datasets[fid]          # <- this is a DataFrame\n",
    "\n",
    "view   = _coerce_and_trim_cols(df, PREFERRED_COLS)\n",
    "prompt = build_prompt(view)\n",
    "\n",
    "try:\n",
    "    text   = call_claude(prompt)\n",
    "    parsed = parse_description_and_summary(text)\n",
    "except Exception as e:\n",
    "    parsed = {\"description\": \"\", \"summary\": \"\", \"raw\": f\"[ERROR] {e}\"}\n",
    "\n",
    "results_rows.append({\n",
    "    \"feature_id\": fid,\n",
    "    \"n_rows_shown\": len(view),\n",
    "    \"description\": parsed[\"description\"],\n",
    "    \"summary\": parsed[\"summary\"],\n",
    "    \"raw_response\": parsed[\"raw\"],\n",
    "})\n",
    "\n",
    "# Save\n",
    "df_results = pd.DataFrame(results_rows)\n",
    "df_results.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"[done] {len(df_results)} features → {OUTPUT_PATH}\")\n",
    "display(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_datasets: Dict[int, pd.DataFrame] where each df includes 'Sequence'\n",
    "from tqdm import tqdm\n",
    "for fid, df in tqdm(list(feature_datasets.items()), total=len(feature_datasets), desc=\"Features\"):\n",
    "    #make a copy to avoid mutating original reference\n",
    "    work = df.copy()\n",
    "\n",
    "    # Ensure sequences are present and aligned\n",
    "    if \"Sequence\" not in work.columns:\n",
    "        # If your UniProt column is named differently, adjust here\n",
    "        raise KeyError(\"Expected a 'Sequence' column in merged annotations.\")\n",
    "    seqs = work[\"Sequence\"].astype(str).fillna(\"\").tolist()\n",
    "\n",
    "    idx_lists, aa_lists, norm_vals, raw_vals = compute_activated_positions_for_feature(fid, seqs, batch_size=BATCH_SIZE, max_per_feature = max_safe)\n",
    "\n",
    "    #compute per-row activated positions for this feature\n",
    "    work['activated_indices'] = idx_lists\n",
    "    work['activated_aas'] = aa_lists\n",
    "    work['seq_max_activation_norm'] = norm_vals\n",
    "\n",
    "    # # (Optional) also add a compact string column for the prompt table\n",
    "    work[\"activated_indices_str\"] = work[\"activated_indices\"].apply(lambda xs: \",\".join(map(str, xs)) if xs else \"\")\n",
    "    work[\"activated_aas_str\"]     = work[\"activated_aas\"].apply(lambda xs: \",\".join(xs) if xs else \"\")\n",
    "    work[\"seq_max_activation_norm_str\"] = work[\"seq_max_activation_norm\"].apply(lambda xs: \",\".join(map(str, xs)) if xs else \"\")\n",
    "\n",
    "    feature_datasets[fid] = work\n",
    "\n",
    "first_fid = list(feature_datasets.keys())[0]\n",
    "feature_datasets[first_fid][[\"uniprot_id\",\"activation\",\"seq_max_activation_norm\", \"activated_indices_str\",\"activated_aas_str\", \"seq_max_activation_norm_str\"]].head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"  # Or your preferred region\n",
    ")\n",
    "\n",
    "# Prepare the request body\n",
    "body = json.dumps({\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 256,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Hello, world\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Use the US cross-region inference profile\n",
    "response = bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=\"us.anthropic.claude-3-opus-20240229-v1:0\",  # Note the \"us.\" prefix\n",
    "    contentType=\"application/json\",\n",
    "    accept=\"application/json\"\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "# Extract the text from the response\n",
    "if \"content\" in response_body:\n",
    "    for content in response_body[\"content\"]:\n",
    "        if content[\"type\"] == \"text\":\n",
    "            print(content[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 ('interplm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1106d1d489397abf5d77132595a521cf67d890f951d991cd34215b053d2a27e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
