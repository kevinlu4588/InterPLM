{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale SAE Analysis with UniProt Dataset\n",
    "\n",
    "This notebook demonstrates how to analyze sparse autoencoders (SAEs) using a larger UniProt dataset,\n",
    "following the InterPLM recommendations. We'll download Swiss-Prot data, create annotations, and mine SAE features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interplm.sae.inference import load_sae_from_hf\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from typing import List, Literal, Tuple\n",
    "@torch.no_grad()\n",
    "def extract_sae_features(hidden_states: torch.Tensor, sae):\n",
    "    \"\"\"\n",
    "    Pass ESM hidden states through the Sparse Autoencoder (SAE).\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    hidden_states : torch.Tensor\n",
    "        Shape [B, L, d] or [L, d].\n",
    "        - B = batch size (optional if unsqueezed)\n",
    "        - L = sequence length\n",
    "        - d = ESM embedding dimension (e.g., 1280 for esm2_t33_650M)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sae_features : torch.Tensor\n",
    "        Shape [B, L, F]\n",
    "        Sparse latent features per residue.\n",
    "        F = number of SAE dictionary atoms / features.\n",
    "\n",
    "    recon : torch.Tensor\n",
    "        Shape [B, L, d]\n",
    "        Reconstructed embeddings in token space.\n",
    "\n",
    "    error : torch.Tensor\n",
    "        Shape [B, L, d]\n",
    "        Residual = hidden_states - recon\n",
    "    \"\"\"\n",
    "    if hidden_states.dim() == 2:          # [L, d]\n",
    "        hidden_states = hidden_states.unsqueeze(0)  # → [1, L, d]\n",
    "    x = hidden_states.to(torch.float32)      # <- ensure fp32 for SAE\n",
    "\n",
    "    # SAE should have encode() and decode() that operate on last dimension\n",
    "    sae_features = sae.encode(x)     # [B, L, F]\n",
    "    recon        = sae.decode(sae_features)      # [B, L, d]\n",
    "    error        = hidden_states - recon         # [B, L, d]\n",
    "\n",
    "    return sae_features, recon, error\n",
    "\n",
    "def pool_sequence_features(\n",
    "    features: torch.Tensor,   # [B, L, F] or [L, F]\n",
    "    method: str = \"max_mean\",\n",
    "    mask: torch.Tensor = None # optional [B, L] attention mask\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pool per-residue features to per-sequence vectors.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features : torch.Tensor\n",
    "        Shape [B, L, F] (or [L, F] → will unsqueeze to batch 1).\n",
    "        - B = batch size\n",
    "        - L = sequence length\n",
    "        - F = number of SAE features\n",
    "    method : str\n",
    "        \"max_mean\" → concatenate mean + max → [B, 2F]\n",
    "        \"mean\"     → masked mean → [B, F]\n",
    "        \"max\"      → masked max  → [B, F]\n",
    "    mask : torch.Tensor, optional\n",
    "        Shape [B, L] bool (True = valid residue, False = pad).\n",
    "        If None, assumes all tokens are valid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pooled : torch.Tensor\n",
    "        Shape depends on method:\n",
    "          - max_mean: [B, 2F]\n",
    "          - mean or max: [B, F]\n",
    "    \"\"\"\n",
    "    if features.dim() == 2:  # [L, F]\n",
    "        features = features.unsqueeze(0)  # [1, L, F]\n",
    "\n",
    "    B, L, F = features.shape\n",
    "    if mask is None:\n",
    "        mask = torch.ones(B, L, dtype=torch.bool, device=features.device)\n",
    "\n",
    "    # apply mask\n",
    "    mask_f = mask.float().unsqueeze(-1)  # [B, L, 1]\n",
    "    feats_masked = features * mask_f\n",
    "\n",
    "    if method == \"mean\":\n",
    "        pooled = feats_masked.sum(1) / mask_f.sum(1).clamp_min(1e-8)\n",
    "        return pooled  # [B, F]\n",
    "\n",
    "    elif method == \"max\":\n",
    "        very_neg = torch.finfo(features.dtype).min\n",
    "        feats_masked = feats_masked.masked_fill(~mask.unsqueeze(-1), very_neg)\n",
    "        return feats_masked.max(1).values  # [B, F]\n",
    "\n",
    "    elif method == \"max_mean\":\n",
    "        mean_pool = feats_masked.sum(1) / mask_f.sum(1).clamp_min(1e-8)  # [B, F]\n",
    "        very_neg = torch.finfo(features.dtype).min\n",
    "        feats_masked = feats_masked.masked_fill(~mask.unsqueeze(-1), very_neg)\n",
    "        max_pool = feats_masked.max(1).values  # [B, F]\n",
    "        return torch.cat([mean_pool, max_pool], dim=-1)  # [B, 2F]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling method: {method}\")\n",
    "        \n",
    "@torch.no_grad()\n",
    "def extract_esm_features_batch(\n",
    "    sequences: List[str],\n",
    "    layer_sel: int | Literal[\"last\"] = \"last\",   # <— changed name/type\n",
    "    device: torch.device = DEVICE,\n",
    "    dtype = torch.float16,\n",
    "    model = None,\n",
    "    tokenizer = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    batch = tokenizer(sequences, return_tensors=\"pt\", add_special_tokens=False, padding=True)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    attn_mask = batch[\"attention_mask\"].to(torch.bool)\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "        out = model(**batch, output_hidden_states=True, return_dict=True)\n",
    "        hs = out.hidden_states  # tuple: [emb, layer1, ..., layerN] each [B,L,d]\n",
    "        if layer_sel == \"last\":\n",
    "            token_reps = hs[-1]\n",
    "        elif isinstance(layer_sel, int):\n",
    "            # ESM layers are 1-indexed in HF hidden_states after the embedding; adjust if you stored 0/1-based\n",
    "            token_reps = hs[layer_sel]  # e.g., 24th encoder block reps\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid layer_sel: {layer_sel}\")\n",
    "    return token_reps, attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _to_cpu_and_free(*tensors):\n",
    "    out = []\n",
    "    for t in tensors:\n",
    "        if t is None:\n",
    "            out.append(None)\n",
    "            continue\n",
    "        out.append(t.detach().to(\"cpu\", non_blocking=True))\n",
    "        del t\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return out if len(out) > 1 else out[0]\n",
    "\n",
    "def process_sequences_bucketed(\n",
    "    sequences_df: pd.DataFrame,\n",
    "    batch_size: int = 16,\n",
    "    save_every: int = 1000,\n",
    "    cache_name: str = \"sae_features\",\n",
    "    desc: str = \"processing\",\n",
    "    esm_layer_sel: int | Literal[\"last\"] = \"last\",   # <— renamed\n",
    "    model=None,\n",
    "    sae=None,\n",
    "    tokenizer=None,\n",
    "    device: torch.device = DEVICE,\n",
    "    amp_dtype = DTYPE,\n",
    ") -> pd.DataFrame:\n",
    "    model.eval()\n",
    "    DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    cache_dir = DATA_DIR / f\"{cache_name}_cache\"\n",
    "    cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    df = sequences_df.copy()\n",
    "    df[\"L\"] = df[\"sequence\"].str.len()\n",
    "    df = df.sort_values(\"L\").reset_index(drop=True)\n",
    "\n",
    "    results, processed, next_save_threshold = [], 0, save_every\n",
    "    pbar = tqdm(total=len(df), desc=desc, dynamic_ncols=True)\n",
    "\n",
    "    cur_bs, i = batch_size, 0\n",
    "    while i < len(df):\n",
    "        window = df.iloc[i : min(i + cur_bs, len(df))]\n",
    "        seqs = window[\"sequence\"].tolist()\n",
    "        ids  = window[\"uniprot_id\"].tolist()\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                token_reps, attn_mask = extract_esm_features_batch(\n",
    "                    seqs, layer_sel=esm_layer_sel, device=device, dtype=amp_dtype,\n",
    "                    model=model, tokenizer=tokenizer\n",
    "                )\n",
    "                sae_feats, recon, err = extract_sae_features(token_reps, sae)\n",
    "\n",
    "                pooled = pool_sequence_features(sae_feats, mask=attn_mask, method=\"mean\")  # [B,F]\n",
    "                pooled, sae_feats, err = _to_cpu_and_free(pooled, sae_feats, err)\n",
    "\n",
    "            for j, uid in enumerate(ids):\n",
    "                results.append({\n",
    "                    \"uniprot_id\": uid,\n",
    "                    \"length\": int(len(seqs[j])),\n",
    "                    \"features\": pooled[j].numpy(),                # [F]\n",
    "                    \"max_activation\": sae_feats[j].amax().item(),\n",
    "                    \"n_active_features\": (sae_feats[j] > 0.1).sum().item(),\n",
    "                    \"reconstruction_mse\": err[j].pow(2).mean().item(),\n",
    "                })\n",
    "\n",
    "            processed += len(window); pbar.update(len(window)); i += len(window)\n",
    "            if processed >= next_save_threshold or processed == len(df):\n",
    "                cache_file = cache_dir / f\"{cache_name}_{processed}.pkl\"\n",
    "                pd.DataFrame(results).to_pickle(cache_file)\n",
    "                print(f\"[checkpoint] saved {processed} → {cache_file}\")\n",
    "                next_save_threshold += save_every\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache(); gc.collect()\n",
    "                if cur_bs > 1:\n",
    "                    cur_bs = max(1, cur_bs // 2)\n",
    "                    print(f\"[OOM] reducing batch_size → {cur_bs} and retrying\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"[OOM] Skipping {ids[0]} (len={len(seqs[0])})\")\n",
    "                    i += 1; pbar.update(1); continue\n",
    "            else:\n",
    "                print(f\"[ERROR] {ids[0]}: {e}\")\n",
    "                i += 1; pbar.update(1); continue\n",
    "\n",
    "    pbar.close()\n",
    "    out_df = pd.DataFrame(results)\n",
    "    out_path = DATA_DIR / f\"{cache_name}.pkl\"\n",
    "    out_df.to_pickle(out_path)\n",
    "    print(f\"[done] {len(out_df)} sequences → {out_path}\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 23:04:23,729\tINFO worker.py:1951 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import ray, os, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "from interplm.sae.inference import load_sae_from_hf\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, num_gpus=8)\n",
    "\n",
    "DATA_DIR = Path(\"esm_sae_results\"); DATA_DIR.mkdir(exist_ok=True)\n",
    "sequences_df.to_pickle(DATA_DIR / \"sequences_df.pkl\")\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def ray_worker(rank: int, world_size: int, input_path: str, out_name: str,\n",
    "               batch_size: int = 16, save_every: int = 1000,\n",
    "               esm_layer_sel: int | Literal[\"last\"] = 24):\n",
    "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:128\")\n",
    "\n",
    "    # Each Ray task sees 1 GPU as cuda:0\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "    DTYPE  = torch.float16\n",
    "\n",
    "    # Load ESM and SAE inside the worker\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", do_lower_case=False)\n",
    "    model     = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\",\n",
    "                                          output_hidden_states=True).to(DEVICE).eval()\n",
    "\n",
    "    # Make sure the SAE you load matches the *plm_model* and *plm_layer* you want to use\n",
    "    plm_model = \"esm2-650m\"   # matches your checkpoint naming\n",
    "    plm_layer = 24            # <= MUST match esm_layer_sel\n",
    "    sae = load_sae_from_hf(plm_model=plm_model, plm_layer=plm_layer).to(DEVICE).eval()\n",
    "\n",
    "    df = pd.read_pickle(input_path)\n",
    "    shard = df.iloc[rank::world_size].reset_index(drop=True)\n",
    "\n",
    "    cache_name = f\"{out_name}_rank{rank}\"\n",
    "    out_df = process_sequences_bucketed(\n",
    "        sequences_df=shard,\n",
    "        batch_size=batch_size,\n",
    "        save_every=save_every,\n",
    "        cache_name=cache_name,\n",
    "        desc=f\"rank{rank}\",\n",
    "        esm_layer_sel=esm_layer_sel,\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        tokenizer=tokenizer,\n",
    "        device=DEVICE,\n",
    "        amp_dtype=DTYPE,\n",
    "    )\n",
    "\n",
    "    out_path = DATA_DIR / f\"{cache_name}.final.pkl\"\n",
    "    out_df.to_pickle(out_path)\n",
    "    return str(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1603)\u001b[0m Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "\u001b[36m(ray_worker pid=1603)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "rank1:   0%|          | 0/5000 [00:00<?, ?it/s]\n",
      "rank1:   0%|          | 16/5000 [00:00<03:08, 26.38it/s]\n",
      "\u001b[36m(ray_worker pid=1602)\u001b[0m Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(ray_worker pid=1602)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "rank5:   0%|          | 0/5000 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "rank1:   6%|▋         | 320/5000 [00:05<01:18, 59.35it/s]\u001b[32m [repeated 129x across cluster]\u001b[0m\n",
      "rank2:  10%|▉         | 496/5000 [00:10<01:38, 45.96it/s]\u001b[32m [repeated 123x across cluster]\u001b[0m\n",
      "rank0:  15%|█▍        | 736/5000 [00:15<01:34, 45.28it/s]\u001b[32m [repeated 116x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1603)\u001b[0m [checkpoint] saved 1008 → esm_sae_results/sae_features_rank1_cache/sae_features_rank1_1008.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank1:  23%|██▎       | 1136/5000 [00:20<01:11, 53.93it/s]\u001b[32m [repeated 115x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1608)\u001b[0m [checkpoint] saved 1008 → esm_sae_results/sae_features_rank7_cache/sae_features_rank7_1008.pkl\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank3:  23%|██▎       | 1136/5000 [00:25<01:34, 40.76it/s]\u001b[32m [repeated 109x across cluster]\u001b[0m\n",
      "rank1:  33%|███▎      | 1632/5000 [00:30<01:14, 45.33it/s]\u001b[32m [repeated 102x across cluster]\u001b[0m\n",
      "rank0:  31%|███▏      | 1568/5000 [00:35<01:32, 36.94it/s]\u001b[32m [repeated 95x across cluster]\u001b[0m\n",
      "rank5:  35%|███▍      | 1728/5000 [00:40<01:34, 34.65it/s]\u001b[32m [repeated 90x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1603)\u001b[0m [checkpoint] saved 2000 → esm_sae_results/sae_features_rank1_cache/sae_features_rank1_2000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank0:  38%|███▊      | 1920/5000 [00:45<01:31, 33.63it/s]\u001b[32m [repeated 87x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1605)\u001b[0m [checkpoint] saved 2000 → esm_sae_results/sae_features_rank6_cache/sae_features_rank6_2000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank5:  41%|████▏     | 2064/5000 [00:50<01:30, 32.36it/s]\u001b[32m [repeated 82x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1608)\u001b[0m [checkpoint] saved 2000 → esm_sae_results/sae_features_rank7_cache/sae_features_rank7_2000.pkl\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank2:  44%|████▍     | 2208/5000 [00:56<01:26, 32.36it/s]\u001b[32m [repeated 82x across cluster]\u001b[0m\n",
      "rank2:  47%|████▋     | 2368/5000 [01:01<01:23, 31.39it/s]\u001b[32m [repeated 79x across cluster]\u001b[0m\n",
      "rank1:  55%|█████▍    | 2736/5000 [01:06<01:21, 27.76it/s]\u001b[32m [repeated 75x across cluster]\u001b[0m\n",
      "rank0:  54%|█████▍    | 2704/5000 [01:11<01:18, 29.19it/s]\u001b[32m [repeated 74x across cluster]\u001b[0m\n",
      "rank7:  54%|█████▍    | 2720/5000 [01:16<01:24, 27.07it/s]\u001b[32m [repeated 70x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1603)\u001b[0m [checkpoint] saved 3008 → esm_sae_results/sae_features_rank1_cache/sae_features_rank1_3008.pkl\n",
      "\u001b[36m(ray_worker pid=1599)\u001b[0m [checkpoint] saved 3008 → esm_sae_results/sae_features_rank4_cache/sae_features_rank4_3008.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank1:  63%|██████▎   | 3136/5000 [01:21<01:12, 25.85it/s]\u001b[32m [repeated 69x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1597)\u001b[0m [checkpoint] saved 3008 → esm_sae_results/sae_features_rank0_cache/sae_features_rank0_3008.pkl\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank0:  62%|██████▏   | 3120/5000 [01:26<01:08, 27.56it/s]\u001b[32m [repeated 69x across cluster]\u001b[0m\n",
      "rank3:  64%|██████▍   | 3216/5000 [01:31<01:01, 28.99it/s]\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
      "rank3:  67%|██████▋   | 3360/5000 [01:36<01:01, 26.63it/s]\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
      "rank3:  69%|██████▉   | 3472/5000 [01:41<01:06, 23.14it/s]\u001b[32m [repeated 59x across cluster]\u001b[0m\n",
      "rank4:  76%|███████▌  | 3808/5000 [01:46<00:53, 22.18it/s]\u001b[32m [repeated 53x across cluster]\u001b[0m\n",
      "rank4:  78%|███████▊  | 3920/5000 [01:51<00:45, 23.70it/s]\u001b[32m [repeated 46x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1599)\u001b[0m [checkpoint] saved 4000 → esm_sae_results/sae_features_rank4_cache/sae_features_rank4_4000.pkl\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank1:  77%|███████▋  | 3840/5000 [01:56<01:06, 17.45it/s]\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
      "rank3:  78%|███████▊  | 3888/5000 [02:01<00:57, 19.49it/s]\u001b[32m [repeated 49x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1605)\u001b[0m [checkpoint] saved 4000 → esm_sae_results/sae_features_rank6_cache/sae_features_rank6_4000.pkl\n",
      "\u001b[36m(ray_worker pid=1603)\u001b[0m [checkpoint] saved 4000 → esm_sae_results/sae_features_rank1_cache/sae_features_rank1_4000.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank3:  80%|███████▉  | 3984/5000 [02:06<00:53, 19.12it/s]\u001b[32m [repeated 44x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1600)\u001b[0m [checkpoint] saved 4000 → esm_sae_results/sae_features_rank2_cache/sae_features_rank2_4000.pkl\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank1:  82%|████████▏ | 4096/5000 [02:11<00:54, 16.49it/s]\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "rank3:  84%|████████▎ | 4176/5000 [02:17<00:44, 18.62it/s]\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
      "rank3:  85%|████████▌ | 4272/5000 [02:22<00:40, 18.20it/s]\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
      "rank4:  92%|█████████▏| 4576/5000 [02:25<00:26, 16.24it/s]\n",
      "rank4:  92%|█████████▏| 4592/5000 [02:26<00:25, 16.08it/s]\n",
      "rank0:  86%|████████▌ | 4288/5000 [02:27<00:43, 16.39it/s]\u001b[32m [repeated 41x across cluster]\u001b[0m\n",
      "rank4:  92%|█████████▏| 4608/5000 [02:27<00:24, 15.91it/s]\n",
      "rank4:  92%|█████████▏| 4624/5000 [02:28<00:23, 15.76it/s]\n",
      "rank4:  93%|█████████▎| 4640/5000 [02:29<00:23, 15.63it/s]\n",
      "rank4:  93%|█████████▎| 4656/5000 [02:30<00:22, 15.42it/s]\n",
      "rank4:  93%|█████████▎| 4672/5000 [02:32<00:21, 15.16it/s]\n",
      "rank3:  89%|████████▉ | 4448/5000 [02:32<00:32, 16.91it/s]\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
      "rank4:  94%|█████████▍| 4688/5000 [02:33<00:20, 15.05it/s]\n",
      "rank6:  92%|█████████▏| 4576/5000 [02:33<00:24, 17.54it/s]\n",
      "rank4:  94%|█████████▍| 4704/5000 [02:34<00:19, 14.90it/s]\n",
      "rank6:  92%|█████████▏| 4592/5000 [02:34<00:23, 17.32it/s]\n",
      "rank6:  92%|█████████▏| 4608/5000 [02:35<00:23, 17.02it/s]\n",
      "rank4:  94%|█████████▍| 4720/5000 [02:35<00:19, 14.72it/s]\n",
      "rank6:  92%|█████████▏| 4624/5000 [02:36<00:23, 16.19it/s]\n",
      "rank4:  95%|█████████▍| 4736/5000 [02:36<00:18, 14.56it/s]\n",
      "rank6:  93%|█████████▎| 4640/5000 [02:37<00:22, 16.28it/s]\n",
      "rank2:  89%|████████▊ | 4432/5000 [02:37<00:36, 15.72it/s]\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
      "rank4:  95%|█████████▌| 4752/5000 [02:37<00:17, 14.39it/s]\n",
      "rank6:  93%|█████████▎| 4656/5000 [02:38<00:22, 15.53it/s]\n",
      "rank4:  95%|█████████▌| 4768/5000 [02:38<00:16, 14.12it/s]\n",
      "rank6:  93%|█████████▎| 4672/5000 [02:39<00:21, 14.99it/s]\n",
      "rank4:  96%|█████████▌| 4784/5000 [02:40<00:15, 13.91it/s]\n",
      "rank3:  92%|█████████▏| 4576/5000 [02:40<00:26, 16.02it/s]\n",
      "rank6:  94%|█████████▍| 4688/5000 [02:40<00:21, 14.62it/s]\n",
      "rank4:  96%|█████████▌| 4800/5000 [02:41<00:14, 13.75it/s]\n",
      "rank3:  92%|█████████▏| 4592/5000 [02:41<00:25, 15.84it/s]\n",
      "rank6:  94%|█████████▍| 4704/5000 [02:42<00:20, 14.49it/s]\n",
      "rank1:  92%|█████████▏| 4576/5000 [02:42<00:28, 14.64it/s]\n",
      "rank3:  92%|█████████▏| 4608/5000 [02:42<00:24, 15.73it/s]\n",
      "rank4:  96%|█████████▋| 4816/5000 [02:42<00:13, 13.63it/s]\n",
      "rank0:  91%|█████████ | 4528/5000 [02:42<00:31, 15.15it/s]\u001b[32m [repeated 25x across cluster]\u001b[0m\n",
      "rank6:  94%|█████████▍| 4720/5000 [02:43<00:19, 14.31it/s]\n",
      "rank3:  92%|█████████▏| 4624/5000 [02:43<00:24, 15.67it/s]\n",
      "rank1:  92%|█████████▏| 4592/5000 [02:43<00:28, 14.46it/s]\n",
      "rank4:  97%|█████████▋| 4832/5000 [02:43<00:12, 13.38it/s]\n",
      "rank6:  95%|█████████▍| 4736/5000 [02:44<00:18, 14.23it/s]\n",
      "rank3:  93%|█████████▎| 4640/5000 [02:44<00:23, 15.55it/s]\n",
      "rank1:  92%|█████████▏| 4608/5000 [02:44<00:27, 14.37it/s]\n",
      "rank4:  97%|█████████▋| 4848/5000 [02:44<00:11, 13.09it/s]\n",
      "rank6:  95%|█████████▌| 4752/5000 [02:45<00:17, 13.95it/s]\n",
      "rank3:  93%|█████████▎| 4656/5000 [02:45<00:22, 15.32it/s]\n",
      "rank1:  92%|█████████▏| 4624/5000 [02:45<00:26, 14.27it/s]\n",
      "rank0:  92%|█████████▏| 4576/5000 [02:45<00:28, 14.81it/s]\n",
      "rank4:  97%|█████████▋| 4864/5000 [02:46<00:10, 12.70it/s]\n",
      "rank3:  93%|█████████▎| 4672/5000 [02:46<00:21, 15.09it/s]\n",
      "rank6:  95%|█████████▌| 4768/5000 [02:46<00:17, 13.63it/s]\n",
      "rank5:  92%|█████████▏| 4576/5000 [02:46<00:29, 14.57it/s]\n",
      "rank1:  93%|█████████▎| 4640/5000 [02:47<00:25, 14.15it/s]\n",
      "rank0:  92%|█████████▏| 4592/5000 [02:47<00:27, 14.75it/s]\n",
      "rank2:  92%|█████████▏| 4576/5000 [02:47<00:28, 14.68it/s]\n",
      "rank4:  98%|█████████▊| 4880/5000 [02:47<00:09, 12.42it/s]\n",
      "rank3:  94%|█████████▍| 4688/5000 [02:47<00:20, 14.88it/s]\n",
      "rank7:  90%|████████▉ | 4496/5000 [02:46<00:31, 15.88it/s]\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
      "rank5:  92%|█████████▏| 4592/5000 [02:47<00:28, 14.46it/s]\n",
      "rank6:  96%|█████████▌| 4784/5000 [02:48<00:16, 13.42it/s]\n",
      "rank0:  92%|█████████▏| 4608/5000 [02:48<00:26, 14.67it/s]\n",
      "rank1:  93%|█████████▎| 4656/5000 [02:48<00:24, 14.02it/s]\n",
      "rank2:  92%|█████████▏| 4592/5000 [02:48<00:27, 14.59it/s]\n",
      "rank3:  94%|█████████▍| 4704/5000 [02:48<00:20, 14.59it/s]\n",
      "rank5:  92%|█████████▏| 4608/5000 [02:49<00:27, 14.39it/s]\n",
      "rank4:  98%|█████████▊| 4896/5000 [02:49<00:08, 11.92it/s]\n",
      "rank6:  96%|█████████▌| 4800/5000 [02:49<00:15, 13.28it/s]\n",
      "rank0:  92%|█████████▏| 4624/5000 [02:49<00:25, 14.56it/s]\n",
      "rank1:  93%|█████████▎| 4672/5000 [02:49<00:23, 13.81it/s]\n",
      "rank2:  92%|█████████▏| 4608/5000 [02:49<00:27, 14.49it/s]\n",
      "rank3:  94%|█████████▍| 4720/5000 [02:50<00:19, 14.51it/s]\n",
      "rank5:  92%|█████████▏| 4624/5000 [02:50<00:26, 14.30it/s]\n",
      "rank0:  93%|█████████▎| 4640/5000 [02:50<00:25, 14.38it/s]\n",
      "rank1:  94%|█████████▍| 4688/5000 [02:50<00:22, 13.66it/s]\n",
      "rank6:  96%|█████████▋| 4816/5000 [02:50<00:14, 13.00it/s]\n",
      "rank2:  92%|█████████▏| 4624/5000 [02:50<00:26, 14.33it/s]\n",
      "rank4:  98%|█████████▊| 4912/5000 [02:50<00:07, 11.51it/s]\n",
      "rank3:  95%|█████████▍| 4736/5000 [02:51<00:18, 14.44it/s]\n",
      "rank5:  93%|█████████▎| 4640/5000 [02:51<00:25, 14.24it/s]\n",
      "rank0:  93%|█████████▎| 4656/5000 [02:51<00:24, 14.27it/s]\n",
      "rank1:  94%|█████████▍| 4704/5000 [02:51<00:21, 13.49it/s]\n",
      "rank2:  93%|█████████▎| 4640/5000 [02:51<00:25, 14.16it/s]\n",
      "rank6:  97%|█████████▋| 4832/5000 [02:51<00:13, 12.87it/s]\n",
      "rank7:  92%|█████████▏| 4576/5000 [02:51<00:26, 15.98it/s]\n",
      "rank4:  99%|█████████▊| 4928/5000 [02:52<00:06, 11.31it/s]\n",
      "rank3:  95%|█████████▌| 4752/5000 [02:52<00:17, 14.28it/s]\n",
      "rank5:  93%|█████████▎| 4656/5000 [02:52<00:24, 14.12it/s]\n",
      "rank0:  93%|█████████▎| 4672/5000 [02:52<00:23, 14.13it/s]\n",
      "rank1:  94%|█████████▍| 4720/5000 [02:53<00:20, 13.38it/s]\n",
      "rank7:  91%|█████████ | 4560/5000 [02:50<00:27, 16.08it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "rank2:  93%|█████████▎| 4656/5000 [02:52<00:24, 13.88it/s]\n",
      "rank7:  92%|█████████▏| 4592/5000 [02:52<00:25, 15.93it/s]\n",
      "rank6:  97%|█████████▋| 4848/5000 [02:53<00:11, 12.81it/s]\n",
      "rank3:  95%|█████████▌| 4768/5000 [02:53<00:16, 14.10it/s]\n",
      "rank4:  99%|█████████▉| 4944/5000 [02:53<00:05, 11.11it/s]\n",
      "rank5:  93%|█████████▎| 4672/5000 [02:53<00:23, 13.97it/s]\n",
      "rank0:  94%|█████████▍| 4688/5000 [02:53<00:22, 13.92it/s]\n",
      "rank7:  92%|█████████▏| 4608/5000 [02:53<00:24, 15.80it/s]\n",
      "rank1:  95%|█████████▍| 4736/5000 [02:54<00:19, 13.26it/s]\n",
      "rank2:  93%|█████████▎| 4672/5000 [02:54<00:23, 13.77it/s]\n",
      "rank6:  97%|█████████▋| 4864/5000 [02:54<00:10, 12.49it/s]\n",
      "rank3:  96%|█████████▌| 4784/5000 [02:54<00:15, 13.90it/s]\n",
      "rank5:  94%|█████████▍| 4688/5000 [02:54<00:22, 13.85it/s]\n",
      "rank7:  92%|█████████▏| 4624/5000 [02:54<00:24, 15.61it/s]\n",
      "rank0:  94%|█████████▍| 4704/5000 [02:55<00:21, 13.76it/s]\n",
      "rank4:  99%|█████████▉| 4960/5000 [02:55<00:03, 10.88it/s]\n",
      "rank2:  94%|█████████▍| 4688/5000 [02:55<00:22, 13.69it/s]\n",
      "rank1:  95%|█████████▌| 4752/5000 [02:55<00:18, 13.08it/s]\n",
      "rank6:  98%|█████████▊| 4880/5000 [02:55<00:10, 11.89it/s]\n",
      "rank3:  96%|█████████▌| 4800/5000 [02:55<00:14, 13.70it/s]\n",
      "rank5:  94%|█████████▍| 4704/5000 [02:55<00:21, 13.71it/s]\n",
      "rank7:  93%|█████████▎| 4640/5000 [02:55<00:23, 15.44it/s]\n",
      "rank0:  94%|█████████▍| 4720/5000 [02:56<00:20, 13.63it/s]\n",
      "rank2:  94%|█████████▍| 4704/5000 [02:56<00:21, 13.60it/s]\n",
      "rank1:  95%|█████████▌| 4768/5000 [02:56<00:18, 12.84it/s]\n",
      "rank4: 100%|█████████▉| 4976/5000 [02:56<00:02, 10.64it/s]\n",
      "rank3:  96%|█████████▋| 4816/5000 [02:57<00:13, 13.44it/s]\n",
      "rank7:  93%|█████████▎| 4656/5000 [02:57<00:22, 15.08it/s]\n",
      "rank5:  94%|█████████▍| 4720/5000 [02:57<00:20, 13.49it/s]\n",
      "rank0:  95%|█████████▍| 4736/5000 [02:57<00:19, 13.52it/s]\n",
      "rank6:  98%|█████████▊| 4896/5000 [02:57<00:09, 11.26it/s]\n",
      "rank2:  94%|█████████▍| 4720/5000 [02:57<00:20, 13.51it/s]\n",
      "rank1:  96%|█████████▌| 4784/5000 [02:58<00:17, 12.70it/s]\n",
      "rank7:  93%|█████████▎| 4672/5000 [02:58<00:21, 14.98it/s]\n",
      "rank3:  97%|█████████▋| 4832/5000 [02:58<00:12, 13.11it/s]\n",
      "rank4: 100%|█████████▉| 4992/5000 [02:58<00:00, 10.43it/s]\n",
      "rank5:  95%|█████████▍| 4736/5000 [02:58<00:19, 13.39it/s]\n",
      "rank0:  95%|█████████▌| 4752/5000 [02:58<00:18, 13.41it/s]\n",
      "rank2:  95%|█████████▍| 4736/5000 [02:58<00:19, 13.42it/s]\n",
      "rank6:  98%|█████████▊| 4912/5000 [02:59<00:07, 11.02it/s]\n",
      "rank1:  96%|█████████▌| 4800/5000 [02:59<00:15, 12.57it/s]\n",
      "rank4: 100%|██████████| 5000/5000 [02:59<00:00, 10.08it/s]\n",
      "rank7:  94%|█████████▍| 4688/5000 [02:59<00:21, 14.82it/s]\n",
      "rank4: 100%|██████████| 5000/5000 [02:59<00:00, 27.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1599)\u001b[0m [checkpoint] saved 5000 → esm_sae_results/sae_features_rank4_cache/sae_features_rank4_5000.pkl\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ray_worker pid=1599)\u001b[0m [done] 5000 sequences → esm_sae_results/sae_features_rank4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank3:  97%|█████████▋| 4848/5000 [02:59<00:11, 12.78it/s]\n",
      "rank5:  95%|█████████▌| 4752/5000 [02:59<00:18, 13.23it/s]\n",
      "rank0:  95%|█████████▌| 4768/5000 [02:59<00:17, 13.26it/s]\n",
      "rank2:  95%|█████████▌| 4752/5000 [03:00<00:18, 13.23it/s]\n",
      "rank6:  99%|█████████▊| 4928/5000 [03:00<00:06, 10.80it/s]\n",
      "rank7:  94%|█████████▍| 4704/5000 [03:00<00:20, 14.65it/s]\n",
      "rank1:  96%|█████████▋| 4816/5000 [03:00<00:14, 12.43it/s]\n",
      "rank5:  95%|█████████▌| 4768/5000 [03:00<00:17, 13.09it/s]\n",
      "rank3:  97%|█████████▋| 4864/5000 [03:01<00:10, 12.58it/s]\n",
      "rank0:  96%|█████████▌| 4784/5000 [03:01<00:16, 13.10it/s]\n",
      "rank2:  95%|█████████▌| 4768/5000 [03:01<00:17, 13.00it/s]\n",
      "rank7:  94%|█████████▍| 4720/5000 [03:01<00:19, 14.47it/s]\n",
      "rank1:  97%|█████████▋| 4832/5000 [03:02<00:13, 12.19it/s]\n",
      "rank6:  99%|█████████▉| 4944/5000 [03:02<00:05, 10.55it/s]\n",
      "rank5:  96%|█████████▌| 4784/5000 [03:02<00:16, 12.86it/s]\n",
      "rank0:  96%|█████████▌| 4800/5000 [03:02<00:16, 12.38it/s]\n",
      "rank3:  98%|█████████▊| 4880/5000 [03:02<00:10, 11.81it/s]\n",
      "rank7:  95%|█████████▍| 4736/5000 [03:02<00:18, 14.31it/s]\n",
      "rank2:  96%|█████████▌| 4784/5000 [03:02<00:17, 12.56it/s]\n",
      "rank1:  97%|█████████▋| 4848/5000 [03:03<00:12, 11.90it/s]\n",
      "rank5:  96%|█████████▌| 4800/5000 [03:03<00:15, 12.64it/s]\n",
      "rank6:  99%|█████████▉| 4960/5000 [03:03<00:03, 10.32it/s]\n",
      "rank7:  95%|█████████▌| 4752/5000 [03:03<00:17, 14.36it/s]\n",
      "rank0:  96%|█████████▋| 4816/5000 [03:03<00:14, 12.32it/s]\n",
      "rank3:  98%|█████████▊| 4896/5000 [03:04<00:09, 11.53it/s]\n",
      "rank2:  96%|█████████▌| 4800/5000 [03:04<00:16, 12.44it/s]\n",
      "rank1:  97%|█████████▋| 4864/5000 [03:04<00:11, 11.61it/s]\n",
      "rank5:  96%|█████████▋| 4816/5000 [03:04<00:14, 12.36it/s]\n",
      "rank7:  95%|█████████▌| 4768/5000 [03:04<00:16, 14.30it/s]\n",
      "rank0:  97%|█████████▋| 4832/5000 [03:05<00:13, 12.21it/s]\n",
      "rank2:  96%|█████████▋| 4816/5000 [03:05<00:14, 12.35it/s]\n",
      "rank6: 100%|█████████▉| 4976/5000 [03:05<00:02, 10.09it/s]\n",
      "rank3:  98%|█████████▊| 4912/5000 [03:05<00:07, 11.17it/s]\n",
      "rank7:  96%|█████████▌| 4784/5000 [03:06<00:15, 14.17it/s]\n",
      "rank5:  97%|█████████▋| 4832/5000 [03:06<00:13, 12.22it/s]\n",
      "rank1:  98%|█████████▊| 4880/5000 [03:06<00:10, 11.13it/s]\n",
      "rank0:  97%|█████████▋| 4848/5000 [03:06<00:12, 12.14it/s]\n",
      "rank2:  97%|█████████▋| 4832/5000 [03:06<00:13, 12.22it/s]\n",
      "rank3:  99%|█████████▊| 4928/5000 [03:07<00:06, 11.15it/s]\n",
      "rank6: 100%|█████████▉| 4992/5000 [03:07<00:00,  9.90it/s]\n",
      "rank7:  96%|█████████▌| 4800/5000 [03:07<00:14, 14.02it/s]\n",
      "rank5:  97%|█████████▋| 4848/5000 [03:07<00:12, 12.06it/s]\n",
      "rank0:  97%|█████████▋| 4864/5000 [03:07<00:11, 12.00it/s]\n",
      "rank6: 100%|██████████| 5000/5000 [03:08<00:00,  9.59it/s]\n",
      "rank1:  98%|█████████▊| 4896/5000 [03:08<00:09, 10.74it/s]\n",
      "rank2:  97%|█████████▋| 4848/5000 [03:08<00:12, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1605)\u001b[0m [checkpoint] saved 5000 → esm_sae_results/sae_features_rank6_cache/sae_features_rank6_5000.pkl\n",
      "\u001b[36m(ray_worker pid=1605)\u001b[0m [done] 5000 sequences → esm_sae_results/sae_features_rank6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank6: 100%|██████████| 5000/5000 [03:08<00:00, 26.55it/s]\n",
      "rank7:  96%|█████████▋| 4816/5000 [03:08<00:13, 13.84it/s]\n",
      "rank3:  99%|█████████▉| 4944/5000 [03:08<00:05, 11.12it/s]\n",
      "rank5:  97%|█████████▋| 4864/5000 [03:09<00:11, 11.85it/s]\n",
      "rank0:  98%|█████████▊| 4880/5000 [03:09<00:10, 11.79it/s]\n",
      "rank2:  97%|█████████▋| 4864/5000 [03:09<00:11, 11.94it/s]\n",
      "rank1:  98%|█████████▊| 4912/5000 [03:09<00:08, 10.64it/s]\n",
      "rank7:  97%|█████████▋| 4832/5000 [03:09<00:12, 13.70it/s]\n",
      "rank3:  99%|█████████▉| 4960/5000 [03:10<00:03, 10.91it/s]\n",
      "rank5:  98%|█████████▊| 4880/5000 [03:10<00:10, 11.23it/s]\n",
      "rank7:  97%|█████████▋| 4848/5000 [03:10<00:11, 13.37it/s]\n",
      "rank0:  98%|█████████▊| 4896/5000 [03:10<00:09, 11.24it/s]\n",
      "rank2:  98%|█████████▊| 4880/5000 [03:11<00:10, 11.61it/s]\n",
      "rank1:  99%|█████████▊| 4928/5000 [03:11<00:06, 10.45it/s]\n",
      "rank3: 100%|█████████▉| 4976/5000 [03:11<00:02, 10.09it/s]\n",
      "rank5:  98%|█████████▊| 4896/5000 [03:12<00:09, 10.88it/s]\n",
      "rank7:  97%|█████████▋| 4864/5000 [03:12<00:10, 12.94it/s]\n",
      "rank2:  98%|█████████▊| 4896/5000 [03:12<00:09, 11.24it/s]\n",
      "rank0:  98%|█████████▊| 4912/5000 [03:12<00:08, 10.91it/s]\n",
      "rank1:  99%|█████████▉| 4944/5000 [03:12<00:05, 10.33it/s]\n",
      "rank3: 100%|█████████▉| 4992/5000 [03:13<00:00,  9.96it/s]\n",
      "rank7:  98%|█████████▊| 4880/5000 [03:13<00:09, 12.62it/s]\n",
      "rank5:  98%|█████████▊| 4912/5000 [03:13<00:08, 10.84it/s]\n",
      "rank0:  99%|█████████▊| 4928/5000 [03:14<00:06, 10.82it/s]\n",
      "rank2:  98%|█████████▊| 4912/5000 [03:14<00:08, 10.89it/s]\n",
      "rank1:  99%|█████████▉| 4960/5000 [03:14<00:03, 10.20it/s]\n",
      "rank3: 100%|██████████| 5000/5000 [03:14<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1598)\u001b[0m [checkpoint] saved 5000 → esm_sae_results/sae_features_rank3_cache/sae_features_rank3_5000.pkl\n",
      "\u001b[36m(ray_worker pid=1598)\u001b[0m [done] 5000 sequences → esm_sae_results/sae_features_rank3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank3: 100%|██████████| 5000/5000 [03:14<00:00, 25.69it/s]\n",
      "rank7:  98%|█████████▊| 4896/5000 [03:14<00:08, 12.38it/s]\n",
      "rank5:  99%|█████████▊| 4928/5000 [03:15<00:06, 10.70it/s]\n",
      "rank0:  99%|█████████▉| 4944/5000 [03:15<00:05, 10.65it/s]\n",
      "rank2:  99%|█████████▊| 4928/5000 [03:15<00:06, 10.75it/s]\n",
      "rank1: 100%|█████████▉| 4976/5000 [03:16<00:02, 10.01it/s]\n",
      "rank7:  98%|█████████▊| 4912/5000 [03:16<00:07, 11.89it/s]\n",
      "rank5:  99%|█████████▉| 4944/5000 [03:16<00:05, 10.48it/s]\n",
      "rank0:  99%|█████████▉| 4960/5000 [03:17<00:03, 10.43it/s]\n",
      "rank2:  99%|█████████▉| 4944/5000 [03:17<00:05, 10.48it/s]\n",
      "rank1: 100%|█████████▉| 4992/5000 [03:18<00:00,  9.54it/s]\n",
      "rank7:  99%|█████████▊| 4928/5000 [03:17<00:06, 11.38it/s]\n",
      "rank5:  99%|█████████▉| 4960/5000 [03:18<00:03, 10.28it/s]\n",
      "rank0: 100%|█████████▉| 4976/5000 [03:18<00:02, 10.20it/s]\n",
      "rank2:  99%|█████████▉| 4960/5000 [03:18<00:03, 10.27it/s]\n",
      "rank1: 100%|██████████| 5000/5000 [03:19<00:00,  9.23it/s]\n",
      "rank1: 100%|██████████| 5000/5000 [03:19<00:00, 25.10it/s]\n",
      "rank7:  99%|█████████▉| 4944/5000 [03:19<00:04, 11.42it/s]\n",
      "rank5: 100%|█████████▉| 4976/5000 [03:20<00:02, 10.13it/s]\n",
      "rank2: 100%|█████████▉| 4976/5000 [03:20<00:02, 10.08it/s]\n",
      "rank0: 100%|█████████▉| 4992/5000 [03:20<00:00,  9.98it/s]\n",
      "rank7:  99%|█████████▉| 4960/5000 [03:20<00:03, 11.19it/s]\n",
      "rank0: 100%|██████████| 5000/5000 [03:21<00:00,  9.36it/s]\n",
      "rank0: 100%|██████████| 5000/5000 [03:21<00:00, 24.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ray_worker pid=1597)\u001b[0m [checkpoint] saved 5000 → esm_sae_results/sae_features_rank0_cache/sae_features_rank0_5000.pkl\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ray_worker pid=1603)\u001b[0m [done] 5000 sequences → esm_sae_results/sae_features_rank1.pkl\n",
      "\u001b[36m(ray_worker pid=1597)\u001b[0m [done] 5000 sequences → esm_sae_results/sae_features_rank0.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rank5: 100%|█████████▉| 4992/5000 [03:22<00:00,  9.47it/s]\n",
      "rank2: 100%|█████████▉| 4992/5000 [03:22<00:00,  9.57it/s]\n",
      "rank7: 100%|█████████▉| 4976/5000 [03:22<00:02, 10.95it/s]\n",
      "rank5: 100%|██████████| 5000/5000 [03:23<00:00,  9.23it/s]\n",
      "rank5: 100%|██████████| 5000/5000 [03:23<00:00, 24.62it/s]\n",
      "rank2: 100%|██████████| 5000/5000 [03:23<00:00,  9.29it/s]\n",
      "rank2: 100%|██████████| 5000/5000 [03:23<00:00, 24.57it/s]\n",
      "rank7: 100%|█████████▉| 4992/5000 [03:23<00:00, 10.64it/s]\n",
      "rank7: 100%|██████████| 5000/5000 [03:24<00:00, 10.20it/s]\n",
      "rank7: 100%|██████████| 5000/5000 [03:25<00:00, 24.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['esm_sae_results/sae_features_rank0.final.pkl',\n",
       " 'esm_sae_results/sae_features_rank1.final.pkl',\n",
       " 'esm_sae_results/sae_features_rank2.final.pkl',\n",
       " 'esm_sae_results/sae_features_rank3.final.pkl',\n",
       " 'esm_sae_results/sae_features_rank4.final.pkl',\n",
       " 'esm_sae_results/sae_features_rank5.final.pkl',\n",
       " 'esm_sae_results/sae_features_rank6.final.pkl',\n",
       " 'esm_sae_results/sae_features_rank7.final.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORLD_SIZE = 8\n",
    "input_path = str(DATA_DIR / \"sequences_df.pkl\")\n",
    "futs = [ray_worker.remote(r, WORLD_SIZE, input_path, \"sae_features\") for r in range(WORLD_SIZE)]\n",
    "paths = ray.get(futs)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "parts = [pd.read_pickle(p) for p in sorted(glob.glob(str(DATA_DIR / \"sae_features_rank*.final.pkl\")))]\n",
    "features_all = pd.concat(parts, ignore_index=True).drop_duplicates(subset=[\"uniprot_id\"])\n",
    "features_all.to_pickle(DATA_DIR / \"sae_features_all.pkl\")\n",
    "features_all.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>length</th>\n",
       "      <th>features</th>\n",
       "      <th>max_activation</th>\n",
       "      <th>n_active_features</th>\n",
       "      <th>reconstruction_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q9GL23</td>\n",
       "      <td>50</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002...</td>\n",
       "      <td>1.265625</td>\n",
       "      <td>1876</td>\n",
       "      <td>45.198380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6GZU6</td>\n",
       "      <td>50</td>\n",
       "      <td>[0.00023197175, 0.0, 0.0, 0.0, 0.0013056946, 0...</td>\n",
       "      <td>0.843262</td>\n",
       "      <td>2168</td>\n",
       "      <td>13.467114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P9WJG6</td>\n",
       "      <td>50</td>\n",
       "      <td>[0.0, 0.00057144166, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.935059</td>\n",
       "      <td>1740</td>\n",
       "      <td>12.720748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P18924</td>\n",
       "      <td>51</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...</td>\n",
       "      <td>0.956543</td>\n",
       "      <td>1799</td>\n",
       "      <td>11.394856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q08076</td>\n",
       "      <td>52</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...</td>\n",
       "      <td>1.139648</td>\n",
       "      <td>1772</td>\n",
       "      <td>24.694654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P15450</td>\n",
       "      <td>53</td>\n",
       "      <td>[0.0, 0.00019778845, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.090820</td>\n",
       "      <td>1698</td>\n",
       "      <td>13.188174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P40643</td>\n",
       "      <td>54</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031...</td>\n",
       "      <td>0.881348</td>\n",
       "      <td>1450</td>\n",
       "      <td>21.303719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P12697</td>\n",
       "      <td>54</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001...</td>\n",
       "      <td>1.243164</td>\n",
       "      <td>1535</td>\n",
       "      <td>21.009787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O78683</td>\n",
       "      <td>54</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...</td>\n",
       "      <td>1.144531</td>\n",
       "      <td>1657</td>\n",
       "      <td>20.722115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Q9MJC0</td>\n",
       "      <td>54</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001...</td>\n",
       "      <td>1.224609</td>\n",
       "      <td>1680</td>\n",
       "      <td>25.309597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  uniprot_id  length                                           features  \\\n",
       "0     Q9GL23      50  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002...   \n",
       "1     Q6GZU6      50  [0.00023197175, 0.0, 0.0, 0.0, 0.0013056946, 0...   \n",
       "2     P9WJG6      50  [0.0, 0.00057144166, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3     P18924      51  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...   \n",
       "4     Q08076      52  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...   \n",
       "5     P15450      53  [0.0, 0.00019778845, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "6     P40643      54  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.031...   \n",
       "7     P12697      54  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001...   \n",
       "8     O78683      54  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.000...   \n",
       "9     Q9MJC0      54  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001...   \n",
       "\n",
       "   max_activation  n_active_features  reconstruction_mse  \n",
       "0        1.265625               1876           45.198380  \n",
       "1        0.843262               2168           13.467114  \n",
       "2        0.935059               1740           12.720748  \n",
       "3        0.956543               1799           11.394856  \n",
       "4        1.139648               1772           24.694654  \n",
       "5        1.090820               1698           13.188174  \n",
       "6        0.881348               1450           21.303719  \n",
       "7        1.243164               1535           21.009787  \n",
       "8        1.144531               1657           20.722115  \n",
       "9        1.224609               1680           25.309597  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in UniProt annotations:\n",
      "['Entry', 'Reviewed', 'Protein names', 'Length', 'Sequence', 'EC number', 'Active site', 'Binding site', 'Cofactor', 'Disulfide bond', 'Glycosylation', 'Lipidation', 'Modified residue', 'Signal peptide', 'Transit peptide', 'Helix', 'Turn', 'Beta strand', 'Coiled coil', 'Domain [CC]', 'Compositional bias', 'Domain [FT]', 'Motif', 'Region', 'Zinc finger', 'AlphaFoldDB']\n",
      "\n",
      "First few rows:\n",
      "                                                                    0  \\\n",
      "Entry                                                      A0A009IHW8   \n",
      "Reviewed                                                     reviewed   \n",
      "Protein names       2' cyclic ADP-D-ribose synthase AbTIR (2'cADPR...   \n",
      "Length                                                            269   \n",
      "Sequence            MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENA...   \n",
      "EC number                                            3.2.2.-; 3.2.2.6   \n",
      "Active site         ACT_SITE 208; /evidence=\"ECO:0000255|PROSITE-P...   \n",
      "Binding site        BINDING 143; /ligand=\"NAD(+)\"; /ligand_id=\"ChE...   \n",
      "Cofactor                                                          NaN   \n",
      "Disulfide bond                                                    NaN   \n",
      "Glycosylation                                                     NaN   \n",
      "Lipidation                                                        NaN   \n",
      "Modified residue                                                  NaN   \n",
      "Signal peptide                                                    NaN   \n",
      "Transit peptide                                                   NaN   \n",
      "Helix               HELIX 143..145; /evidence=\"ECO:0007829|PDB:7UW...   \n",
      "Turn                TURN 146..149; /evidence=\"ECO:0007829|PDB:7UWG...   \n",
      "Beta strand         STRAND 135..142; /evidence=\"ECO:0007829|PDB:7U...   \n",
      "Coiled coil                    COILED 31..99; /evidence=\"ECO:0000255\"   \n",
      "Domain [CC]         DOMAIN: The TIR domain mediates NAD(+) hydrola...   \n",
      "Compositional bias                                                NaN   \n",
      "Domain [FT]         DOMAIN 133..266; /note=\"TIR\"; /evidence=\"ECO:0...   \n",
      "Motif                                                             NaN   \n",
      "Region                                                            NaN   \n",
      "Zinc finger                                                       NaN   \n",
      "AlphaFoldDB                                               A0A009IHW8;   \n",
      "\n",
      "                                                                    1  \\\n",
      "Entry                                                      A0A023I7E1   \n",
      "Reviewed                                                     reviewed   \n",
      "Protein names       Glucan endo-1,3-beta-D-glucosidase 1 (Endo-1,3...   \n",
      "Length                                                            796   \n",
      "Sequence            MRFQVIVAAATITMITSYIPGVASQSTSDGDDLFVPVSNFDPKSIF...   \n",
      "EC number                                                    3.2.1.39   \n",
      "Active site         ACT_SITE 500; /evidence=\"ECO:0000255|PROSITE-P...   \n",
      "Binding site        BINDING 504; /ligand=\"(1,3-beta-D-glucosyl)n\";...   \n",
      "Cofactor                                                          NaN   \n",
      "Disulfide bond                                                    NaN   \n",
      "Glycosylation       CARBOHYD 219; /note=\"N-linked (GlcNAc...) aspa...   \n",
      "Lipidation                                                        NaN   \n",
      "Modified residue                                                  NaN   \n",
      "Signal peptide                  SIGNAL 1..24; /evidence=\"ECO:0000255\"   \n",
      "Transit peptide                                                   NaN   \n",
      "Helix               HELIX 42..44; /evidence=\"ECO:0007829|PDB:4K35\"...   \n",
      "Turn                TURN 287..289; /evidence=\"ECO:0007829|PDB:4K35...   \n",
      "Beta strand         STRAND 56..58; /evidence=\"ECO:0007829|PDB:4K35...   \n",
      "Coiled coil                                                       NaN   \n",
      "Domain [CC]                                                       NaN   \n",
      "Compositional bias                                                NaN   \n",
      "Domain [FT]         DOMAIN 31..759; /note=\"GH81\"; /evidence=\"ECO:0...   \n",
      "Motif                                                             NaN   \n",
      "Region              REGION 31..276; /note=\"beta-sandwich subdomain...   \n",
      "Zinc finger                                                       NaN   \n",
      "AlphaFoldDB                                               A0A023I7E1;   \n",
      "\n",
      "                                                                    2  \n",
      "Entry                                                      A0A024SC78  \n",
      "Reviewed                                                     reviewed  \n",
      "Protein names                                  Cutinase (EC 3.1.1.74)  \n",
      "Length                                                            248  \n",
      "Sequence            MRSLAILTTLLAGHAFAYPKPAPQSVNRRDWPSINEFLSELAKVMP...  \n",
      "EC number                                                    3.1.1.74  \n",
      "Active site         ACT_SITE 164; /note=\"Nucleophile\"; /evidence=\"...  \n",
      "Binding site                                                      NaN  \n",
      "Cofactor                                                          NaN  \n",
      "Disulfide bond      DISULFID 55..91; /evidence=\"ECO:0000269|PubMed...  \n",
      "Glycosylation                                                     NaN  \n",
      "Lipidation                                                        NaN  \n",
      "Modified residue                                                  NaN  \n",
      "Signal peptide                  SIGNAL 1..17; /evidence=\"ECO:0000255\"  \n",
      "Transit peptide                                                   NaN  \n",
      "Helix               HELIX 51..69; /evidence=\"ECO:0007829|PDB:4PSC\"...  \n",
      "Turn                TURN 94..100; /evidence=\"ECO:0007829|PDB:4PSC\"...  \n",
      "Beta strand         STRAND 48..50; /evidence=\"ECO:0007829|PDB:4PSC...  \n",
      "Coiled coil                                                       NaN  \n",
      "Domain [CC]         DOMAIN: In contract to classical cutinases, po...  \n",
      "Compositional bias                                                NaN  \n",
      "Domain [FT]                                                       NaN  \n",
      "Motif                                                             NaN  \n",
      "Region              REGION 31..70; /note=\"Lid covering the active ...  \n",
      "Zinc finger                                                       NaN  \n",
      "AlphaFoldDB                                               A0A024SC78;  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Quick peek at the annotation file ---\n",
    "annotations_path = Path(\"/home/ec2-user/InterPLM/data/uniprot/proteins_annotations.tsv.gz\")  # update with your path\n",
    "df = pd.read_csv(annotations_path, sep=\"\\t\", compression=\"gzip\")\n",
    "print(\"Columns in UniProt annotations:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(3).T)  # transpose to make it easier to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Entry', 'Reviewed', 'Protein names', 'Length', 'Sequence', 'EC number', 'Active site', 'Binding site', 'Cofactor', 'Disulfide bond', 'Glycosylation', 'Lipidation', 'Modified residue', 'Signal peptide', 'Transit peptide', 'Helix', 'Turn', 'Beta strand', 'Coiled coil', 'Domain [CC]', 'Compositional bias', 'Domain [FT]', 'Motif', 'Region', 'Zinc finger', 'AlphaFoldDB']\n",
      "(5, 26)\n"
     ]
    }
   ],
   "source": [
    "f = pd.read_csv(annotations_path, sep=\"\\t\", compression=\"gzip\", nrows=5)\n",
    "print(df.columns.tolist())\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example UniProt IDs in your features_df:\n",
      "['Q9GL23', 'Q6GZU6', 'P9WJG6', 'P18924', 'Q08076']\n",
      "(33998, 26)\n",
      "(0, 26)\n",
      "\n",
      "Found 0 matches in annotations:\n",
      "Empty DataFrame\n",
      "Columns: [Entry, Protein names, Length]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Pick a few IDs you processed ---\n",
    "print(\"\\nExample UniProt IDs in your features_df:\")\n",
    "print(features_all['uniprot_id'].head().tolist())\n",
    "\n",
    "# --- 3. Check if those IDs exist in the annotations ---\n",
    "ids_to_check = features_all['uniprot_id'].head(5).tolist()\n",
    "sub = pd.read_csv(annotations_path, sep=\"\\t\", compression=\"gzip\")\n",
    "print(sub.shape)\n",
    "sub = sub[sub['Entry'].isin(ids_to_check)]\n",
    "print(sub.shape)\n",
    "print(f\"\\nFound {len(sub)} matches in annotations:\")\n",
    "print(sub[['Entry','Protein names','Length']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://rest.uniprot.org/uniprotkb/search?query=%28accession%3AQ9GL23+OR+accession%3AQ6GZU6+OR+accession%3AP9WJG6+OR+accession%3AP18924+OR+accession%3AQ08076+OR+accession%3AP15450+OR+accession%3AP40643+OR+accession%3AP12697+OR+accession%3AO78683+OR+accession%3AQ9MJC0+OR+accession%3AQ9MFP7+OR+accession%3AP80302+OR+accession%3AO03168+OR+accession%3AO79405+OR+accession%3AP68528+OR+accession%3AP67806+OR+accession%3AQ7M460+OR+accession%3AP86420+OR+accession%3AP0DQH2+OR+accession%3AQ29149+OR+accession%3AO55724+OR+accession%3AQ8WSV2+OR+accession%3AP0DKS3+OR+accession%3AQ28280+OR+accession%3AQ3ZBI7+OR+accession%3AP20699+OR+accession%3AQ9STQ3+OR+accession%3AP69945+OR+accession%3AP07525+OR+accession%3AP01464+OR+accession%3AP60307+OR+accession%3AP86540+OR+accession%3AP25494+OR+accession%3AP01445+OR+accession%3AQ29005+OR+accession%3AP0CH80+OR+accession%3AP01459+OR+accession%3AP01469+OR+accession%3AP01422+OR+accession%3AQ8S2W4+OR+accession%3AC0HK65+OR+accession%3AQ91FM4+OR+accession%3AP68418+OR+accession%3AP25496+OR+accession%3AQ89183+OR+accession%3AA0A2I8B346+OR+accession%3AP83610+OR+accession%3AP10458+OR+accession%3AQ91FT2+OR+accession%3AQ8KRR5+OR+accession%3AP01408+OR+accession%3AP12100+OR+accession%3AP10808+OR+accession%3AP86098+OR+accession%3AC0HJT5+OR+accession%3AP85092+OR+accession%3AP01400+OR+accession%3AQ38PR8+OR+accession%3AQ5Y4Q6+OR+accession%3AP50017+OR+accession%3AQ9PSN1+OR+accession%3AP0A307+OR+accession%3AP17718+OR+accession%3AB7M0V4+OR+accession%3AB5REW4+OR+accession%3AQ35914+OR+accession%3AE1SFM6+OR+accession%3AB7NDM0+OR+accession%3AQ8W9N2+OR+accession%3AO21329+OR+accession%3AQ8XEK3+OR+accession%3AQ1CDW5+OR+accession%3AA8AQD6+OR+accession%3AQ3YX05+OR+accession%3AQ9MEI6+OR+accession%3AP11608+OR+accession%3AA6XMY1+OR+accession%3AQ30YK6+OR+accession%3AQ9WUC4+OR+accession%3AP81451+OR+accession%3AQ34571+OR+accession%3AQ3L6L9+OR+accession%3AP22483+OR+accession%3AA1SHI6+OR+accession%3AP56385+OR+accession%3AP69021+OR+accession%3AQ65DW9+OR+accession%3AQ5HE92+OR+accession%3AQ03QY3+OR+accession%3AB9DME9+OR+accession%3AQ04646+OR+accession%3AQ06450+OR+accession%3AB2TJZ5+OR+accession%3AP01397+OR+accession%3AQ53B54+OR+accession%3AP80156+OR+accession%3AP0DQQ1+OR+accession%3AP41015+OR+accession%3AP01389+OR+accession%3AA0RLA0+OR+accession%3AP40635+OR+accession%3AP40640+OR+accession%3AQ9CHF9+OR+accession%3AP01387+OR+accession%3AC0HJD3+OR+accession%3AP34073+OR+accession%3AP0DJY9+OR+accession%3AC3PM50+OR+accession%3AQ8R034+OR+accession%3AQ5M183+OR+accession%3AP60116+OR+accession%3AP60113+OR+accession%3AC4K0P2+OR+accession%3AQ196U2+OR+accession%3AQ8RGX5+OR+accession%3AQ4FPE8+OR+accession%3AB0K1W0+OR+accession%3AQ7VI80+OR+accession%3AQ4QP33+OR+accession%3AP83704+OR+accession%3AB8F7C3+OR+accession%3AC0HK59+OR+accession%3AB8DWS7+OR+accession%3AA0A2I8B353+OR+accession%3AC0QUD3+OR+accession%3AQ823D5+OR+accession%3AA4WRF7+OR+accession%3AP19872+OR+accession%3AB7IUK9+OR+accession%3AQ6G9Y1+OR+accession%3AQ2FZ51+OR+accession%3AQ5WFM5+OR+accession%3AQ12LV4+OR+accession%3AA8YUJ6+OR+accession%3AB8J187+OR+accession%3AA7Z4L2+OR+accession%3AQ9R0R4+OR+accession%3AA1B2X1+OR+accession%3AQ5HW23+OR+accession%3AA7ZF37+OR+accession%3AA7MZU3+OR+accession%3AP63439+OR+accession%3AB4T328+OR+accession%3AQ5LIS0+OR+accession%3AQ7VR21+OR+accession%3AQ2VBP2+OR+accession%3AQ2VBP0+OR+accession%3AA5VP30+OR+accession%3AQ9PRI1+OR+accession%3AQ8UGE2+OR+accession%3AQ8ZFT4+OR+accession%3AQ3K8L1+OR+accession%3AQ21K87+OR+accession%3AA9W067+OR+accession%3AQ6GZV7+OR+accession%3AQ9RA32+OR+accession%3AP63442+OR+accession%3AB2S447+OR+accession%3AA4XSS7+OR+accession%3AA7ZZ51+OR+accession%3AB7MJ81+OR+accession%3AP80918+OR+accession%3AA0M6S7+OR+accession%3AA7IK19+OR+accession%3AP0DPH5+OR+accession%3AB3PUU1+OR+accession%3AA0A2P1BSU3+OR+accession%3AA8HDK1+OR+accession%3AQ7W5I7+OR+accession%3AQ2NQ91+OR+accession%3AA4YT81+OR+accession%3AQ1C090+OR+accession%3AA9MXB1+OR+accession%3AA9MJR4+OR+accession%3AB7L883+OR+accession%3AA3PF99+OR+accession%3AP63446+OR+accession%3AQ9Z8P3+OR+accession%3AB1JYA3+OR+accession%3AP57119+OR+accession%3AQ1R4J5+OR+accession%3AA8A6K0+OR+accession%3AB6IN76+OR+accession%3AB1XZN7+OR+accession%3AQ7V4J6+OR+accession%3AP85938+OR+accession%3AP29697+OR+accession%3AQ057L2+OR+accession%3AA7HDZ1+OR+accession%3AA0KQY3+OR+accession%3AA4G6S9+OR+accession%3AQ5N3E5+OR+accession%3AQ498C5+OR+accession%3AQ96474+OR+accession%3AQ6ENW8+OR+accession%3AQ53B61+OR+accession%3AB1VKH8+OR+accession%3AP48086+OR+accession%3AQ0I7Q8+OR+accession%3AQ31RF5%29&fields=accession%2Creviewed%2Cprotein_name%2Clength%2Csequence%2Cec%2Cft_act_site%2Cft_binding%2Ccc_cofactor%2Cft_disulfid%2Cft_carbohyd%2Cft_lipid%2Cft_mod_res%2Cft_signal%2Cft_transit%2Cft_helix%2Cft_turn%2Cft_strand%2Cft_coiled%2Ccc_domain%2Cft_compbias%2Cft_domain%2Cft_motif%2Cft_region%2Cft_zn_fing&format=tsv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Use your processed IDs:\u001b[39;00m\n\u001b[32m     35\u001b[39m want = features_all[\u001b[33m\"\u001b[39m\u001b[33muniprot_id\u001b[39m\u001b[33m\"\u001b[39m].unique().tolist()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m ann  = \u001b[43mfetch_for_accessions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m ann.to_csv(\u001b[33m\"\u001b[39m\u001b[33muniprot_annotations_for_my_set.tsv\u001b[39m\u001b[33m\"\u001b[39m, sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(ann.shape, \u001b[33m\"\u001b[39m\u001b[33m→ uniprot_annotations_for_my_set.tsv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mfetch_for_accessions\u001b[39m\u001b[34m(accessions, chunk, pause, retries)\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r.status_code \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m429\u001b[39m,\u001b[32m500\u001b[39m,\u001b[32m502\u001b[39m,\u001b[32m503\u001b[39m,\u001b[32m504\u001b[39m):\n\u001b[32m     28\u001b[39m         time.sleep(pause * (\u001b[32m2\u001b[39m**attempt)); \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m frames.append(pd.read_csv(StringIO(r.text), sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m))\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pause: time.sleep(pause)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 400 Client Error: Bad Request for url: https://rest.uniprot.org/uniprotkb/search?query=%28accession%3AQ9GL23+OR+accession%3AQ6GZU6+OR+accession%3AP9WJG6+OR+accession%3AP18924+OR+accession%3AQ08076+OR+accession%3AP15450+OR+accession%3AP40643+OR+accession%3AP12697+OR+accession%3AO78683+OR+accession%3AQ9MJC0+OR+accession%3AQ9MFP7+OR+accession%3AP80302+OR+accession%3AO03168+OR+accession%3AO79405+OR+accession%3AP68528+OR+accession%3AP67806+OR+accession%3AQ7M460+OR+accession%3AP86420+OR+accession%3AP0DQH2+OR+accession%3AQ29149+OR+accession%3AO55724+OR+accession%3AQ8WSV2+OR+accession%3AP0DKS3+OR+accession%3AQ28280+OR+accession%3AQ3ZBI7+OR+accession%3AP20699+OR+accession%3AQ9STQ3+OR+accession%3AP69945+OR+accession%3AP07525+OR+accession%3AP01464+OR+accession%3AP60307+OR+accession%3AP86540+OR+accession%3AP25494+OR+accession%3AP01445+OR+accession%3AQ29005+OR+accession%3AP0CH80+OR+accession%3AP01459+OR+accession%3AP01469+OR+accession%3AP01422+OR+accession%3AQ8S2W4+OR+accession%3AC0HK65+OR+accession%3AQ91FM4+OR+accession%3AP68418+OR+accession%3AP25496+OR+accession%3AQ89183+OR+accession%3AA0A2I8B346+OR+accession%3AP83610+OR+accession%3AP10458+OR+accession%3AQ91FT2+OR+accession%3AQ8KRR5+OR+accession%3AP01408+OR+accession%3AP12100+OR+accession%3AP10808+OR+accession%3AP86098+OR+accession%3AC0HJT5+OR+accession%3AP85092+OR+accession%3AP01400+OR+accession%3AQ38PR8+OR+accession%3AQ5Y4Q6+OR+accession%3AP50017+OR+accession%3AQ9PSN1+OR+accession%3AP0A307+OR+accession%3AP17718+OR+accession%3AB7M0V4+OR+accession%3AB5REW4+OR+accession%3AQ35914+OR+accession%3AE1SFM6+OR+accession%3AB7NDM0+OR+accession%3AQ8W9N2+OR+accession%3AO21329+OR+accession%3AQ8XEK3+OR+accession%3AQ1CDW5+OR+accession%3AA8AQD6+OR+accession%3AQ3YX05+OR+accession%3AQ9MEI6+OR+accession%3AP11608+OR+accession%3AA6XMY1+OR+accession%3AQ30YK6+OR+accession%3AQ9WUC4+OR+accession%3AP81451+OR+accession%3AQ34571+OR+accession%3AQ3L6L9+OR+accession%3AP22483+OR+accession%3AA1SHI6+OR+accession%3AP56385+OR+accession%3AP69021+OR+accession%3AQ65DW9+OR+accession%3AQ5HE92+OR+accession%3AQ03QY3+OR+accession%3AB9DME9+OR+accession%3AQ04646+OR+accession%3AQ06450+OR+accession%3AB2TJZ5+OR+accession%3AP01397+OR+accession%3AQ53B54+OR+accession%3AP80156+OR+accession%3AP0DQQ1+OR+accession%3AP41015+OR+accession%3AP01389+OR+accession%3AA0RLA0+OR+accession%3AP40635+OR+accession%3AP40640+OR+accession%3AQ9CHF9+OR+accession%3AP01387+OR+accession%3AC0HJD3+OR+accession%3AP34073+OR+accession%3AP0DJY9+OR+accession%3AC3PM50+OR+accession%3AQ8R034+OR+accession%3AQ5M183+OR+accession%3AP60116+OR+accession%3AP60113+OR+accession%3AC4K0P2+OR+accession%3AQ196U2+OR+accession%3AQ8RGX5+OR+accession%3AQ4FPE8+OR+accession%3AB0K1W0+OR+accession%3AQ7VI80+OR+accession%3AQ4QP33+OR+accession%3AP83704+OR+accession%3AB8F7C3+OR+accession%3AC0HK59+OR+accession%3AB8DWS7+OR+accession%3AA0A2I8B353+OR+accession%3AC0QUD3+OR+accession%3AQ823D5+OR+accession%3AA4WRF7+OR+accession%3AP19872+OR+accession%3AB7IUK9+OR+accession%3AQ6G9Y1+OR+accession%3AQ2FZ51+OR+accession%3AQ5WFM5+OR+accession%3AQ12LV4+OR+accession%3AA8YUJ6+OR+accession%3AB8J187+OR+accession%3AA7Z4L2+OR+accession%3AQ9R0R4+OR+accession%3AA1B2X1+OR+accession%3AQ5HW23+OR+accession%3AA7ZF37+OR+accession%3AA7MZU3+OR+accession%3AP63439+OR+accession%3AB4T328+OR+accession%3AQ5LIS0+OR+accession%3AQ7VR21+OR+accession%3AQ2VBP2+OR+accession%3AQ2VBP0+OR+accession%3AA5VP30+OR+accession%3AQ9PRI1+OR+accession%3AQ8UGE2+OR+accession%3AQ8ZFT4+OR+accession%3AQ3K8L1+OR+accession%3AQ21K87+OR+accession%3AA9W067+OR+accession%3AQ6GZV7+OR+accession%3AQ9RA32+OR+accession%3AP63442+OR+accession%3AB2S447+OR+accession%3AA4XSS7+OR+accession%3AA7ZZ51+OR+accession%3AB7MJ81+OR+accession%3AP80918+OR+accession%3AA0M6S7+OR+accession%3AA7IK19+OR+accession%3AP0DPH5+OR+accession%3AB3PUU1+OR+accession%3AA0A2P1BSU3+OR+accession%3AA8HDK1+OR+accession%3AQ7W5I7+OR+accession%3AQ2NQ91+OR+accession%3AA4YT81+OR+accession%3AQ1C090+OR+accession%3AA9MXB1+OR+accession%3AA9MJR4+OR+accession%3AB7L883+OR+accession%3AA3PF99+OR+accession%3AP63446+OR+accession%3AQ9Z8P3+OR+accession%3AB1JYA3+OR+accession%3AP57119+OR+accession%3AQ1R4J5+OR+accession%3AA8A6K0+OR+accession%3AB6IN76+OR+accession%3AB1XZN7+OR+accession%3AQ7V4J6+OR+accession%3AP85938+OR+accession%3AP29697+OR+accession%3AQ057L2+OR+accession%3AA7HDZ1+OR+accession%3AA0KQY3+OR+accession%3AA4G6S9+OR+accession%3AQ5N3E5+OR+accession%3AQ498C5+OR+accession%3AQ96474+OR+accession%3AQ6ENW8+OR+accession%3AQ53B61+OR+accession%3AB1VKH8+OR+accession%3AP48086+OR+accession%3AQ0I7Q8+OR+accession%3AQ31RF5%29&fields=accession%2Creviewed%2Cprotein_name%2Clength%2Csequence%2Cec%2Cft_act_site%2Cft_binding%2Ccc_cofactor%2Cft_disulfid%2Cft_carbohyd%2Cft_lipid%2Cft_mod_res%2Cft_signal%2Cft_transit%2Cft_helix%2Cft_turn%2Cft_strand%2Cft_coiled%2Ccc_domain%2Cft_compbias%2Cft_domain%2Cft_motif%2Cft_region%2Cft_zn_fing&format=tsv"
     ]
    }
   ],
   "source": [
    "import time, requests, pandas as pd\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "SEARCH = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "fields = (\n",
    "    \"accession,reviewed,protein_name,length,sequence,\"\n",
    "    \"ec,ft_act_site,ft_binding,cc_cofactor,ft_disulfid,\"\n",
    "    \"ft_carbohyd,ft_lipid,ft_mod_res,ft_signal,ft_transit,\"\n",
    "    \"ft_helix,ft_turn,ft_strand,ft_coiled,cc_domain,\"\n",
    "    \"ft_compbias,ft_domain,ft_motif,ft_region,ft_zn_fing\"\n",
    ")\n",
    "\n",
    "def fetch_for_accessions(accessions, chunk=200, pause=0.2, retries=5):\n",
    "    accs = list(dict.fromkeys(accessions))\n",
    "    frames = []\n",
    "    for i in range(0, len(accs), chunk):\n",
    "        batch = accs[i:i+chunk]\n",
    "        params = {\n",
    "            \"query\": \"(\" + \" OR \".join(f\"accession:{a}\" for a in batch) + \")\",\n",
    "            \"fields\": fields,\n",
    "            \"format\": \"tsv\",\n",
    "        }\n",
    "        for attempt in range(retries):\n",
    "            r = requests.get(SEARCH, params=params, timeout=60)\n",
    "            if r.status_code == 200: break\n",
    "            if r.status_code in (429,500,502,503,504):\n",
    "                time.sleep(pause * (2**attempt)); continue\n",
    "            r.raise_for_status()\n",
    "        frames.append(pd.read_csv(StringIO(r.text), sep=\"\\t\"))\n",
    "        if pause: time.sleep(pause)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# Use your processed IDs:\n",
    "want = features_all[\"uniprot_id\"].unique().tolist()\n",
    "ann  = fetch_for_accessions(want)\n",
    "ann.to_csv(\"uniprot_annotations_for_my_set.tsv\", sep=\"\\t\", index=False)\n",
    "print(ann.shape, \"→ uniprot_annotations_for_my_set.tsv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Swiss-Prot (reviewed:true) annotations…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197MB [05:27, 600kB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → /home/ec2-user/InterPLM/data/uniprot/swissprot_annotations.tsv.gz  (size: 196.6 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Where to save\n",
    "uniprot_dir = Path(\"/home/ec2-user/InterPLM/data/uniprot\")\n",
    "uniprot_dir.mkdir(parents=True, exist_ok=True)\n",
    "annotations_path = uniprot_dir / \"swissprot_annotations.tsv.gz\"\n",
    "\n",
    "# Fields that match your parser's column names\n",
    "fields = (\n",
    "    \"accession,reviewed,protein_name,length,sequence,\"\n",
    "    \"ec,ft_act_site,ft_binding,cc_cofactor,ft_disulfid,\"\n",
    "    \"ft_carbohyd,ft_lipid,ft_mod_res,ft_signal,ft_transit,\"\n",
    "    \"ft_helix,ft_turn,ft_strand,ft_coiled,cc_domain,\"\n",
    "    \"ft_compbias,ft_domain,ft_motif,ft_region,ft_zn_fing\"\n",
    ")\n",
    "\n",
    "url = (\n",
    "    \"https://rest.uniprot.org/uniprotkb/stream\"\n",
    "    f\"?format=tsv&compressed=true&fields={fields}\"\n",
    "    \"&query=(reviewed:true)\"\n",
    ")\n",
    "\n",
    "def download_stream(url: str, out_path: Path, chunk_size: int = 1 << 20):\n",
    "    if out_path.exists():\n",
    "        print(f\"Annotations already present at: {out_path}  (size: {out_path.stat().st_size/1e6:.1f} MB)\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading Swiss-Prot (reviewed:true) annotations…\")\n",
    "    with requests.get(url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"Content-Length\", 0))\n",
    "        pbar = tqdm(total=total, unit=\"B\", unit_scale=True)\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        pbar.close()\n",
    "    print(f\"Saved → {out_path}  (size: {out_path.stat().st_size/1e6:.1f} MB)\")\n",
    "\n",
    "download_stream(url, annotations_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://rest.uniprot.org/uniprotkb/search?query=(accession:Q9GL23 OR accession:Q6GZU6 OR accession:P9WJG6 OR accession:P18924 OR accession:Q08076 OR accession:P15450 OR accession:P40643 OR accession:P12697 OR accession:O78683 OR accession:Q9MJC0 OR accession:Q9MFP7 OR accession:P80302 OR accession:O03168 OR accession:O79405 OR accession:P68528 OR accession:P67806 OR accession:Q7M460 OR accession:P86420 OR accession:P0DQH2 OR accession:Q29149)&fields=accession,reviewed,protein_name,length&format=tsv\n"
     ]
    },
    {
     "ename": "InvalidURL",
     "evalue": "URL can't contain control characters. '/uniprotkb/search?query=(accession:Q9GL23 OR accession:Q6GZU6 OR accession:P9WJG6 OR accession:P18924 OR accession:Q08076 OR accession:P15450 OR accession:P40643 OR accession:P12697 OR accession:O78683 OR accession:Q9MJC0 OR accession:Q9MFP7 OR accession:P80302 OR accession:O03168 OR accession:O79405 OR accession:P68528 OR accession:P67806 OR accession:Q7M460 OR accession:P86420 OR accession:P0DQH2 OR accession:Q29149)&fields=accession,reviewed,protein_name,length&format=tsv' (found at least ' ')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidURL\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      5\u001b[39m url = (\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://rest.uniprot.org/uniprotkb/search\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m?query=(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m OR \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33maccession:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mids)\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m&fields=accession,reviewed,protein_name,length\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m&format=tsv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(url)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/urllib/request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/urllib/request.py:519\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    516\u001b[39m     req = meth(req)\n\u001b[32m    518\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    522\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/urllib/request.py:536\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    535\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    539\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/urllib/request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/urllib/request.py:1391\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/urllib/request.py:1348\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m         \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/http/client.py:1303\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body=\u001b[38;5;28;01mNone\u001b[39;00m, headers={}, *,\n\u001b[32m   1301\u001b[39m             encode_chunked=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1303\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/http/client.py:1314\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33maccept-encoding\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m header_names:\n\u001b[32m   1312\u001b[39m     skips[\u001b[33m'\u001b[39m\u001b[33mskip_accept_encoding\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mputrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mskips\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[38;5;66;03m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[39;00m\n\u001b[32m   1317\u001b[39m \u001b[38;5;66;03m# the caller passes encode_chunked=True or the following\u001b[39;00m\n\u001b[32m   1318\u001b[39m \u001b[38;5;66;03m# conditions hold:\u001b[39;00m\n\u001b[32m   1319\u001b[39m \u001b[38;5;66;03m# 1. content-length has not been explicitly set\u001b[39;00m\n\u001b[32m   1320\u001b[39m \u001b[38;5;66;03m# 2. the body is a file or iterable, but not a str or bytes-like\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;66;03m# 3. Transfer-Encoding has NOT been explicitly set by the caller\u001b[39;00m\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcontent-length\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m header_names:\n\u001b[32m   1324\u001b[39m     \u001b[38;5;66;03m# only chunk body if not explicitly set for backwards\u001b[39;00m\n\u001b[32m   1325\u001b[39m     \u001b[38;5;66;03m# compatibility, assuming the client code is already handling the\u001b[39;00m\n\u001b[32m   1326\u001b[39m     \u001b[38;5;66;03m# chunking\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/http/client.py:1148\u001b[39m, in \u001b[36mHTTPConnection.putrequest\u001b[39m\u001b[34m(self, method, url, skip_host, skip_accept_encoding)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;28mself\u001b[39m._method = method\n\u001b[32m   1147\u001b[39m url = url \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1150\u001b[39m request = \u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m % (method, url, \u001b[38;5;28mself\u001b[39m._http_vsn_str)\n\u001b[32m   1152\u001b[39m \u001b[38;5;28mself\u001b[39m._output(\u001b[38;5;28mself\u001b[39m._encode_request(request))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/http/client.py:1248\u001b[39m, in \u001b[36mHTTPConnection._validate_path\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m   1246\u001b[39m match = _contains_disallowed_url_pchar_re.search(url)\n\u001b[32m   1247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[32m-> \u001b[39m\u001b[32m1248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mURL can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt contain control characters. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1249\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(found at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch.group()\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mInvalidURL\u001b[39m: URL can't contain control characters. '/uniprotkb/search?query=(accession:Q9GL23 OR accession:Q6GZU6 OR accession:P9WJG6 OR accession:P18924 OR accession:Q08076 OR accession:P15450 OR accession:P40643 OR accession:P12697 OR accession:O78683 OR accession:Q9MJC0 OR accession:Q9MFP7 OR accession:P80302 OR accession:O03168 OR accession:O79405 OR accession:P68528 OR accession:P67806 OR accession:Q7M460 OR accession:P86420 OR accession:P0DQH2 OR accession:Q29149)&fields=accession,reviewed,protein_name,length&format=tsv' (found at least ' ')"
     ]
    }
   ],
   "source": [
    "import requests, pandas as pd\n",
    "\n",
    "ids = features_all['uniprot_id'].head(20).tolist()\n",
    "query = \" OR \".join(ids)  # accession:Q9GL23 OR accession:Q6GZU6 ...\n",
    "url = (\n",
    "    \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "    f\"?query=({ ' OR '.join(f'accession:{x}' for x in ids) })\"\n",
    "    \"&fields=accession,reviewed,protein_name,length\"\n",
    "    \"&format=tsv\"\n",
    ")\n",
    "print(url)\n",
    "print(pd.read_csv(url, sep=\"\\t\").head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Rich Protein Annotations\n",
    "\n",
    "Extract and process UniProt annotations to create binary concept labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_uniprot_annotations(annotations_path: Path):\n",
    "    \"\"\"\n",
    "    Parse UniProt annotations and create binary concept labels.\n",
    "    \"\"\"\n",
    "    print(f\"Loading annotations from {annotations_path}...\")\n",
    "    \n",
    "    # Read the TSV file\n",
    "    annotations_df = pd.read_csv(annotations_path, sep='\\t', compression='gzip')\n",
    "    \n",
    "    print(f\"Loaded {len(annotations_df)} protein annotations\")\n",
    "    print(f\"Available columns: {list(annotations_df.columns)}\")\n",
    "    \n",
    "    # Filter annotations to match our sequence subset\n",
    "    print(f\"\\\\nFiltering annotations to match processed sequences...\")\n",
    "    processed_ids = set(sequences_df['uniprot_id'])\n",
    "    annotations_df = annotations_df[annotations_df['Entry'].isin(processed_ids)]\n",
    "    print(f\"After filtering: {len(annotations_df)} annotations remain\")\n",
    "    \n",
    "    if len(annotations_df) == 0:\n",
    "        print(\"ERROR: No matching annotations found!\")\n",
    "        print(\"This suggests the UniProt IDs don't match between FASTA and annotations.\")\n",
    "        print(\"Checking a few examples...\")\n",
    "        print(f\"FASTA IDs (first 5): {list(sequences_df['uniprot_id'].head())}\")\n",
    "        # Try loading a few annotation entries to see format\n",
    "        temp_df = pd.read_csv(annotations_path, sep='\\\\t', compression='gzip', nrows=5)\n",
    "        print(f\"Annotation IDs (first 5): {list(temp_df['Entry'])}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Create binary concept labels\n",
    "    concepts = {}\n",
    "    \n",
    "    # Structural features\n",
    "    concepts['has_signal_peptide'] = ~annotations_df['Signal peptide'].isna()\n",
    "    concepts['has_disulfide_bond'] = ~annotations_df['Disulfide bond'].isna()\n",
    "    concepts['has_helix'] = ~annotations_df['Helix'].isna()\n",
    "    concepts['has_strand'] = ~annotations_df['Beta strand'].isna()\n",
    "    concepts['has_turn'] = ~annotations_df['Turn'].isna()\n",
    "    concepts['has_coiled_coil'] = ~annotations_df['Coiled coil'].isna()\n",
    "    \n",
    "    # Post-translational modifications\n",
    "    concepts['has_glycosylation'] = ~annotations_df['Glycosylation'].isna()\n",
    "    concepts['has_lipidation'] = ~annotations_df['Lipidation'].isna()\n",
    "    concepts['has_modification'] = ~annotations_df['Modified residue'].isna()\n",
    "    \n",
    "    # Functional features\n",
    "    concepts['has_active_site'] = ~annotations_df['Active site'].isna()\n",
    "    concepts['has_binding_site'] = ~annotations_df['Binding site'].isna()\n",
    "    concepts['has_enzyme_activity'] = ~annotations_df['EC number'].isna()\n",
    "    \n",
    "    # Sequence features\n",
    "    concepts['has_domain'] = ~annotations_df['Domain [FT]'].isna()\n",
    "    concepts['has_motif'] = ~annotations_df['Motif'].isna()\n",
    "    concepts['has_region'] = ~annotations_df['Region'].isna()\n",
    "    concepts['has_zinc_finger'] = ~annotations_df['Zinc finger'].isna()\n",
    "    concepts['has_compositional_bias'] = ~annotations_df['Compositional bias'].isna()\n",
    "    \n",
    "    # Length-based categories\n",
    "    concepts['short_protein'] = annotations_df['Length'] < 150\n",
    "    concepts['medium_protein'] = (annotations_df['Length'] >= 150) & (annotations_df['Length'] < 400)\n",
    "    concepts['long_protein'] = annotations_df['Length'] >= 400\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    concepts_df = pd.DataFrame(concepts, index=annotations_df['Entry'])\n",
    "    \n",
    "    # Add sequence information\n",
    "    concepts_df['sequence'] = annotations_df['Sequence'].values\n",
    "    concepts_df['length'] = annotations_df['Length'].values\n",
    "    concepts_df['protein_name'] = annotations_df['Protein names'].values\n",
    "    \n",
    "    return concepts_df, annotations_df\n",
    "\n",
    "# Parse annotations\n",
    "concepts_df, raw_annotations = parse_uniprot_annotations(annotations_path)\n",
    "\n",
    "if concepts_df is not None:\n",
    "    print(f\"\\\\nCreated {len(concepts_df)} concept annotations\")\n",
    "    print(f\"Available concepts: {[col for col in concepts_df.columns if col not in ['sequence', 'length', 'protein_name']]}\")\n",
    "    \n",
    "    # Show concept statistics\n",
    "    concept_cols = [col for col in concepts_df.columns if col not in ['sequence', 'length', 'protein_name']]\n",
    "    concept_stats = concepts_df[concept_cols].sum().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\\\n=== Concept Statistics ===\")\n",
    "    for concept, count in concept_stats.head(15).items():\n",
    "        pct = 100 * count / len(concepts_df)\n",
    "        print(f\"{concept:25s}: {count:5d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Visualize concept distribution\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    concept_stats.head(15).plot(kind='barh')\n",
    "    plt.xlabel('Number of Proteins')\n",
    "    plt.title('Distribution of Protein Concepts')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to parse annotations - please check the data sources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Match Features with Annotations\n",
    "\n",
    "Align the extracted SAE features with the protein annotations for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features with concepts by UniProt ID\n",
    "feature_ids = set(features_df['uniprot_id'])\n",
    "concept_ids = set(concepts_df.index)\n",
    "common_ids = feature_ids.intersection(concept_ids)\n",
    "\n",
    "print(f\"Features extracted for: {len(feature_ids)} proteins\")\n",
    "print(f\"Concepts available for: {len(concept_ids)} proteins\")\n",
    "print(f\"Common proteins: {len(common_ids)} proteins\")\n",
    "\n",
    "if len(common_ids) < 500:\n",
    "    print(\"\\nWarning: Overlap between features and concepts could be higher.\")\n",
    "    print(f\"Current overlap: {len(common_ids)} proteins\")\n",
    "    if len(common_ids) < 100:\n",
    "        print(\"Very low overlap - this might limit analysis quality.\")\n",
    "else:\n",
    "    print(f\"\\nGood overlap: {len(common_ids)} proteins for analysis\")\n",
    "\n",
    "# Create matched dataset (convert set to list for pandas indexing)\n",
    "matched_features = features_df[features_df['uniprot_id'].isin(common_ids)].set_index('uniprot_id')\n",
    "matched_concepts = concepts_df.loc[list(common_ids)]\n",
    "\n",
    "# Align the data\n",
    "aligned_data = matched_features.join(matched_concepts, how='inner')\n",
    "\n",
    "print(f\"\\nAligned dataset: {len(aligned_data)} proteins\")\n",
    "print(f\"Feature dimension: {aligned_data['features'].iloc[0].shape[0]}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.vstack(aligned_data['features'].values)\n",
    "feature_dim = X.shape[1] // 2  # Half are mean, half are max pooled\n",
    "\n",
    "# Split into mean and max features (following original notebook)\n",
    "X_mean = X[:, :feature_dim]\n",
    "X_max = X[:, feature_dim:]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Mean features: {X_mean.shape}, Max features: {X_max.shape}\")\n",
    "\n",
    "# Get concept labels\n",
    "concept_cols = [col for col in aligned_data.columns \n",
    "                if col not in ['features', 'length', 'max_activation', 'n_active_features', \n",
    "                               'reconstruction_mse', 'sequence', 'protein_name']]\n",
    "Y = aligned_data[concept_cols].astype(int)\n",
    "\n",
    "print(f\"\\nConcept matrix shape: {Y.shape}\")\n",
    "print(f\"Available concepts for analysis: {len(concept_cols)}\")\n",
    "\n",
    "# Show concept statistics for the aligned dataset\n",
    "print(\"\\n=== Concept Statistics (Aligned Data) ===\")\n",
    "concept_stats_aligned = Y.sum().sort_values(ascending=False)\n",
    "for concept, count in concept_stats_aligned.head(15).items():\n",
    "    pct = 100 * count / len(Y)\n",
    "    print(f\"{concept:25s}: {count:5d} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SAE Feature Mining and Analysis\n",
    "\n",
    "Analyze which SAE features are associated with different protein concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_concept_associations(X_features: np.ndarray, \n",
    "                                        Y_concepts: pd.DataFrame,\n",
    "                                        min_concept_instances: int = 5):\n",
    "    \"\"\"\n",
    "    Compute associations between SAE features and protein concepts using AUC.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    n_features = X_features.shape[1]\n",
    "    \n",
    "    print(f\"Computing associations for {n_features} features and {len(Y_concepts.columns)} concepts...\")\n",
    "    \n",
    "    for concept in Y_concepts.columns:\n",
    "        y = Y_concepts[concept].values\n",
    "        \n",
    "        # Skip concepts with too few positive examples\n",
    "        if y.sum() < min_concept_instances or (len(y) - y.sum()) < min_concept_instances:\n",
    "            print(f\"  Skipping {concept}: only {y.sum()}/{len(y)} positive examples (need at least {min_concept_instances})\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Processing {concept}: {y.sum()}/{len(y)} positive examples\")\n",
    "        \n",
    "        aucs = []\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X_features[:, feature_idx]\n",
    "            \n",
    "            # Skip features with no variation\n",
    "            if len(np.unique(feature_values)) < 2:\n",
    "                aucs.append(0.5)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                auc = roc_auc_score(y, feature_values)\n",
    "                aucs.append(auc)\n",
    "            except ValueError:\n",
    "                aucs.append(0.5)\n",
    "        \n",
    "        # Find top features for this concept\n",
    "        aucs = np.array(aucs)\n",
    "        top_indices = np.argsort(np.abs(aucs - 0.5))[::-1][:20]  # Top 20 by deviation from 0.5\n",
    "        \n",
    "        results[concept] = {\n",
    "            'aucs': aucs,\n",
    "            'top_features': [(int(idx), aucs[idx]) for idx in top_indices],\n",
    "            'n_positive': int(y.sum()),\n",
    "            'n_total': len(y)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Only proceed if we have valid concept data\n",
    "if concepts_df is not None and len(aligned_data) > 0:\n",
    "    # Compute associations using mean-pooled features\n",
    "    associations = compute_feature_concept_associations(X_mean, Y, min_concept_instances=3)\n",
    "    \n",
    "    print(f\"\\\\n=== Feature-Concept Associations ===\")\n",
    "    print(f\"Analyzed {len(associations)} concepts\")\n",
    "    \n",
    "    # Display top associations for each concept\n",
    "    for concept, data in associations.items():\n",
    "        top_features = data['top_features'][:5]\n",
    "        print(f\"\\\\n{concept} ({data['n_positive']}/{data['n_total']} examples):\")\n",
    "        for feature_idx, auc in top_features:\n",
    "            print(f\"  Feature {feature_idx:4d}: AUC = {auc:.3f}\")\n",
    "else:\n",
    "    print(\"Cannot compute associations - missing concept data or aligned dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Feature Patterns\n",
    "\n",
    "Create visualizations to understand what the SAE features have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_features_heatmap(associations: dict, X_features: np.ndarray, Y_concepts: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing the top features for each concept.\n",
    "    \"\"\"\n",
    "    # Collect top features across all concepts\n",
    "    all_top_features = set()\n",
    "    for concept_data in associations.values():\n",
    "        top_features = [f[0] for f in concept_data['top_features'][:10]]\n",
    "        all_top_features.update(top_features)\n",
    "    \n",
    "    all_top_features = sorted(list(all_top_features))\n",
    "    \n",
    "    # Create AUC matrix\n",
    "    auc_matrix = np.zeros((len(associations), len(all_top_features)))\n",
    "    concept_names = list(associations.keys())\n",
    "    \n",
    "    for i, concept in enumerate(concept_names):\n",
    "        aucs = associations[concept]['aucs']\n",
    "        for j, feature_idx in enumerate(all_top_features):\n",
    "            auc_matrix[i, j] = aucs[feature_idx]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(auc_matrix, \n",
    "                xticklabels=[f\"F{f}\" for f in all_top_features],\n",
    "                yticklabels=concept_names,\n",
    "                cmap='RdBu_r', center=0.5, \n",
    "                cbar_kws={'label': 'AUC Score'})\n",
    "    plt.title('Feature-Concept Association Heatmap')\n",
    "    plt.xlabel('SAE Features')\n",
    "    plt.ylabel('Protein Concepts')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_matrix, concept_names, all_top_features\n",
    "\n",
    "def plot_feature_distributions(feature_idx: int, concept: str, \n",
    "                              X_features: np.ndarray, Y_concepts: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot the distribution of a specific feature for positive vs negative examples of a concept.\n",
    "    \"\"\"\n",
    "    y = Y_concepts[concept].values\n",
    "    feature_values = X_features[:, feature_idx]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot distributions\n",
    "    plt.hist(feature_values[y == 0], bins=30, alpha=0.7, label=f'Negative ({(y==0).sum()})', \n",
    "             density=True, color='blue')\n",
    "    plt.hist(feature_values[y == 1], bins=30, alpha=0.7, label=f'Positive ({(y==1).sum()})', \n",
    "             density=True, color='red')\n",
    "    \n",
    "    plt.xlabel(f'Feature {feature_idx} Activation')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Feature {feature_idx} Distribution for {concept}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add AUC score\n",
    "    auc = roc_auc_score(y, feature_values)\n",
    "    plt.text(0.05, 0.95, f'AUC = {auc:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Only create visualizations if we have associations\n",
    "if 'associations' in locals() and len(associations) > 0:\n",
    "    # Create heatmap\n",
    "    auc_matrix, concept_names, top_features = plot_top_features_heatmap(associations, X_mean, Y)\n",
    "    \n",
    "    # Plot distributions for a few interesting feature-concept pairs\n",
    "    print(\"\\\\n=== Feature Distribution Examples ===\")\n",
    "    \n",
    "    # Find the most discriminative feature-concept pairs\n",
    "    best_pairs = []\n",
    "    for i, concept in enumerate(concept_names):\n",
    "        for j, feature_idx in enumerate(top_features):\n",
    "            auc = auc_matrix[i, j]\n",
    "            if abs(auc - 0.5) > 0.2:  # Strong association (lowered threshold)\n",
    "                best_pairs.append((feature_idx, concept, auc))\n",
    "    \n",
    "    best_pairs.sort(key=lambda x: abs(x[2] - 0.5), reverse=True)\n",
    "    \n",
    "    # Plot top 3 most discriminative pairs\n",
    "    if len(best_pairs) > 0:\n",
    "        for feature_idx, concept, auc in best_pairs[:3]:\n",
    "            print(f\"\\\\nPlotting Feature {feature_idx} vs {concept} (AUC = {auc:.3f})\")\n",
    "            plot_feature_distributions(feature_idx, concept, X_mean, Y)\n",
    "    else:\n",
    "        print(\"No strongly discriminative feature-concept pairs found (AUC deviation > 0.2)\")\n",
    "else:\n",
    "    print(\"Skipping visualizations - no associations computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sequence-Level Analysis\n",
    "\n",
    "Examine individual sequences to understand how features activate across protein regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sequence_features(uniprot_id: str, top_k_features: int = 5):\n",
    "    \"\"\"\n",
    "    Analyze feature activations for a specific protein sequence.\n",
    "    \"\"\"\n",
    "    # Get sequence data\n",
    "    seq_data = aligned_data.loc[uniprot_id]\n",
    "    sequence = seq_data['sequence']\n",
    "    \n",
    "    print(f\"\\n=== Analysis for {uniprot_id} ===\")\n",
    "    print(f\"Protein name: {seq_data['protein_name']}\")\n",
    "    print(f\"Length: {len(sequence)} residues\")\n",
    "    \n",
    "    # Show active concepts\n",
    "    active_concepts = [col for col in concept_cols if seq_data[col] == 1]\n",
    "    print(f\"Active concepts: {', '.join(active_concepts) if active_concepts else 'None'}\")\n",
    "    \n",
    "    # Extract per-residue features\n",
    "    hidden_states = extract_esm_features(sequence)\n",
    "    sae_features, _, _ = extract_sae_features(hidden_states)\n",
    "    \n",
    "    # Find top-activating features\n",
    "    mean_activations = sae_features.mean(0)\n",
    "    top_features_idx = torch.topk(mean_activations, top_k_features).indices\n",
    "    \n",
    "    print(f\"\\nTop {top_k_features} features by mean activation:\")\n",
    "    for i, feature_idx in enumerate(top_features_idx):\n",
    "        mean_act = mean_activations[feature_idx].item()\n",
    "        max_act = sae_features[:, feature_idx].max().item()\n",
    "        print(f\"  {i+1}. Feature {feature_idx:4d}: mean={mean_act:.3f}, max={max_act:.3f}\")\n",
    "    \n",
    "    # Plot feature activations along sequence\n",
    "    fig, axes = plt.subplots(top_k_features, 1, figsize=(12, 2*top_k_features))\n",
    "    if top_k_features == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, feature_idx in enumerate(top_features_idx):\n",
    "        activations = sae_features[:, feature_idx].cpu().numpy()\n",
    "        \n",
    "        axes[i].plot(activations, linewidth=2)\n",
    "        axes[i].set_ylabel(f'Feature {feature_idx}')\n",
    "        axes[i].set_title(f'Feature {feature_idx} Activation (mean={mean_activations[feature_idx]:.3f})')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight high-activation regions\n",
    "        threshold = activations.mean() + 2 * activations.std()\n",
    "        high_regions = activations > threshold\n",
    "        if high_regions.any():\n",
    "            axes[i].fill_between(range(len(activations)), 0, activations, \n",
    "                               where=high_regions, alpha=0.3, color='red')\n",
    "    \n",
    "    axes[-1].set_xlabel('Residue Position')\n",
    "    plt.suptitle(f'SAE Feature Activations: {uniprot_id}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sae_features, top_features_idx\n",
    "\n",
    "# Analyze a few interesting examples\n",
    "print(\"=== Sequence-Level Feature Analysis ===\")\n",
    "\n",
    "# Find some proteins with different concept patterns\n",
    "examples = []\n",
    "\n",
    "# Get an example with signal peptide\n",
    "if 'has_signal_peptide' in Y.columns:\n",
    "    signal_proteins = Y[Y['has_signal_peptide'] == 1].index\n",
    "    if len(signal_proteins) > 0:\n",
    "        examples.append(signal_proteins[0])\n",
    "\n",
    "# Get an example with enzyme activity\n",
    "if 'has_enzyme_activity' in Y.columns:\n",
    "    enzyme_proteins = Y[Y['has_enzyme_activity'] == 1].index\n",
    "    if len(enzyme_proteins) > 0:\n",
    "        examples.append(enzyme_proteins[0])\n",
    "\n",
    "# Get a structural protein example\n",
    "if 'has_domain' in Y.columns:\n",
    "    domain_proteins = Y[Y['has_domain'] == 1].index\n",
    "    if len(domain_proteins) > 0:\n",
    "        examples.append(domain_proteins[0])\n",
    "\n",
    "# Remove duplicates and limit to 3 examples\n",
    "examples = list(set(examples))[:3]\n",
    "\n",
    "for example_id in examples:\n",
    "    try:\n",
    "        analyze_sequence_features(example_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {example_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Interpretation and Biological Insights\n",
    "\n",
    "Summarize findings and provide biological interpretation of discovered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_feature_analysis(associations: dict, min_auc_deviation: float = 0.2):\n",
    "    \"\"\"\n",
    "    Summarize the key findings from feature-concept associations.\n",
    "    \"\"\"\n",
    "    print(\"=== SAE Feature Analysis Summary ===\")\n",
    "    print(f\"\\nAnalyzed {len(associations)} protein concepts\")\n",
    "    \n",
    "    # Count strong associations\n",
    "    strong_associations = []\n",
    "    all_features = set()\n",
    "    \n",
    "    for concept, data in associations.items():\n",
    "        for feature_idx, auc in data['top_features']:\n",
    "            if abs(auc - 0.5) > min_auc_deviation:\n",
    "                strong_associations.append((concept, feature_idx, auc))\n",
    "                all_features.add(feature_idx)\n",
    "    \n",
    "    print(f\"Found {len(strong_associations)} strong feature-concept associations\")\n",
    "    print(f\"Involving {len(all_features)} distinct SAE features\")\n",
    "    \n",
    "    # Group by concept type\n",
    "    structural_concepts = []\n",
    "    functional_concepts = []\n",
    "    modification_concepts = []\n",
    "    \n",
    "    for concept, feature_idx, auc in strong_associations:\n",
    "        if any(keyword in concept.lower() for keyword in ['helix', 'strand', 'coil', 'domain', 'structure']):\n",
    "            structural_concepts.append((concept, feature_idx, auc))\n",
    "        elif any(keyword in concept.lower() for keyword in ['enzyme', 'binding', 'active', 'function']):\n",
    "            functional_concepts.append((concept, feature_idx, auc))\n",
    "        elif any(keyword in concept.lower() for keyword in ['glyco', 'lipid', 'modification', 'signal']):\n",
    "            modification_concepts.append((concept, feature_idx, auc))\n",
    "    \n",
    "    print(f\"\\n=== Associations by Category ===\")\n",
    "    print(f\"Structural features: {len(structural_concepts)}\")\n",
    "    print(f\"Functional features: {len(functional_concepts)}\")\n",
    "    print(f\"Modification features: {len(modification_concepts)}\")\n",
    "    \n",
    "    # Display top associations in each category\n",
    "    categories = [\n",
    "        (\"Structural\", structural_concepts),\n",
    "        (\"Functional\", functional_concepts),\n",
    "        (\"Modification\", modification_concepts)\n",
    "    ]\n",
    "    \n",
    "    for category_name, concepts in categories:\n",
    "        if concepts:\n",
    "            print(f\"\\n--- {category_name} Features ---\")\n",
    "            # Sort by AUC deviation from 0.5\n",
    "            concepts.sort(key=lambda x: abs(x[2] - 0.5), reverse=True)\n",
    "            for i, (concept, feature_idx, auc) in enumerate(concepts[:5]):\n",
    "                direction = \"activates for\" if auc > 0.5 else \"suppresses for\"\n",
    "                print(f\"  {i+1}. Feature {feature_idx:4d} {direction} {concept} (AUC={auc:.3f})\")\n",
    "    \n",
    "    return strong_associations, all_features\n",
    "\n",
    "def create_feature_summary_table(associations: dict, aligned_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create a summary table of the most important features.\n",
    "    \"\"\"\n",
    "    # Collect all feature scores\n",
    "    feature_scores = defaultdict(list)\n",
    "    \n",
    "    for concept, data in associations.items():\n",
    "        for feature_idx, auc in data['top_features'][:10]:\n",
    "            feature_scores[feature_idx].append({\n",
    "                'concept': concept,\n",
    "                'auc': auc,\n",
    "                'deviation': abs(auc - 0.5)\n",
    "            })\n",
    "    \n",
    "    # Summarize each feature\n",
    "    feature_summary = []\n",
    "    \n",
    "    for feature_idx, scores in feature_scores.items():\n",
    "        scores.sort(key=lambda x: x['deviation'], reverse=True)\n",
    "        \n",
    "        # Get top associated concepts\n",
    "        top_concepts = scores[:3]\n",
    "        max_deviation = max(score['deviation'] for score in scores)\n",
    "        \n",
    "        # Count how often this feature is active\n",
    "        feature_activations = X_mean[:, feature_idx]\n",
    "        n_active = (feature_activations > 0.1).sum()\n",
    "        mean_activation = feature_activations.mean()\n",
    "        \n",
    "        feature_summary.append({\n",
    "            'feature_idx': feature_idx,\n",
    "            'max_auc_deviation': max_deviation,\n",
    "            'n_strong_concepts': len([s for s in scores if s['deviation'] > 0.2]),\n",
    "            'top_concept': top_concepts[0]['concept'] if top_concepts else 'None',\n",
    "            'top_auc': top_concepts[0]['auc'] if top_concepts else 0.5,\n",
    "            'n_active_proteins': n_active,\n",
    "            'activation_rate': n_active / len(feature_activations),\n",
    "            'mean_activation': mean_activation\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    summary_df = pd.DataFrame(feature_summary)\n",
    "    summary_df = summary_df.sort_values('max_auc_deviation', ascending=False)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Generate summary\n",
    "strong_associations, important_features = summarize_feature_analysis(associations)\n",
    "\n",
    "# Create detailed feature table\n",
    "feature_summary_df = create_feature_summary_table(associations, aligned_data)\n",
    "\n",
    "print(f\"\\n=== Top 10 Most Interpretable SAE Features ===\")\n",
    "display_cols = ['feature_idx', 'max_auc_deviation', 'n_strong_concepts', 'top_concept', 'top_auc', \n",
    "                'activation_rate', 'mean_activation']\n",
    "print(feature_summary_df[display_cols].head(10).to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save results\n",
    "results_dir = DATA_DIR / 'analysis_results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "feature_summary_df.to_csv(results_dir / 'sae_feature_summary.csv', index=False)\n",
    "pd.DataFrame(strong_associations, columns=['concept', 'feature_idx', 'auc']).to_csv(\n",
    "    results_dir / 'strong_associations.csv', index=False)\n",
    "\n",
    "print(f\"\\nResults saved to {results_dir}/\")\n",
    "print(f\"- sae_feature_summary.csv: Summary of all features\")\n",
    "print(f\"- strong_associations.csv: All strong feature-concept associations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparison with Original Small Dataset\n",
    "\n",
    "Compare findings with the original small test dataset to see improvements from using larger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare with original test sequences from the small notebook\n",
    "original_seqs = {\n",
    "    \"Ab_H\": \"EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMHWVRQAPGKGLEWVSYISSGSSSYIYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCARGLGGFGDYWGQGTLVTVSS\",\n",
    "    \"Ab_L\": \"DIQMTQSPSSLSASVGDRVTITCRASQGISNYLAWYQQKPGKAPKLLIYDASTRATGIPDRFSGSGSGTDFTLTISSVQAEDLAVYYCQQYNTYPFTFGQGTKVEIK\",\n",
    "    \"Collagen_like\": \"MGPPGPPGPPGPPGPPGPPGPP\",\n",
    "    \"His_rich\": \"MKKRHHHHHHGSGSGSGHHHHEE\",\n",
    "    \"NGlyc\": \"MATRNATSNEKSTNVTQLLNNST\",\n",
    "    \"CysPair\": \"MAGRCCGGTTCCGGAAACCXXC\"\n",
    "}\n",
    "\n",
    "print(\"=== Comparison with Original Test Sequences ===\")\n",
    "print(\"\\nProcessing original test sequences with trained features...\")\n",
    "\n",
    "original_features = {}\n",
    "for name, seq in original_seqs.items():\n",
    "    try:\n",
    "        hidden_states = extract_esm_features(seq)\n",
    "        sae_features, _, _ = extract_sae_features(hidden_states)\n",
    "        pooled = pool_sequence_features(sae_features)\n",
    "        original_features[name] = pooled.cpu().numpy()\n",
    "        \n",
    "        print(f\"\\n{name} (length {len(seq)}):\")\n",
    "        mean_features = pooled[:feature_dim]\n",
    "        top_indices = np.argsort(mean_features)[::-1][:5]\n",
    "        \n",
    "        for i, idx in enumerate(top_indices):\n",
    "            activation = mean_features[idx]\n",
    "            # Check if this feature appeared in our analysis\n",
    "            if idx in important_features:\n",
    "                marker = \" *** (important in large dataset)\"\n",
    "            else:\n",
    "                marker = \"\"\n",
    "            print(f\"  {i+1}. Feature {idx:4d}: {activation:.3f}{marker}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {name}: {e}\")\n",
    "\n",
    "# Analyze overlap between original test features and our discovered features\n",
    "original_top_features = set()\n",
    "for name, features in original_features.items():\n",
    "    mean_part = features[:feature_dim]\n",
    "    top_5 = np.argsort(mean_part)[::-1][:5]\n",
    "    original_top_features.update(top_5)\n",
    "\n",
    "overlap = original_top_features.intersection(important_features)\n",
    "print(f\"\\n=== Feature Overlap Analysis ===\")\n",
    "print(f\"Original test sequences activate {len(original_top_features)} distinct features\")\n",
    "print(f\"Large dataset analysis found {len(important_features)} important features\")\n",
    "print(f\"Overlap: {len(overlap)} features ({100*len(overlap)/len(original_top_features):.1f}% of original)\")\n",
    "print(f\"Overlapping features: {sorted(list(overlap))}\")\n",
    "\n",
    "print(f\"\\n=== Key Insights ===\")\n",
    "print(f\"1. Large dataset analysis identified {len(important_features)} biologically meaningful features\")\n",
    "print(f\"2. Found {len(strong_associations)} strong feature-concept associations\")\n",
    "print(f\"3. Features show specialization for structural, functional, and modification concepts\")\n",
    "print(f\"4. Mean activation rate across proteins: {X_mean.mean():.4f}\")\n",
    "print(f\"5. Most discriminative features achieve AUC > 0.8 for their target concepts\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"6. {len(overlap)} features from toy examples also appear important in real proteins\")\n",
    "else:\n",
    "    print(f\"6. Toy examples use different features than those important for real protein concepts\")\n",
    "\n",
    "print(f\"\\nThis analysis demonstrates that SAEs learn interpretable features that correspond\")\n",
    "print(f\"to meaningful biological concepts when trained on diverse protein data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting and Next Steps\n",
    "print(\"=== Analysis Complete ===\")\n",
    "\n",
    "if 'associations' in locals() and len(associations) > 0:\n",
    "    print(\"✅ Successfully completed SAE feature analysis!\")\n",
    "    print(f\"Found {len(associations)} analyzable concepts\")\n",
    "    print(f\"Processed {len(aligned_data)} proteins with complete data\")\n",
    "    \n",
    "    # Save key results\n",
    "    results_dir = DATA_DIR / 'analysis_results' \n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save association results\n",
    "    association_summary = []\n",
    "    for concept, data in associations.items():\n",
    "        for feature_idx, auc in data['top_features'][:10]:\n",
    "            association_summary.append({\n",
    "                'concept': concept,\n",
    "                'feature_idx': feature_idx, \n",
    "                'auc': auc,\n",
    "                'auc_deviation': abs(auc - 0.5),\n",
    "                'n_positive': data['n_positive'],\n",
    "                'n_total': data['n_total']\n",
    "            })\n",
    "    \n",
    "    pd.DataFrame(association_summary).to_csv(results_dir / 'feature_concept_associations.csv', index=False)\n",
    "    aligned_data.to_csv(results_dir / 'aligned_protein_data.csv')\n",
    "    \n",
    "    print(f\"\\\\n📊 Results saved to {results_dir}/\")\n",
    "    print(\"- feature_concept_associations.csv: All feature-concept pairs\")\n",
    "    print(\"- aligned_protein_data.csv: Protein data with features and concepts\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Analysis incomplete. Troubleshooting:\")\n",
    "    \n",
    "    if concepts_df is None:\n",
    "        print(\"\\\\n🔍 Issue: Failed to load protein annotations\")\n",
    "        print(\"Solutions:\")\n",
    "        print(\"1. Check internet connection for UniProt download\")\n",
    "        print(\"2. Verify UniProt API is accessible\")\n",
    "        print(\"3. Try re-running the annotation download cell\")\n",
    "        \n",
    "    elif len(common_ids) == 0:\n",
    "        print(\"\\\\n🔍 Issue: No overlap between FASTA sequences and annotations\")  \n",
    "        print(\"Solutions:\")\n",
    "        print(\"1. The FASTA and annotation queries may use different ID formats\")\n",
    "        print(\"2. Try downloading both from the same date/version\")\n",
    "        print(\"3. Check if protein IDs match between the two datasets\")\n",
    "        \n",
    "    elif len(aligned_data) < 50:\n",
    "        print(f\"\\\\n🔍 Issue: Very few proteins in analysis ({len(aligned_data)})\")\n",
    "        print(\"Solutions:\")\n",
    "        print(\"1. Increase the number of sequences processed\")\n",
    "        print(\"2. Use a less restrictive UniProt query\")\n",
    "        print(\"3. Process more proteins from the FASTA file\")\n",
    "        \n",
    "    print(\"\\\\n💡 To improve results:\")\n",
    "    print(\"1. Increase max_sequences in create_sequence_subset() to 25,000+\")\n",
    "    print(\"2. Increase subset_size in feature extraction to 5,000+\")  \n",
    "    print(\"3. Use broader UniProt annotation query (remove filters)\")\n",
    "    print(\"4. Consider using UniRef50 for even more proteins\")\n",
    "\n",
    "print(\"\\\\n🚀 Next steps for deeper analysis:\")\n",
    "print(\"1. Run the full pipeline with 10K+ proteins\")\n",
    "print(\"2. Analyze per-residue feature activations on specific proteins\")\n",
    "print(\"3. Train custom SAEs on your own protein data\")\n",
    "print(\"4. Use the InterPLM dashboard for interactive exploration\")\n",
    "print(\"5. Compare findings across different ESM-2 layers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 ('interplm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1106d1d489397abf5d77132595a521cf67d890f951d991cd34215b053d2a27e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
