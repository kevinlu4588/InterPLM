{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale SAE Analysis with UniProt Dataset\n",
    "\n",
    "This notebook demonstrates how to analyze sparse autoencoders (SAEs) using a larger UniProt dataset,\n",
    "following the InterPLM recommendations. We'll download Swiss-Prot data, create annotations, and mine SAE features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import gzip\n",
    "import urllib.request\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# InterPLM imports\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "from interplm.sae.inference import load_sae_from_hf\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set up data directory\n",
    "DATA_DIR = Path(os.environ.get('INTERPLM_DATA', '/home/ec2-user/InterPLM/data'))\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Large UniProt Dataset\n",
    "\n",
    "Following InterPLM's recommendations, we'll download Swiss-Prot which contains high-quality, manually curated protein sequences with rich annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uniprot_data(data_dir: Path, subset_size: int = 10000):\n",
    "    \"\"\"\n",
    "    Download UniProt Swiss-Prot data with annotations.\n",
    "    Following InterPLM README recommendations for larger dataset analysis.\n",
    "    \"\"\"\n",
    "    uniprot_dir = data_dir / 'uniprot'\n",
    "    uniprot_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Download Swiss-Prot FASTA file\n",
    "    fasta_url = \"https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\"\n",
    "    fasta_path = uniprot_dir / \"uniprot_sprot.fasta.gz\"\n",
    "    \n",
    "    if not fasta_path.exists():\n",
    "        print(\"Downloading Swiss-Prot FASTA file (this may take a while)...\")\n",
    "        urllib.request.urlretrieve(fasta_url, fasta_path)\n",
    "        print(f\"Downloaded to {fasta_path}\")\n",
    "    else:\n",
    "        print(f\"FASTA file already exists at {fasta_path}\")\n",
    "    \n",
    "    # Download UniProtKB annotations (TSV format with rich metadata)\n",
    "    # This query gets high-quality proteins with 3D structures and good annotations\n",
    "    annotations_url = (\n",
    "        \"https://rest.uniprot.org/uniprotkb/stream?compressed=true&\"\n",
    "        \"fields=accession%2Creviewed%2Cprotein_name%2Clength%2Csequence%2C\"\n",
    "        \"ec%2Cft_act_site%2Cft_binding%2Ccc_cofactor%2Cft_disulfid%2C\"\n",
    "        \"ft_carbohyd%2Cft_lipid%2Cft_mod_res%2Cft_signal%2Cft_transit%2C\"\n",
    "        \"ft_helix%2Cft_turn%2Cft_strand%2Cft_coiled%2Ccc_domain%2C\"\n",
    "        \"ft_compbias%2Cft_domain%2Cft_motif%2Cft_region%2Cft_zn_fing%2C\"\n",
    "        \"xref_alphafolddb&format=tsv&query=%28reviewed%3Atrue%29+AND+\"\n",
    "        \"%28proteins_with%3A1%29+AND+%28length%3A%5B50+TO+1022%5D%29\"\n",
    "    )\n",
    "    \n",
    "    annotations_path = uniprot_dir / \"proteins_annotations.tsv.gz\"\n",
    "    \n",
    "    if not annotations_path.exists():\n",
    "        print(\"Downloading UniProt annotations...\")\n",
    "        urllib.request.urlretrieve(annotations_url, annotations_path)\n",
    "        print(f\"Downloaded annotations to {annotations_path}\")\n",
    "    else:\n",
    "        print(f\"Annotations file already exists at {annotations_path}\")\n",
    "    \n",
    "    return fasta_path, annotations_path\n",
    "\n",
    "# Download the data\n",
    "fasta_path, annotations_path = download_uniprot_data(DATA_DIR)\n",
    "\n",
    "print(f\"\\nFASTA file: {fasta_path}\")\n",
    "print(f\"Annotations file: {annotations_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process and Filter Sequences\n",
    "\n",
    "Create a manageable subset of sequences for analysis while maintaining diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence_subset(fasta_path: Path, output_path: Path, \n",
    "                          max_sequences: int = 5000, \n",
    "                          min_length: int = 50, \n",
    "                          max_length: int = 1022):\n",
    "    \"\"\"\n",
    "    Create a filtered subset of sequences for ESM-2 compatibility.\n",
    "    Following InterPLM's recommendations for sequence length limits.\n",
    "    \"\"\"\n",
    "    print(f\"Processing sequences from {fasta_path}...\")\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    with gzip.open(fasta_path, 'rt') as handle:\n",
    "        for i, record in enumerate(SeqIO.parse(handle, \"fasta\")):\n",
    "            if len(sequences) >= max_sequences:\n",
    "                break\n",
    "                \n",
    "            seq_len = len(record.seq)\n",
    "            if min_length <= seq_len <= max_length:\n",
    "                # Extract UniProt ID and description\n",
    "                uniprot_id = record.id.split('|')[1] if '|' in record.id else record.id\n",
    "                description = record.description\n",
    "                \n",
    "                sequences.append({\n",
    "                    'uniprot_id': uniprot_id,\n",
    "                    'sequence': str(record.seq),\n",
    "                    'length': seq_len,\n",
    "                    'description': description\n",
    "                })\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(f\"Processed {i:,} sequences, kept {len(sequences):,}\")\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = pd.DataFrame(sequences)\n",
    "    \n",
    "    # Save as both CSV and FASTA\n",
    "    df.to_csv(output_path.with_suffix('.csv'), index=False)\n",
    "    \n",
    "    # Write FASTA file\n",
    "    with open(output_path, 'w') as f:\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(f\">{row['uniprot_id']}|{row['description']}\\n\")\n",
    "            f.write(f\"{row['sequence']}\\n\")\n",
    "    \n",
    "    print(f\"\\nCreated subset with {len(df):,} sequences\")\n",
    "    print(f\"Length range: {df['length'].min()} - {df['length'].max()}\")\n",
    "    print(f\"Mean length: {df['length'].mean():.1f}\")\n",
    "    print(f\"Saved to {output_path} and {output_path.with_suffix('.csv')}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create subset\n",
    "subset_path = DATA_DIR / 'uniprot' / 'subset_5k.fasta'\n",
    "sequences_df = create_sequence_subset(fasta_path, subset_path, max_sequences=5000)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n=== Sequence Statistics ===\")\n",
    "print(sequences_df['length'].describe())\n",
    "\n",
    "# Plot length distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sequences_df['length'], bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Protein Sequence Lengths')\n",
    "plt.axvline(sequences_df['length'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {sequences_df[\"length\"].mean():.1f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Models and Extract Features\n",
    "\n",
    "Load the ESM-2 model and pre-trained SAE, then extract features from our protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_name = 'facebook/esm2_t33_650M_UR50D'  # Using 650M model for better features\n",
    "plm_model = \"esm2-650m\"\n",
    "plm_layer = 24  # Middle layer often has good interpretable features\n",
    "\n",
    "print(f\"Loading ESM-2 model: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "esm_model = EsmModel.from_pretrained(model_name, output_hidden_states=True).to(device).eval()\n",
    "\n",
    "print(f\"Loading SAE for {plm_model} layer {plm_layer}\")\n",
    "sae = load_sae_from_hf(plm_model=plm_model, plm_layer=plm_layer).to(device).eval()\n",
    "\n",
    "print(f\"SAE architecture: {sae}\")\n",
    "print(f\"Dictionary size: {sae.encoder.out_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_esm_features(sequence: str):\n",
    "    \"\"\"\n",
    "    Extract hidden states from ESM-2 for a given sequence.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(sequence, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    outputs = esm_model(**tokens)\n",
    "    hidden_states = outputs.hidden_states[plm_layer][0]  # (L+2, d)\n",
    "    return hidden_states[1:-1]  # Remove CLS and EOS tokens -> (L, d)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_sae_features(hidden_states):\n",
    "    \"\"\"\n",
    "    Extract SAE features from ESM hidden states.\n",
    "    \"\"\"\n",
    "    sae_features = sae.encode(hidden_states)  # (L, D_dict)\n",
    "    reconstructed = sae.decode(sae_features)\n",
    "    reconstruction_error = hidden_states - reconstructed\n",
    "    return sae_features, reconstructed, reconstruction_error\n",
    "\n",
    "def pool_sequence_features(features: torch.Tensor, method: str = 'max_mean'):\n",
    "    \"\"\"\n",
    "    Pool per-residue features to sequence-level features.\n",
    "    \"\"\"\n",
    "    if method == 'max_mean':\n",
    "        # Concatenate max and mean pooling (as in original notebook)\n",
    "        return torch.cat([features.mean(0), features.max(0).values])\n",
    "    elif method == 'mean':\n",
    "        return features.mean(0)\n",
    "    elif method == 'max':\n",
    "        return features.max(0).values\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling method: {method}\")\n",
    "\n",
    "# Test with a few sequences\n",
    "test_sequences = sequences_df.head(3)\n",
    "\n",
    "print(\"Testing feature extraction on sample sequences...\")\n",
    "for i, row in test_sequences.iterrows():\n",
    "    seq = row['sequence']\n",
    "    print(f\"\\nProcessing {row['uniprot_id']} (length: {len(seq)})\")\n",
    "    \n",
    "    # Extract features\n",
    "    hidden_states = extract_esm_features(seq)\n",
    "    sae_features, reconstructed, error = extract_sae_features(hidden_states)\n",
    "    \n",
    "    print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "    print(f\"SAE features shape: {sae_features.shape}\")\n",
    "    print(f\"Active features (>0.1): {(sae_features > 0.1).sum().item()}\")\n",
    "    print(f\"Max activation: {sae_features.max().item():.3f}\")\n",
    "    print(f\"Reconstruction MSE: {torch.mean(error**2).item():.6f}\")\n",
    "    \n",
    "    if i == 0:  # Show top features for first sequence\n",
    "        pooled = pool_sequence_features(sae_features)\n",
    "        mean_features = pooled[:sae_features.shape[1]]\n",
    "        top_features = torch.topk(mean_features, 10)\n",
    "        print(f\"Top 10 features: {list(zip(top_features.indices.cpu().numpy(), top_features.values.cpu().numpy()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Full Dataset and Extract Features\n",
    "\n",
    "Extract SAE features for all sequences in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences_batch(sequences_df: pd.DataFrame, \n",
    "                           batch_size: int = 32,\n",
    "                           save_every: int = 500):\n",
    "    \"\"\"\n",
    "    Process sequences in batches and extract SAE features.\n",
    "    Save intermediate results to avoid losing progress.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    cache_dir = DATA_DIR / 'sae_features_cache'\n",
    "    cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing {len(sequences_df)} sequences...\")\n",
    "    \n",
    "    for i in range(0, len(sequences_df), batch_size):\n",
    "        batch = sequences_df.iloc[i:i+batch_size]\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            try:\n",
    "                seq = row['sequence']\n",
    "                uniprot_id = row['uniprot_id']\n",
    "                \n",
    "                # Extract features\n",
    "                hidden_states = extract_esm_features(seq)\n",
    "                sae_features, _, error = extract_sae_features(hidden_states)\n",
    "                \n",
    "                # Pool to sequence level\n",
    "                pooled_features = pool_sequence_features(sae_features)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'uniprot_id': uniprot_id,\n",
    "                    'length': len(seq),\n",
    "                    'features': pooled_features.cpu().numpy(),\n",
    "                    'max_activation': sae_features.max().item(),\n",
    "                    'n_active_features': (sae_features > 0.1).sum().item(),\n",
    "                    'reconstruction_mse': torch.mean(error**2).item()\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {row['uniprot_id']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Progress update\n",
    "        processed = min(i + batch_size, len(sequences_df))\n",
    "        print(f\"Processed {processed}/{len(sequences_df)} sequences ({100*processed/len(sequences_df):.1f}%)\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        if processed % save_every == 0 or processed == len(sequences_df):\n",
    "            cache_file = cache_dir / f'features_batch_{processed}.pkl'\n",
    "            pd.DataFrame(results).to_pickle(cache_file)\n",
    "            print(f\"Saved intermediate results to {cache_file}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Process a subset first (for faster testing)\n",
    "subset_size = min(1000, len(sequences_df))  # Process first 1000 sequences\n",
    "print(f\"Processing first {subset_size} sequences for analysis...\")\n",
    "\n",
    "features_df = process_sequences_batch(sequences_df.head(subset_size), batch_size=16)\n",
    "\n",
    "print(f\"\\nExtracted features for {len(features_df)} sequences\")\n",
    "print(f\"Feature dimensions: {features_df['features'].iloc[0].shape}\")\n",
    "\n",
    "# Save complete results\n",
    "features_path = DATA_DIR / 'sae_features.pkl'\n",
    "features_df.to_pickle(features_path)\n",
    "print(f\"Saved all features to {features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Rich Protein Annotations\n",
    "\n",
    "Extract and process UniProt annotations to create binary concept labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_uniprot_annotations(annotations_path: Path):\n",
    "    \"\"\"\n",
    "    Parse UniProt annotations and create binary concept labels.\n",
    "    \"\"\"\n",
    "    print(f\"Loading annotations from {annotations_path}...\")\n",
    "    \n",
    "    # Read the TSV file\n",
    "    annotations_df = pd.read_csv(annotations_path, sep='\\t', compression='gzip')\n",
    "    \n",
    "    print(f\"Loaded {len(annotations_df)} protein annotations\")\n",
    "    print(f\"Available columns: {list(annotations_df.columns)}\")\n",
    "    \n",
    "    # Create binary concept labels\n",
    "    concepts = {}\n",
    "    \n",
    "    # Structural features\n",
    "    concepts['has_signal_peptide'] = ~annotations_df['Signal peptide'].isna()\n",
    "    concepts['has_disulfide_bond'] = ~annotations_df['Disulfide bond'].isna()\n",
    "    concepts['has_helix'] = ~annotations_df['Helix'].isna()\n",
    "    concepts['has_strand'] = ~annotations_df['Beta strand'].isna()\n",
    "    concepts['has_turn'] = ~annotations_df['Turn'].isna()\n",
    "    concepts['has_coiled_coil'] = ~annotations_df['Coiled coil'].isna()\n",
    "    \n",
    "    # Post-translational modifications\n",
    "    concepts['has_glycosylation'] = ~annotations_df['Glycosylation'].isna()\n",
    "    concepts['has_lipidation'] = ~annotations_df['Lipidation'].isna()\n",
    "    concepts['has_modification'] = ~annotations_df['Modified residue'].isna()\n",
    "    \n",
    "    # Functional features\n",
    "    concepts['has_active_site'] = ~annotations_df['Active site'].isna()\n",
    "    concepts['has_binding_site'] = ~annotations_df['Binding site'].isna()\n",
    "    concepts['has_enzyme_activity'] = ~annotations_df['EC number'].isna()\n",
    "    \n",
    "    # Sequence features\n",
    "    concepts['has_domain'] = ~annotations_df['Domain [FT]'].isna()\n",
    "    concepts['has_motif'] = ~annotations_df['Motif'].isna()\n",
    "    concepts['has_region'] = ~annotations_df['Region'].isna()\n",
    "    concepts['has_zinc_finger'] = ~annotations_df['Zinc finger'].isna()\n",
    "    concepts['has_compositional_bias'] = ~annotations_df['Compositional bias'].isna()\n",
    "    \n",
    "    # Length-based categories\n",
    "    concepts['short_protein'] = annotations_df['Length'] < 150\n",
    "    concepts['medium_protein'] = (annotations_df['Length'] >= 150) & (annotations_df['Length'] < 400)\n",
    "    concepts['long_protein'] = annotations_df['Length'] >= 400\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    concepts_df = pd.DataFrame(concepts, index=annotations_df['Entry'])\n",
    "    \n",
    "    # Add sequence information\n",
    "    concepts_df['sequence'] = annotations_df['Sequence'].values\n",
    "    concepts_df['length'] = annotations_df['Length'].values\n",
    "    concepts_df['protein_name'] = annotations_df['Protein names'].values\n",
    "    \n",
    "    return concepts_df, annotations_df\n",
    "\n",
    "# Parse annotations\n",
    "concepts_df, raw_annotations = parse_uniprot_annotations(annotations_path)\n",
    "\n",
    "print(f\"\\nCreated {len(concepts_df)} concept annotations\")\n",
    "print(f\"Available concepts: {[col for col in concepts_df.columns if col not in ['sequence', 'length', 'protein_name']]}\")\n",
    "\n",
    "# Show concept statistics\n",
    "concept_cols = [col for col in concepts_df.columns if col not in ['sequence', 'length', 'protein_name']]\n",
    "concept_stats = concepts_df[concept_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n=== Concept Statistics ===\")\n",
    "for concept, count in concept_stats.head(15).items():\n",
    "    pct = 100 * count / len(concepts_df)\n",
    "    print(f\"{concept:25s}: {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "# Visualize concept distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "concept_stats.head(15).plot(kind='barh')\n",
    "plt.xlabel('Number of Proteins')\n",
    "plt.title('Distribution of Protein Concepts')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Match Features with Annotations\n",
    "\n",
    "Align the extracted SAE features with the protein annotations for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features with concepts by UniProt ID\n",
    "feature_ids = set(features_df['uniprot_id'])\n",
    "concept_ids = set(concepts_df.index)\n",
    "common_ids = feature_ids.intersection(concept_ids)\n",
    "\n",
    "print(f\"Features extracted for: {len(feature_ids)} proteins\")\n",
    "print(f\"Concepts available for: {len(concept_ids)} proteins\")\n",
    "print(f\"Common proteins: {len(common_ids)} proteins\")\n",
    "\n",
    "if len(common_ids) < 100:\n",
    "    print(\"\\nWarning: Low overlap between features and concepts.\")\n",
    "    print(\"This might happen if the annotation query was too restrictive.\")\n",
    "    print(\"Consider downloading a broader set of annotations.\")\n",
    "\n",
    "# Create matched dataset\n",
    "matched_features = features_df[features_df['uniprot_id'].isin(common_ids)].set_index('uniprot_id')\n",
    "matched_concepts = concepts_df.loc[common_ids]\n",
    "\n",
    "# Align the data\n",
    "aligned_data = matched_features.join(matched_concepts, how='inner')\n",
    "\n",
    "print(f\"\\nAligned dataset: {len(aligned_data)} proteins\")\n",
    "print(f\"Feature dimension: {aligned_data['features'].iloc[0].shape[0]}\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.vstack(aligned_data['features'].values)\n",
    "feature_dim = X.shape[1] // 2  # Half are mean, half are max pooled\n",
    "\n",
    "# Split into mean and max features (following original notebook)\n",
    "X_mean = X[:, :feature_dim]\n",
    "X_max = X[:, feature_dim:]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Mean features: {X_mean.shape}, Max features: {X_max.shape}\")\n",
    "\n",
    "# Get concept labels\n",
    "concept_cols = [col for col in aligned_data.columns \n",
    "                if col not in ['features', 'length', 'max_activation', 'n_active_features', \n",
    "                               'reconstruction_mse', 'sequence', 'protein_name']]\n",
    "Y = aligned_data[concept_cols].astype(int)\n",
    "\n",
    "print(f\"\\nConcept matrix shape: {Y.shape}\")\n",
    "print(f\"Available concepts for analysis: {len(concept_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SAE Feature Mining and Analysis\n",
    "\n",
    "Analyze which SAE features are associated with different protein concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_concept_associations(X_features: np.ndarray, \n",
    "                                        Y_concepts: pd.DataFrame,\n",
    "                                        min_concept_instances: int = 10):\n",
    "    \"\"\"\n",
    "    Compute associations between SAE features and protein concepts using AUC.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    n_features = X_features.shape[1]\n",
    "    \n",
    "    print(f\"Computing associations for {n_features} features and {len(Y_concepts.columns)} concepts...\")\n",
    "    \n",
    "    for concept in Y_concepts.columns:\n",
    "        y = Y_concepts[concept].values\n",
    "        \n",
    "        # Skip concepts with too few positive examples\n",
    "        if y.sum() < min_concept_instances or (len(y) - y.sum()) < min_concept_instances:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Processing {concept}: {y.sum()}/{len(y)} positive examples\")\n",
    "        \n",
    "        aucs = []\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X_features[:, feature_idx]\n",
    "            \n",
    "            # Skip features with no variation\n",
    "            if len(np.unique(feature_values)) < 2:\n",
    "                aucs.append(0.5)\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                auc = roc_auc_score(y, feature_values)\n",
    "                aucs.append(auc)\n",
    "            except ValueError:\n",
    "                aucs.append(0.5)\n",
    "        \n",
    "        # Find top features for this concept\n",
    "        aucs = np.array(aucs)\n",
    "        top_indices = np.argsort(np.abs(aucs - 0.5))[::-1][:20]  # Top 20 by deviation from 0.5\n",
    "        \n",
    "        results[concept] = {\n",
    "            'aucs': aucs,\n",
    "            'top_features': [(int(idx), aucs[idx]) for idx in top_indices],\n",
    "            'n_positive': int(y.sum()),\n",
    "            'n_total': len(y)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compute associations using mean-pooled features\n",
    "associations = compute_feature_concept_associations(X_mean, Y, min_concept_instances=5)\n",
    "\n",
    "print(f\"\\n=== Feature-Concept Associations ===\")\n",
    "print(f\"Analyzed {len(associations)} concepts\")\n",
    "\n",
    "# Display top associations for each concept\n",
    "for concept, data in associations.items():\n",
    "    top_features = data['top_features'][:5]\n",
    "    print(f\"\\n{concept} ({data['n_positive']}/{data['n_total']} examples):\")\n",
    "    for feature_idx, auc in top_features:\n",
    "        print(f\"  Feature {feature_idx:4d}: AUC = {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Feature Patterns\n",
    "\n",
    "Create visualizations to understand what the SAE features have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_features_heatmap(associations: dict, X_features: np.ndarray, Y_concepts: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing the top features for each concept.\n",
    "    \"\"\"\n",
    "    # Collect top features across all concepts\n",
    "    all_top_features = set()\n",
    "    for concept_data in associations.values():\n",
    "        top_features = [f[0] for f in concept_data['top_features'][:10]]\n",
    "        all_top_features.update(top_features)\n",
    "    \n",
    "    all_top_features = sorted(list(all_top_features))\n",
    "    \n",
    "    # Create AUC matrix\n",
    "    auc_matrix = np.zeros((len(associations), len(all_top_features)))\n",
    "    concept_names = list(associations.keys())\n",
    "    \n",
    "    for i, concept in enumerate(concept_names):\n",
    "        aucs = associations[concept]['aucs']\n",
    "        for j, feature_idx in enumerate(all_top_features):\n",
    "            auc_matrix[i, j] = aucs[feature_idx]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(auc_matrix, \n",
    "                xticklabels=[f\"F{f}\" for f in all_top_features],\n",
    "                yticklabels=concept_names,\n",
    "                cmap='RdBu_r', center=0.5, \n",
    "                cbar_kws={'label': 'AUC Score'})\n",
    "    plt.title('Feature-Concept Association Heatmap')\n",
    "    plt.xlabel('SAE Features')\n",
    "    plt.ylabel('Protein Concepts')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_matrix, concept_names, all_top_features\n",
    "\n",
    "def plot_feature_distributions(feature_idx: int, concept: str, \n",
    "                              X_features: np.ndarray, Y_concepts: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot the distribution of a specific feature for positive vs negative examples of a concept.\n",
    "    \"\"\"\n",
    "    y = Y_concepts[concept].values\n",
    "    feature_values = X_features[:, feature_idx]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot distributions\n",
    "    plt.hist(feature_values[y == 0], bins=30, alpha=0.7, label=f'Negative ({(y==0).sum()})', \n",
    "             density=True, color='blue')\n",
    "    plt.hist(feature_values[y == 1], bins=30, alpha=0.7, label=f'Positive ({(y==1).sum()})', \n",
    "             density=True, color='red')\n",
    "    \n",
    "    plt.xlabel(f'Feature {feature_idx} Activation')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Feature {feature_idx} Distribution for {concept}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add AUC score\n",
    "    auc = roc_auc_score(y, feature_values)\n",
    "    plt.text(0.05, 0.95, f'AUC = {auc:.3f}', transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create heatmap\n",
    "auc_matrix, concept_names, top_features = plot_top_features_heatmap(associations, X_mean, Y)\n",
    "\n",
    "# Plot distributions for a few interesting feature-concept pairs\n",
    "print(\"\\n=== Feature Distribution Examples ===\")\n",
    "\n",
    "# Find the most discriminative feature-concept pairs\n",
    "best_pairs = []\n",
    "for i, concept in enumerate(concept_names):\n",
    "    for j, feature_idx in enumerate(top_features):\n",
    "        auc = auc_matrix[i, j]\n",
    "        if abs(auc - 0.5) > 0.3:  # Strong association\n",
    "            best_pairs.append((feature_idx, concept, auc))\n",
    "\n",
    "best_pairs.sort(key=lambda x: abs(x[2] - 0.5), reverse=True)\n",
    "\n",
    "# Plot top 3 most discriminative pairs\n",
    "for feature_idx, concept, auc in best_pairs[:3]:\n",
    "    print(f\"\\nPlotting Feature {feature_idx} vs {concept} (AUC = {auc:.3f})\")\n",
    "    plot_feature_distributions(feature_idx, concept, X_mean, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sequence-Level Analysis\n",
    "\n",
    "Examine individual sequences to understand how features activate across protein regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sequence_features(uniprot_id: str, top_k_features: int = 5):\n",
    "    \"\"\"\n",
    "    Analyze feature activations for a specific protein sequence.\n",
    "    \"\"\"\n",
    "    # Get sequence data\n",
    "    seq_data = aligned_data.loc[uniprot_id]\n",
    "    sequence = seq_data['sequence']\n",
    "    \n",
    "    print(f\"\\n=== Analysis for {uniprot_id} ===\")\n",
    "    print(f\"Protein name: {seq_data['protein_name']}\")\n",
    "    print(f\"Length: {len(sequence)} residues\")\n",
    "    \n",
    "    # Show active concepts\n",
    "    active_concepts = [col for col in concept_cols if seq_data[col] == 1]\n",
    "    print(f\"Active concepts: {', '.join(active_concepts) if active_concepts else 'None'}\")\n",
    "    \n",
    "    # Extract per-residue features\n",
    "    hidden_states = extract_esm_features(sequence)\n",
    "    sae_features, _, _ = extract_sae_features(hidden_states)\n",
    "    \n",
    "    # Find top-activating features\n",
    "    mean_activations = sae_features.mean(0)\n",
    "    top_features_idx = torch.topk(mean_activations, top_k_features).indices\n",
    "    \n",
    "    print(f\"\\nTop {top_k_features} features by mean activation:\")\n",
    "    for i, feature_idx in enumerate(top_features_idx):\n",
    "        mean_act = mean_activations[feature_idx].item()\n",
    "        max_act = sae_features[:, feature_idx].max().item()\n",
    "        print(f\"  {i+1}. Feature {feature_idx:4d}: mean={mean_act:.3f}, max={max_act:.3f}\")\n",
    "    \n",
    "    # Plot feature activations along sequence\n",
    "    fig, axes = plt.subplots(top_k_features, 1, figsize=(12, 2*top_k_features))\n",
    "    if top_k_features == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, feature_idx in enumerate(top_features_idx):\n",
    "        activations = sae_features[:, feature_idx].cpu().numpy()\n",
    "        \n",
    "        axes[i].plot(activations, linewidth=2)\n",
    "        axes[i].set_ylabel(f'Feature {feature_idx}')\n",
    "        axes[i].set_title(f'Feature {feature_idx} Activation (mean={mean_activations[feature_idx]:.3f})')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight high-activation regions\n",
    "        threshold = activations.mean() + 2 * activations.std()\n",
    "        high_regions = activations > threshold\n",
    "        if high_regions.any():\n",
    "            axes[i].fill_between(range(len(activations)), 0, activations, \n",
    "                               where=high_regions, alpha=0.3, color='red')\n",
    "    \n",
    "    axes[-1].set_xlabel('Residue Position')\n",
    "    plt.suptitle(f'SAE Feature Activations: {uniprot_id}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return sae_features, top_features_idx\n",
    "\n",
    "# Analyze a few interesting examples\n",
    "print(\"=== Sequence-Level Feature Analysis ===\")\n",
    "\n",
    "# Find some proteins with different concept patterns\n",
    "examples = []\n",
    "\n",
    "# Get an example with signal peptide\n",
    "if 'has_signal_peptide' in Y.columns:\n",
    "    signal_proteins = Y[Y['has_signal_peptide'] == 1].index\n",
    "    if len(signal_proteins) > 0:\n",
    "        examples.append(signal_proteins[0])\n",
    "\n",
    "# Get an example with enzyme activity\n",
    "if 'has_enzyme_activity' in Y.columns:\n",
    "    enzyme_proteins = Y[Y['has_enzyme_activity'] == 1].index\n",
    "    if len(enzyme_proteins) > 0:\n",
    "        examples.append(enzyme_proteins[0])\n",
    "\n",
    "# Get a structural protein example\n",
    "if 'has_domain' in Y.columns:\n",
    "    domain_proteins = Y[Y['has_domain'] == 1].index\n",
    "    if len(domain_proteins) > 0:\n",
    "        examples.append(domain_proteins[0])\n",
    "\n",
    "# Remove duplicates and limit to 3 examples\n",
    "examples = list(set(examples))[:3]\n",
    "\n",
    "for example_id in examples:\n",
    "    try:\n",
    "        analyze_sequence_features(example_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {example_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Interpretation and Biological Insights\n",
    "\n",
    "Summarize findings and provide biological interpretation of discovered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_feature_analysis(associations: dict, min_auc_deviation: float = 0.2):\n",
    "    \"\"\"\n",
    "    Summarize the key findings from feature-concept associations.\n",
    "    \"\"\"\n",
    "    print(\"=== SAE Feature Analysis Summary ===\")\n",
    "    print(f\"\\nAnalyzed {len(associations)} protein concepts\")\n",
    "    \n",
    "    # Count strong associations\n",
    "    strong_associations = []\n",
    "    all_features = set()\n",
    "    \n",
    "    for concept, data in associations.items():\n",
    "        for feature_idx, auc in data['top_features']:\n",
    "            if abs(auc - 0.5) > min_auc_deviation:\n",
    "                strong_associations.append((concept, feature_idx, auc))\n",
    "                all_features.add(feature_idx)\n",
    "    \n",
    "    print(f\"Found {len(strong_associations)} strong feature-concept associations\")\n",
    "    print(f\"Involving {len(all_features)} distinct SAE features\")\n",
    "    \n",
    "    # Group by concept type\n",
    "    structural_concepts = []\n",
    "    functional_concepts = []\n",
    "    modification_concepts = []\n",
    "    \n",
    "    for concept, feature_idx, auc in strong_associations:\n",
    "        if any(keyword in concept.lower() for keyword in ['helix', 'strand', 'coil', 'domain', 'structure']):\n",
    "            structural_concepts.append((concept, feature_idx, auc))\n",
    "        elif any(keyword in concept.lower() for keyword in ['enzyme', 'binding', 'active', 'function']):\n",
    "            functional_concepts.append((concept, feature_idx, auc))\n",
    "        elif any(keyword in concept.lower() for keyword in ['glyco', 'lipid', 'modification', 'signal']):\n",
    "            modification_concepts.append((concept, feature_idx, auc))\n",
    "    \n",
    "    print(f\"\\n=== Associations by Category ===\")\n",
    "    print(f\"Structural features: {len(structural_concepts)}\")\n",
    "    print(f\"Functional features: {len(functional_concepts)}\")\n",
    "    print(f\"Modification features: {len(modification_concepts)}\")\n",
    "    \n",
    "    # Display top associations in each category\n",
    "    categories = [\n",
    "        (\"Structural\", structural_concepts),\n",
    "        (\"Functional\", functional_concepts),\n",
    "        (\"Modification\", modification_concepts)\n",
    "    ]\n",
    "    \n",
    "    for category_name, concepts in categories:\n",
    "        if concepts:\n",
    "            print(f\"\\n--- {category_name} Features ---\")\n",
    "            # Sort by AUC deviation from 0.5\n",
    "            concepts.sort(key=lambda x: abs(x[2] - 0.5), reverse=True)\n",
    "            for i, (concept, feature_idx, auc) in enumerate(concepts[:5]):\n",
    "                direction = \"activates for\" if auc > 0.5 else \"suppresses for\"\n",
    "                print(f\"  {i+1}. Feature {feature_idx:4d} {direction} {concept} (AUC={auc:.3f})\")\n",
    "    \n",
    "    return strong_associations, all_features\n",
    "\n",
    "def create_feature_summary_table(associations: dict, aligned_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create a summary table of the most important features.\n",
    "    \"\"\"\n",
    "    # Collect all feature scores\n",
    "    feature_scores = defaultdict(list)\n",
    "    \n",
    "    for concept, data in associations.items():\n",
    "        for feature_idx, auc in data['top_features'][:10]:\n",
    "            feature_scores[feature_idx].append({\n",
    "                'concept': concept,\n",
    "                'auc': auc,\n",
    "                'deviation': abs(auc - 0.5)\n",
    "            })\n",
    "    \n",
    "    # Summarize each feature\n",
    "    feature_summary = []\n",
    "    \n",
    "    for feature_idx, scores in feature_scores.items():\n",
    "        scores.sort(key=lambda x: x['deviation'], reverse=True)\n",
    "        \n",
    "        # Get top associated concepts\n",
    "        top_concepts = scores[:3]\n",
    "        max_deviation = max(score['deviation'] for score in scores)\n",
    "        \n",
    "        # Count how often this feature is active\n",
    "        feature_activations = X_mean[:, feature_idx]\n",
    "        n_active = (feature_activations > 0.1).sum()\n",
    "        mean_activation = feature_activations.mean()\n",
    "        \n",
    "        feature_summary.append({\n",
    "            'feature_idx': feature_idx,\n",
    "            'max_auc_deviation': max_deviation,\n",
    "            'n_strong_concepts': len([s for s in scores if s['deviation'] > 0.2]),\n",
    "            'top_concept': top_concepts[0]['concept'] if top_concepts else 'None',\n",
    "            'top_auc': top_concepts[0]['auc'] if top_concepts else 0.5,\n",
    "            'n_active_proteins': n_active,\n",
    "            'activation_rate': n_active / len(feature_activations),\n",
    "            'mean_activation': mean_activation\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    summary_df = pd.DataFrame(feature_summary)\n",
    "    summary_df = summary_df.sort_values('max_auc_deviation', ascending=False)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Generate summary\n",
    "strong_associations, important_features = summarize_feature_analysis(associations)\n",
    "\n",
    "# Create detailed feature table\n",
    "feature_summary_df = create_feature_summary_table(associations, aligned_data)\n",
    "\n",
    "print(f\"\\n=== Top 10 Most Interpretable SAE Features ===\")\n",
    "display_cols = ['feature_idx', 'max_auc_deviation', 'n_strong_concepts', 'top_concept', 'top_auc', \n",
    "                'activation_rate', 'mean_activation']\n",
    "print(feature_summary_df[display_cols].head(10).to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Save results\n",
    "results_dir = DATA_DIR / 'analysis_results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "feature_summary_df.to_csv(results_dir / 'sae_feature_summary.csv', index=False)\n",
    "pd.DataFrame(strong_associations, columns=['concept', 'feature_idx', 'auc']).to_csv(\n",
    "    results_dir / 'strong_associations.csv', index=False)\n",
    "\n",
    "print(f\"\\nResults saved to {results_dir}/\")\n",
    "print(f\"- sae_feature_summary.csv: Summary of all features\")\n",
    "print(f\"- strong_associations.csv: All strong feature-concept associations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparison with Original Small Dataset\n",
    "\n",
    "Compare findings with the original small test dataset to see improvements from using larger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare with original test sequences from the small notebook\n",
    "original_seqs = {\n",
    "    \"Ab_H\": \"EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMHWVRQAPGKGLEWVSYISSGSSSYIYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCARGLGGFGDYWGQGTLVTVSS\",\n",
    "    \"Ab_L\": \"DIQMTQSPSSLSASVGDRVTITCRASQGISNYLAWYQQKPGKAPKLLIYDASTRATGIPDRFSGSGSGTDFTLTISSVQAEDLAVYYCQQYNTYPFTFGQGTKVEIK\",\n",
    "    \"Collagen_like\": \"MGPPGPPGPPGPPGPPGPPGPP\",\n",
    "    \"His_rich\": \"MKKRHHHHHHGSGSGSGHHHHEE\",\n",
    "    \"NGlyc\": \"MATRNATSNEKSTNVTQLLNNST\",\n",
    "    \"CysPair\": \"MAGRCCGGTTCCGGAAACCXXC\"\n",
    "}\n",
    "\n",
    "print(\"=== Comparison with Original Test Sequences ===\")\n",
    "print(\"\\nProcessing original test sequences with trained features...\")\n",
    "\n",
    "original_features = {}\n",
    "for name, seq in original_seqs.items():\n",
    "    try:\n",
    "        hidden_states = extract_esm_features(seq)\n",
    "        sae_features, _, _ = extract_sae_features(hidden_states)\n",
    "        pooled = pool_sequence_features(sae_features)\n",
    "        original_features[name] = pooled.cpu().numpy()\n",
    "        \n",
    "        print(f\"\\n{name} (length {len(seq)}):\")\n",
    "        mean_features = pooled[:feature_dim]\n",
    "        top_indices = np.argsort(mean_features)[::-1][:5]\n",
    "        \n",
    "        for i, idx in enumerate(top_indices):\n",
    "            activation = mean_features[idx]\n",
    "            # Check if this feature appeared in our analysis\n",
    "            if idx in important_features:\n",
    "                marker = \" *** (important in large dataset)\"\n",
    "            else:\n",
    "                marker = \"\"\n",
    "            print(f\"  {i+1}. Feature {idx:4d}: {activation:.3f}{marker}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {name}: {e}\")\n",
    "\n",
    "# Analyze overlap between original test features and our discovered features\n",
    "original_top_features = set()\n",
    "for name, features in original_features.items():\n",
    "    mean_part = features[:feature_dim]\n",
    "    top_5 = np.argsort(mean_part)[::-1][:5]\n",
    "    original_top_features.update(top_5)\n",
    "\n",
    "overlap = original_top_features.intersection(important_features)\n",
    "print(f\"\\n=== Feature Overlap Analysis ===\")\n",
    "print(f\"Original test sequences activate {len(original_top_features)} distinct features\")\n",
    "print(f\"Large dataset analysis found {len(important_features)} important features\")\n",
    "print(f\"Overlap: {len(overlap)} features ({100*len(overlap)/len(original_top_features):.1f}% of original)\")\n",
    "print(f\"Overlapping features: {sorted(list(overlap))}\")\n",
    "\n",
    "print(f\"\\n=== Key Insights ===\")\n",
    "print(f\"1. Large dataset analysis identified {len(important_features)} biologically meaningful features\")\n",
    "print(f\"2. Found {len(strong_associations)} strong feature-concept associations\")\n",
    "print(f\"3. Features show specialization for structural, functional, and modification concepts\")\n",
    "print(f\"4. Mean activation rate across proteins: {X_mean.mean():.4f}\")\n",
    "print(f\"5. Most discriminative features achieve AUC > 0.8 for their target concepts\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"6. {len(overlap)} features from toy examples also appear important in real proteins\")\n",
    "else:\n",
    "    print(f\"6. Toy examples use different features than those important for real protein concepts\")\n",
    "\n",
    "print(f\"\\nThis analysis demonstrates that SAEs learn interpretable features that correspond\")\n",
    "print(f\"to meaningful biological concepts when trained on diverse protein data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}