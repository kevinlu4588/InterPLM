{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip, math, random, json, gc\n",
    "from pathlib import Path\n",
    "from typing import List, Iterable, Tuple, Dict\n",
    "\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from utils import extract_esm_features_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FASTA_GZ = Path(\"/home/ec2-user/SageMaker/InterPLM/data/uniprot/uniprot_sprot.fasta.gz\")\n",
    "OUT_DIR  = Path(\"/home/ec2-user/SageMaker/InterPLM/sae_runs/es2_swissprot_layer24\")\n",
    "\n",
    "# ---------- Device ----------\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESM_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "MAX_LEN = 1024 #truncate longer sequences\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "BATCH_SIZE_SEQ = 16\n",
    "LAYER_TO_TRAIN=24\n",
    "TOEKNS_PER_STEP = 4096 #otken vectors per SAE update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(ESM_NAME, do_lower_case=False)\n",
    "mdl = AutoModel.from_pretrained(ESM_NAME).eval().to(device)\n",
    "\n",
    "d_in = mdl.config.hidden_size  # SAE input dimension\n",
    "d_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL\n",
      "('sp|Q6GZX4|001R_FRG3G', 'MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL')\n",
      "MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQTCASGFCTSQPLCARIKKTQVCGLRYSSKGKDPLVSAEWDSRGAPYVRCTYDADLIDTQAQVDQFVSMFGESPSLAERYCMRGVKNTAGELVSRVSSDADPAGGWCRKWYSAHRGPDQDAALGSFCIKNPGAADCKCINRASDPVYQKVKTLHAYPDQCWYVPCAADVGELKMGTQRDTPTNCPTQVCQIVFNMLDDGSVTMDDVKNTINCDFSKYVPPPPPPKPTPPTPPTPPTPPTPPTPPTPPTPRPVHNRKVMFFVAGAVLVAILISTVRW\n",
      "('sp|Q6GZX3|002L_FRG3G', 'MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQTCASGFCTSQPLCARIKKTQVCGLRYSSKGKDPLVSAEWDSRGAPYVRCTYDADLIDTQAQVDQFVSMFGESPSLAERYCMRGVKNTAGELVSRVSSDADPAGGWCRKWYSAHRGPDQDAALGSFCIKNPGAADCKCINRASDPVYQKVKTLHAYPDQCWYVPCAADVGELKMGTQRDTPTNCPTQVCQIVFNMLDDGSVTMDDVKNTINCDFSKYVPPPPPPKPTPPTPPTPPTPPTPPTPPTPPTPRPVHNRKVMFFVAGAVLVAILISTVRW')\n",
      "MASNTVSAQGGSNRPVRDFSNIQDVAQFLLFDPIWNEQPGSIVPWKMNREQALAERYPELQTSEPSEDYSGPVESLELLPLEIKLDIMQYLSWEQISWCKHPWLWTRWYKDNVVRVSAITFEDFQREYAFPEKIQEIHFTDTRAEEIKAILETTPNVTRLVIRRIDDMNYNTHGDLGLDDLEFLTHLMVEDACGFTDFWAPSLTHLTIKNLDMHPRWFGPVMDGIKSMQSTLKYLYIFETYGVNKPFVQWCTDNIETFYCTNSYRYENVPRPIYVWVLFQEDEWHGYRVEDNKFHRRYMYSTILHKRDTDWVENNPLKTPAQVEMYKFLLRISQLNRDGTGYESDSDPENEHFDDESFSSGEEDSSDEDDPTWAPDSDDSDWETETEEEPSVAARILEKGKLTITNLMKSLGFKPKPKKIQSIDRYFCSLDSNYNSEDEDFEYDSDSEDDDSDSEDDC\n",
      "('sp|Q197F8|002R_IIV3', 'MASNTVSAQGGSNRPVRDFSNIQDVAQFLLFDPIWNEQPGSIVPWKMNREQALAERYPELQTSEPSEDYSGPVESLELLPLEIKLDIMQYLSWEQISWCKHPWLWTRWYKDNVVRVSAITFEDFQREYAFPEKIQEIHFTDTRAEEIKAILETTPNVTRLVIRRIDDMNYNTHGDLGLDDLEFLTHLMVEDACGFTDFWAPSLTHLTIKNLDMHPRWFGPVMDGIKSMQSTLKYLYIFETYGVNKPFVQWCTDNIETFYCTNSYRYENVPRPIYVWVLFQEDEWHGYRVEDNKFHRRYMYSTILHKRDTDWVENNPLKTPAQVEMYKFLLRISQLNRDGTGYESDSDPENEHFDDESFSSGEEDSSDEDDPTWAPDSDDSDWETETEEEPSVAARILEKGKLTITNLMKSLGFKPKPKKIQSIDRYFCSLDSNYNSEDEDFEYDSDSEDDDSDSEDDC')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def iter_swissprot_sequences(fasta_gz: Path, max_len: int = MAX_LEN):\n",
    "    with gzip.open(fasta_gz, \"rt\") as fh:\n",
    "        for rec in SeqIO.parse(fh, \"fasta\"):\n",
    "            seq = str(rec.seq)\n",
    "            if not seq:\n",
    "                continue\n",
    "            yield rec.id, (seq[:max_len] if len(seq) > max_len else seq)\n",
    "\n",
    "def batched_sequences(fasta_gz: Path, batch_size: int = BATCH_SIZE_SEQ):\n",
    "    buf = []\n",
    "    for _, seq in iter_swissprot_sequences(fasta_gz):\n",
    "        buf.append(seq)\n",
    "        if len(buf) == batch_size:\n",
    "            yield buf\n",
    "            buf = []\n",
    "    if buf:\n",
    "        yield buf #yields array of [B, L] sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab ESM activations in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def activation_batches(\n",
    "    fasta_gz: Path,\n",
    "    tokens_per_step: int = TOEKNS_PER_STEP,\n",
    "    layer_sel=LAYER_TO_TRAIN,\n",
    "    dtype = torch.float16,\n",
    ") -> Iterable[torch.Tensor]:\n",
    "\n",
    "    buf = []\n",
    "    total = 0\n",
    "    for seq_batch in batched_sequences(fasta_gz, BATCH_SIZE_SEQ):\n",
    "        reps, attn = extract_esm_features_batch(\n",
    "            sequences=seq_batch,\n",
    "            layer_sel=layer_sel,\n",
    "            device=DEVICE,\n",
    "            dtype=dtype,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        ) #reps: [B,L,d], attn:[B, L]\n",
    "\n",
    "        B, L, D = reps.shape #Batch, sequence length, hidden dimension = 1280\n",
    "        for b in range(B): #for sequence in the batch\n",
    "            valid = attn[b] # bool[L], which ones were we supposed to attend to?\n",
    "            if valid.any():\n",
    "                buf.append(reps[b][valid].detach().to(\"cpu\", dtype=torch.float32)) #float32 for SAE\n",
    "                total += int(valid.sum())\n",
    "     \n",
    "        # When we have enough tokens, build a randomized batch and a randomized remainder.\n",
    "        #Yield turns the function into a generator, so the local variables are preserved between next() calls. once we drop references to it or stop iteration, it gets cleaned up\n",
    "        if total >= tokens_per_step:\n",
    "            all_tokens = torch.cat(buf, dim=0)                # [N, d_in]\n",
    "            N = all_tokens.size(0)\n",
    "            perm = torch.randperm(N)                          # random order\n",
    "            take = perm[:tokens_per_step]\n",
    "            keep = perm[tokens_per_step:]\n",
    "\n",
    "            X    = all_tokens[take]                           # [tokens_per_step, d_in]\n",
    "            rest = all_tokens[keep]                           # [N - tokens_per_step, d_in]\n",
    "\n",
    "            # Carry forward the (already shuffled) remainder\n",
    "            buf = ([rest] if rest.numel() else [])\n",
    "            total = int(rest.size(0)) if rest.numel() else 0\n",
    "\n",
    "            yield X\n",
    "\n",
    "    # flush remainder (possibly a short batch)\n",
    "    if buf:\n",
    "        X = torch.cat(buf, dim=0)\n",
    "        if X.numel() > 0:\n",
    "            # optional: shuffle this short batch, too\n",
    "            idx = torch.randperm(X.size(0))\n",
    "            X = X[idx]\n",
    "            yield X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder (ReLU activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, d_in: int, d_hidden: int, tied: bool=False, use_layernorm: bool = False):\n",
    "        super().__init__()\n",
    "        self.d_in, self.d_hidden, self.tied=d_in, d_hidden, tied\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.function as F \n",
    "\n",
    "class SAE(nn.Module):\n",
    "    \"\"\"\n",
    "    One-layer sparse autoencoder (untied decoder).\n",
    "\n",
    "    Design:\n",
    "      x -- (center by bias) --> encoder (Linear) --> LayerNorm? --> ReLU --> f (sparse features)\n",
    "      f -- decoder(linear) --> + bias --> xhat\n",
    "\n",
    "    Shapes (let:\n",
    "    \n",
    "        B = batch size in *tokens* (each row is one token embedding),\n",
    "        D = input dim (ESM hidden size for a given layer, e.g., 320/480/640/1280)\n",
    "        K = number of features or Dictionary size\n",
    "    ):\n",
    "        -x: [B, D]\n",
    "        -f: [B, K]\n",
    "        -x_hat: [B, D]\n",
    "        -encoder.weight: [K, D] (each row maps input -> a feature pre-activation)\n",
    "        -decoder.weight: [D, K] (each column is a dictionary atom in input space)\n",
    "    \n",
    "    Notes:\n",
    "        - The learned bias centers inputs before encoding, then is added back after decoding.\n",
    "        - Decoder is *untied* (separate matrix from encoder) - standard in mech interp saes.\n",
    "        - ReLU enforces Nonnegativity/sparsity in f.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int, #D\n",
    "        d_hidden: int, #K\n",
    "        use_layernorm: bool=False,\n",
    "        init_unitnorm_decoder: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in #D\n",
    "        self.d_hidden = d_hidden #K\n",
    "\n",
    "        #Learned centering bias b \\ in R^D\n",
    "        self.bias = nn.Parameter(torch.zeros(d_in)) # [D]\n",
    "\n",
    "        #Encoder W_e: R^D => Expanded into R^K (plus bias)\n",
    "        self.encoder = nn.Linear(d_in, d_hidden, bias=True) #weight: [K, D], bias: [K]\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_hidden) if use_layernorm else nn.Identity()\n",
    "\n",
    "        #Decoder W_d: R^k => R^D (no bias; we add +b afterwards)\n",
    "        self.decoder = nn.Linear(d-Hidden, d_in, bias=False) # weight: [D, K]\n",
    "\n",
    "        #Initialization\n",
    "\n",
    "        nn.init.xavier_uniform_(self.encoder.weight) #[K, D], _ represents in place operation\n",
    "        if self.encoder.bias is not None:\n",
    "            nn.init.zeros_(self.encoder.bias) #[K]\n",
    "        \n",
    "        if init_unitnorm_decoder:\n",
    "            #make decoder columns unit norm: each atom d_i = decoder.weight[:, i] / ||.||\n",
    "            with torch.no_grad():\n",
    "                W = torch.randn_like(self.decoder.weight) #[D, K]\n",
    "                W = W / (W.norm(dim=0, keepdim=True) + 1e-12)\n",
    "                self.decoder.weight.copy_(W)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.decoder.weight) #[D, K]\n",
    "    \n",
    "    #Core API\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, D]\n",
    "        returns f: [B, K] (sparse, nonnegatvie)\n",
    "        #Multiplication happens as x @ W_e.T we transpose for dimensions to work out\n",
    "        #[B, D] @ [D, K] => [B, K]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #cneter: (x - b) keeps encoder from wasting capacity ont he mean\n",
    "        f_pre = self.encoder(x - self.bias) #[B, K], encoder has weights [K, D]\n",
    "        f. F.relu(self.ln(f_pre)) #[B, K]\n",
    "        return f\n",
    "\n",
    "    def decode(self, f: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        f: [B, K]\n",
    "        returns x_hat: [B,D]\n",
    "\n",
    "        #Same thing here, where is f @ W_d.t + bias\n",
    "        so [B, K] @ [K, D]\n",
    "\n",
    "        but the decoder is actually [D, K] where the columns represent each feature\n",
    "        \"\"\"\n",
    "\n",
    "        x_hat_no_bias = self.decoder(f) #[B, D]\n",
    "        return x_hat_no_bias + self.bias #[B, D]\n",
    "\n",
    "    def forward(self, x.torch.Tensor, output_features: bool = False) -> Tuple[torch.Tensor, torch.Tensor] | torhc.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, D]\n",
    "        returns:\n",
    "            - if output_features=False: x_hat[B, D]  \n",
    "            - if output_features=True: (x_hat [B, D], f [B, K])    \n",
    "        \"\"\"\n",
    "        f = self.encode(x) #[B, K] feature activations for each dictionary vector\n",
    "        x_hat = self.decode(f) #[B, D]\n",
    "\n",
    "        return (x_hat, f) if output_features else x_hat\n",
    "    \n",
    "    #Helpers\n",
    "    @staticmethod\n",
    "    def l1(f: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mean absolute activation (sparsity penalty).\n",
    "        f: [B,K] -> scalar\n",
    "        \n",
    "        \"\"\"\n",
    "        return f.abs().mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def l0(f: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Average number of active (nonzero) features per example.\n",
    "        f: [B, K] -> scalar\n",
    "        \"\"\"\n",
    "        return (f > eps).float().sum(dim=-1).mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def rescale_features_(self, scale: torch.Tensor, eps: float=1e-8) -> None:\n",
    "        \"\"\"\n",
    "        Per-feature rescaling that keeps reconstructions invariant but changes feature scale.\n",
    "        Use this after estimating per-feature maxima or P99 values.\n",
    "        Goal: want f' = f/s, and x_hat' = x_hat.\n",
    "\n",
    "        Achieve by:\n",
    "            encoder.weight(rows) /= s\n",
    "            encoder.bias /= s\n",
    "            decoder.weight(cols) *= s\n",
    "        Args: \n",
    "            scale: [K] positive scale per feature (clipped ot >= eps)\n",
    "        \"\"\"\n",
    "        s = torch.clamp(scale.detach().to(self.encoder.wight.device), min=eps) #[K]\n",
    "\n",
    "        #encoder.weight: [K, D] (row i corresponds to feature i)\n",
    "        self.encoder.weight.data.div_(s[:, None])\n",
    "        if self.encoder.bias is not None:\n",
    "            self.encoder.bias.data.div_(s)\n",
    "        \n",
    "        #decoder.weight: [D, K] (column i corresponds to feature i)\n",
    "        self.decoder.weight.data.mul_(s[None, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 ('interplm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1106d1d489397abf5d77132595a521cf67d890f951d991cd34215b053d2a27e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
