{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip, math, random, json, gc\n",
    "from pathlib import Path\n",
    "from typing import List, Iterable, Tuple, Dict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from utils import extract_esm_features_batch\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FASTA_GZ = Path(\"/home/ec2-user/SageMaker/InterPLM/data/uniprot/uniprot_sprot.fasta.gz\")\n",
    "OUT_DIR  = Path(\"/home/ec2-user/SageMaker/InterPLM/sae_runs/es2_swissprot_layer24\")\n",
    "\n",
    "# ---------- Device ----------\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ESM_NAME, do_lower_case=False)\n",
    "model = AutoModel.from_pretrained(ESM_NAME).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESM_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "MAX_LEN = 1024 #truncate longer sequences\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "BATCH_SIZE_SEQ = 16\n",
    "LAYER_TO_TRAIN=24\n",
    "TOKENS_PER_STEP = 4096 #otken vectors per SAE update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Swissprot protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_swissprot_sequences(fasta_gz: Path, max_len: int = MAX_LEN):\n",
    "    with gzip.open(fasta_gz, \"rt\") as fh:\n",
    "        for rec in SeqIO.parse(fh, \"fasta\"):\n",
    "            seq = str(rec.seq)\n",
    "            if not seq:\n",
    "                continue\n",
    "            yield rec.id, (seq[:max_len] if len(seq) > max_len else seq)\n",
    "\n",
    "def batched_sequences(fasta_gz: Path, batch_size: int = BATCH_SIZE_SEQ):\n",
    "    buf = []\n",
    "    for _, seq in iter_swissprot_sequences(fasta_gz):\n",
    "        buf.append(seq)\n",
    "        if len(buf) == batch_size:\n",
    "            yield buf\n",
    "            buf = []\n",
    "    if buf:\n",
    "        yield buf #yields array of [B, L] sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab ESM activations in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def activation_batches(\n",
    "    fasta_gz: Path,\n",
    "    tokens_per_step: int = TOKENS_PER_STEP,\n",
    "    layer_sel=LAYER_TO_TRAIN,\n",
    "    dtype = torch.float16,\n",
    "    *,\n",
    "    model,\n",
    "    tokenizer,\n",
    ") -> Iterable[torch.Tensor]:\n",
    "\n",
    "    buf = []\n",
    "    total = 0\n",
    "    for seq_batch in batched_sequences(fasta_gz, BATCH_SIZE_SEQ):\n",
    "        reps, attn = extract_esm_features_batch(\n",
    "            sequences=seq_batch,\n",
    "            layer_sel=layer_sel,\n",
    "            device=DEVICE,\n",
    "            dtype=dtype,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        ) #reps: [B,L,d], attn:[B, L]\n",
    "\n",
    "        B, L, D = reps.shape #Batch, sequence length, hidden dimension = 1280\n",
    "        for b in range(B): #for sequence in the batch\n",
    "            valid = attn[b] # bool[L], which ones were we supposed to attend to?\n",
    "            if valid.any():\n",
    "                buf.append(reps[b][valid].detach().to(\"cpu\", dtype=torch.float32)) #float32 for SAE\n",
    "                total += int(valid.sum())\n",
    "     \n",
    "        # When we have enough tokens, build a randomized batch and a randomized remainder.\n",
    "        #Yield turns the function into a generator, so the local variables are preserved between next() calls. once we drop references to it or stop iteration, it gets cleaned up\n",
    "        if total >= tokens_per_step:\n",
    "            all_tokens = torch.cat(buf, dim=0)                # [N, d_in]\n",
    "            N = all_tokens.size(0)\n",
    "            perm = torch.randperm(N)                          # random order\n",
    "            take = perm[:tokens_per_step]\n",
    "            keep = perm[tokens_per_step:]\n",
    "\n",
    "            X    = all_tokens[take]                           # [tokens_per_step, d_in]\n",
    "            rest = all_tokens[keep]                           # [N - tokens_per_step, d_in]\n",
    "\n",
    "            # Carry forward the (already shuffled) remainder\n",
    "            buf = ([rest] if rest.numel() else [])\n",
    "            total = int(rest.size(0)) if rest.numel() else 0\n",
    "\n",
    "            yield X\n",
    "\n",
    "    # flush remainder (possibly a short batch)\n",
    "    if buf:\n",
    "        X = torch.cat(buf, dim=0)\n",
    "        if X.numel() > 0:\n",
    "            # optional: shuffle this short batch, too\n",
    "            idx = torch.randperm(X.size(0))\n",
    "            X = X[idx]\n",
    "            yield X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder (ReLU activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAE(nn.Module):\n",
    "    \"\"\"\n",
    "    One-layer sparse autoencoder (untied decoder).\n",
    "\n",
    "    Design:\n",
    "      x -- (center by bias) --> encoder (Linear) --> LayerNorm? --> ReLU --> f (sparse features)\n",
    "      f -- decoder(linear) --> + bias --> xhat\n",
    "\n",
    "    Shapes (let:\n",
    "    \n",
    "        B = batch size in *tokens* (each row is one token embedding),\n",
    "        D = input dim (ESM hidden size for a given layer, e.g., 320/480/640/1280)\n",
    "        K = number of features or Dictionary size\n",
    "    ):\n",
    "        -x: [B, D]\n",
    "        -f: [B, K]\n",
    "        -x_hat: [B, D]\n",
    "        -encoder.weight: [K, D] (each row maps input -> a feature pre-activation)\n",
    "        -decoder.weight: [D, K] (each column is a dictionary atom in input space)\n",
    "    \n",
    "    Notes:\n",
    "        - The learned bias centers inputs before encoding, then is added back after decoding.\n",
    "        - Decoder is *untied* (separate matrix from encoder) - standard in mech interp saes.\n",
    "        - ReLU enforces Nonnegativity/sparsity in f.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int, #D\n",
    "        d_hidden: int, #K\n",
    "        use_layernorm: bool=False,\n",
    "        init_unitnorm_decoder: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in #D\n",
    "        self.d_hidden = d_hidden #K\n",
    "\n",
    "        #Learned centering bias b \\ in R^D\n",
    "        self.bias = nn.Parameter(torch.zeros(d_in)) # [D]\n",
    "\n",
    "        #Encoder W_e: R^D => Expanded into R^K (plus bias)\n",
    "        self.encoder = nn.Linear(d_in, d_hidden, bias=True) #weight: [K, D], bias: [K]\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_hidden) if use_layernorm else nn.Identity()\n",
    "\n",
    "        #Decoder W_d: R^k => R^D (no bias; we add +b afterwards)\n",
    "        self.decoder = nn.Linear(d_hidden, d_in, bias=False) # weight: [D, K]\n",
    "\n",
    "        #Initialization\n",
    "\n",
    "        nn.init.xavier_uniform_(self.encoder.weight) #[K, D], _ represents in place operation\n",
    "        if self.encoder.bias is not None:\n",
    "            nn.init.zeros_(self.encoder.bias) #[K]\n",
    "        \n",
    "        if init_unitnorm_decoder:\n",
    "            #make decoder columns unit norm: each atom d_i = decoder.weight[:, i] / ||.||\n",
    "            with torch.no_grad():\n",
    "                W = torch.randn_like(self.decoder.weight) #[D, K]\n",
    "                W = W / (W.norm(dim=0, keepdim=True) + 1e-12)\n",
    "                self.decoder.weight.copy_(W)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.decoder.weight) #[D, K]\n",
    "    \n",
    "    #Core API\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, D]\n",
    "        returns f: [B, K] (sparse, nonnegatvie)\n",
    "        #Multiplication happens as x @ W_e.T we transpose for dimensions to work out\n",
    "        #[B, D] @ [D, K] => [B, K]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #cneter: (x - b) keeps encoder from wasting capacity ont he mean\n",
    "        f_pre = self.encoder(x - self.bias) #[B, K], encoder has weights [K, D]\n",
    "        f = F.relu(self.ln(f_pre)) #[B, K]\n",
    "        return f\n",
    "\n",
    "    def decode(self, f: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        f: [B, K]\n",
    "        returns x_hat: [B,D]\n",
    "\n",
    "        #Same thing here, where is f @ W_d.t + bias\n",
    "        so [B, K] @ [K, D]\n",
    "\n",
    "        but the decoder is actually [D, K] where the columns represent each feature\n",
    "        \"\"\"\n",
    "\n",
    "        x_hat_no_bias = self.decoder(f) #[B, D]\n",
    "        return x_hat_no_bias + self.bias #[B, D]\n",
    "\n",
    "    def forward(self, x:torch.Tensor, output_features: bool = False) -> Tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, D]\n",
    "        returns:\n",
    "            - if output_features=False: x_hat[B, D]  \n",
    "            - if output_features=True: (x_hat [B, D], f [B, K])    \n",
    "        \"\"\"\n",
    "        f = self.encode(x) #[B, K] feature activations for each dictionary vector\n",
    "        x_hat = self.decode(f) #[B, D]\n",
    "\n",
    "        return (x_hat, f) if output_features else x_hat\n",
    "    \n",
    "    #Helpers\n",
    "    @staticmethod\n",
    "    def l1(f: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mean absolute activation (sparsity penalty).\n",
    "        f: [B,K] -> scalar\n",
    "        \n",
    "        \"\"\"\n",
    "        return f.abs().mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def l0(f: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Average number of active (nonzero) features per example.\n",
    "        f: [B, K] -> scalar\n",
    "        \"\"\"\n",
    "        return (f > eps).float().sum(dim=-1).mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def rescale_features_(self, scale: torch.Tensor, eps: float=1e-8) -> None:\n",
    "        \"\"\"\n",
    "        Per-feature rescaling that keeps reconstructions invariant but changes feature scale.\n",
    "        Use this after estimating per-feature maxima or P99 values.\n",
    "        Goal: want f' = f/s, and x_hat' = x_hat.\n",
    "\n",
    "        Achieve by:\n",
    "            encoder.weight(rows) /= s\n",
    "            encoder.bias /= s\n",
    "            decoder.weight(cols) *= s\n",
    "        Args: \n",
    "            scale: [K] positive scale per feature (clipped ot >= eps)\n",
    "        \"\"\"\n",
    "        s = torch.clamp(scale.detach().to(self.encoder.weight.device), min=eps) #[K]\n",
    "\n",
    "        #encoder.weight: [K, D] (row i corresponds to feature i)\n",
    "        self.encoder.weight.data.div_(s[:, None])\n",
    "        if self.encoder.bias is not None:\n",
    "            self.encoder.bias.data.div_(s)\n",
    "        \n",
    "        #decoder.weight: [D, K] (column i corresponds to feature i)\n",
    "        self.decoder.weight.data.mul_(s[None, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAERunnerConfig:\n",
    "    d_in: int # ESM layer hidden size (D)\n",
    "    d_hidden: int #dictionary size (K)\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    lambda_l1: float = 0.1 #sparsity strength\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    amp_dtype: torch.dtype = torch.float16\n",
    "    use_amp: bool = True\n",
    "    device: str = \"cuda\"\n",
    "    save_dir: Optional[str] = None\n",
    "    log_every: int = 50 #Steps for printing\n",
    "    max_steps: Optional[int] = None #stops early if set\n",
    "\n",
    "class SAERunner: \n",
    "    def __init__(self, ae: nn.Module, cfg: SAERunnerConfig):\n",
    "        self.ae = ae.to(cfg.device)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.opt = torch.optim.AdamW(self.ae.parameters(), lr = cfg.lr, weight_decay=cfg.weight_decay)\n",
    "        self.scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "        if cfg.save_dir:\n",
    "            os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "            os.makedirs(os.path.join(cfg.save_dir, \"checkpoints\"), exist_ok=True)\n",
    "        self.step = 0\n",
    "    \n",
    "    def _step(self, x_cpu: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        x_cpu: [B, D] float32 on CPU from activation_batches.\n",
    "        Moves to device and runs one optimization step.\n",
    "        \"\"\"\n",
    "        # Move batch to device (e.g. cuda:0)\n",
    "        x = x_cpu.to(self.cfg.device, non_blocking=True)  # [B, D]\n",
    "\n",
    "        # Forward + loss\n",
    "        x_hat, f = self.ae(x, output_features=True)       # x_hat: [B, D], f: [B, K]\n",
    "        mse = F.mse_loss(x_hat, x)                        # reconstruction loss\n",
    "        l1  = f.abs().mean()                              # sparsity penalty\n",
    "        loss = mse + self.cfg.lambda_l1 * l1              # total loss\n",
    "\n",
    "        # Backward + step\n",
    "        self.opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        # Compute l0 (non-differentiable, just for logging)\n",
    "        with torch.no_grad():\n",
    "            l0 = (f > 1e-12).float().sum(dim=-1).mean().item()\n",
    "\n",
    "        return {\n",
    "            \"loss\": float(loss.item()),\n",
    "            \"mse\": float(mse.item()),\n",
    "            \"l1\": float(l1.item()),\n",
    "            \"l0\": float(l0),\n",
    "        }\n",
    "\n",
    "\n",
    "    def save(self, name: str):\n",
    "        if not self.cfg.save_dir:\n",
    "            return\n",
    "        path = os.path.join(self.cfg.save_dir, \"checkpoints\", f\"{name}.pt\")\n",
    "        torch.save({\n",
    "            \"step\": self.step,\n",
    "            \"ae_state_dict\": self.ae.state_dict(),\n",
    "            \"opt_state_dict\": self.opt.state_dict(),\n",
    "            \"cfg\": self.cfg.__dict__,\n",
    "        }, path)\n",
    "\n",
    "    def train_stream(self, stream: Iterable[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        stream: yields CPU float32 tensors X of shape [N_tokens, D].\n",
    "        \"\"\"\n",
    "        total_steps = self.cfg.max_steps if self.cfg.max_steps is not None else None\n",
    "\n",
    "        with tqdm(total=total_steps, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "            for x in stream:\n",
    "                stats = self._step(x)\n",
    "                self.step += 1\n",
    "\n",
    "                # Update bar\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": f\"{stats['loss']:.4f}\",\n",
    "                    \"mse\":  f\"{stats['mse']:.4f}\",\n",
    "                    \"l1\":   f\"{stats['l1']:.4f}\",\n",
    "                    \"l0\":   f\"{stats['l0']:.1f}\",\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "                if self.cfg.save_dir and (self.step % (self.cfg.log_every * 10) == 0):\n",
    "                    self.save(f\"ae_{self.step}\")\n",
    "\n",
    "                if self.cfg.max_steps is not None and self.step >= self.cfg.max_steps:\n",
    "                    break\n",
    "\n",
    "            # Final checkpoint\n",
    "            self.save(\"final\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sae_on_esm_stream(\n",
    "    fasta_gz: Path,\n",
    "    d_in: int, #ESM layer hidden dim, e.g. 320/480/640/1280\n",
    "    d_hidden: int, #dictionary size(features), e.g. 32 * d_in\n",
    "    tokens_per_step: int,\n",
    "    max_steps: int,\n",
    "    save_dir: str,\n",
    "    *,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    layer_sel = LAYER_TO_TRAIN,\n",
    "):\n",
    "# 1) build model\n",
    "\n",
    "    ae = SAE(d_in = d_in, d_hidden=d_hidden, use_layernorm=False,init_unitnorm_decoder=True)\n",
    "\n",
    "    #runner config\n",
    "    cfg = SAERunnerConfig(\n",
    "        d_in=d_in,\n",
    "        d_hidden=d_hidden,\n",
    "        lr = 1e-3,\n",
    "        weight_decay=0.0,\n",
    "        lambda_l1 = 0.1, #tune based on desired sparsity\n",
    "        grad_clip = 1.0,\n",
    "        use_amp=False,\n",
    "        amp_dtype=torch.float16,\n",
    "        device=DEVICE, \n",
    "        save_dir = save_dir,\n",
    "        log_every=50,\n",
    "        max_steps=max_steps,\n",
    "    )\n",
    "\n",
    "    runner = SAERunner(ae, cfg)\n",
    "\n",
    "    #3) stream activations and train\n",
    "    stream = activation_batches(\n",
    "        fasta_gz = fasta_gz,\n",
    "        tokens_per_step = tokens_per_step,\n",
    "        layer_sel = layer_sel,\n",
    "        dtype=torch.float16,\n",
    "        model = model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    runner.train_stream(stream)\n",
    "    return runner.ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61325/3094311539.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(enabled=cfg.use_amp)\n",
      "Training:   1%|          | 47/5000 [00:13<23:07,  3.57batch/s, loss=26.1822, mse=26.1697, l1=0.1245, l0=1504.1]    \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 46.94 MiB is free. Including non-PyTorch memory, this process has 79.20 GiB memory in use. Of the allocated memory 74.81 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m DEVICE=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m d_in = mdl.config.hidden_size  \u001b[38;5;66;03m# SAE input dimension\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m ae = \u001b[43mtrain_sae_on_esm_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfasta_gz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfasta_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_in\u001b[49m\u001b[43m=\u001b[49m\u001b[43mESM_D_IN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_hidden\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens_per_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOKENS_PER_STEP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_sae_on_esm_stream\u001b[39m\u001b[34m(fasta_gz, d_in, d_hidden, tokens_per_step, max_steps, save_dir, model, tokenizer, layer_sel)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#3) stream activations and train\u001b[39;00m\n\u001b[32m     36\u001b[39m stream = activation_batches(\n\u001b[32m     37\u001b[39m     fasta_gz = fasta_gz,\n\u001b[32m     38\u001b[39m     tokens_per_step = tokens_per_step,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     tokenizer=tokenizer\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m runner.ae\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mSAERunner.train_stream\u001b[39m\u001b[34m(self, stream)\u001b[39m\n\u001b[32m     75\u001b[39m total_steps = \u001b[38;5;28mself\u001b[39m.cfg.max_steps \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.max_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=total_steps, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/utils/_contextlib.py:57\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m                 response = gen.send(request)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mactivation_batches\u001b[39m\u001b[34m(fasta_gz, tokens_per_step, layer_sel, dtype, model, tokenizer)\u001b[39m\n\u001b[32m     13\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seq_batch \u001b[38;5;129;01min\u001b[39;00m batched_sequences(fasta_gz, BATCH_SIZE_SEQ):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     reps, attn = \u001b[43mextract_esm_features_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_sel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_sel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#reps: [B,L,d], attn:[B, L]\u001b[39;00m\n\u001b[32m     24\u001b[39m     B, L, D = reps.shape \u001b[38;5;66;03m#Batch, sequence length, hidden dimension = 1280\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B): \u001b[38;5;66;03m#for sequence in the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/InterPLM/utils.py:141\u001b[39m, in \u001b[36mextract_esm_features_batch\u001b[39m\u001b[34m(sequences, layer_sel, device, dtype, model, tokenizer)\u001b[39m\n\u001b[32m    138\u001b[39m attn_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(torch.bool)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, dtype=dtype):\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     hs = out.hidden_states  \u001b[38;5;66;03m# tuple: [emb, layer1, ..., layerN] each [B,L,d]\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m layer_sel == \u001b[33m\"\u001b[39m\u001b[33mlast\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1062\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py:749\u001b[39m, in \u001b[36mEsmModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    746\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    747\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    758\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/utils/generic.py:940\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    939\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    942\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py:558\u001b[39m, in \u001b[36mEsmEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, **kwargs)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m    557\u001b[39m     layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     hidden_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.emb_layer_norm_after:\n\u001b[32m    568\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.emb_layer_norm_after(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/utils/generic.py:1024\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1022\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1026\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/utils/generic.py:1024\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper.<locals>.make_capture_wrapper.<locals>.wrapped_forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1022\u001b[39m         output = orig_forward(*args, **kwargs)\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     output = \u001b[43morig_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1026\u001b[39m     collected_outputs[key] += (output,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py:528\u001b[39m, in \u001b[36mEsmLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, **kwargs)\u001b[39m\n\u001b[32m    514\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    515\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIf `encoder_hidden_states` are passed, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has to be instantiated\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m with cross-attention layers by setting `config.add_cross_attention=True`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    517\u001b[39m         )\n\u001b[32m    519\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.crossattention(\n\u001b[32m    520\u001b[39m         attention_output,\n\u001b[32m    521\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    525\u001b[39m         **kwargs,\n\u001b[32m    526\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py:533\u001b[39m, in \u001b[36mEsmLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    532\u001b[39m     attention_output_ln = \u001b[38;5;28mself\u001b[39m.LayerNorm(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output_ln\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py:463\u001b[39m, in \u001b[36mEsmIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m    462\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dense(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     hidden_states = \u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py:61\u001b[39m, in \u001b[36mgelu\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgelu\u001b[39m(x):\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    This is the gelu implementation from the original ESM repo. Using F.gelu yields subtly wrong results.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x * \u001b[32m0.5\u001b[39m * (\u001b[32m1.0\u001b[39m + \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43merf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 46.94 MiB is free. Including non-PyTorch memory, this process has 79.20 GiB memory in use. Of the allocated memory 74.81 GiB is allocated by PyTorch, and 3.88 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "fasta_path = Path(\"/home/ec2-user/SageMaker/InterPLM/data/uniprot/uniprot_sprot.fasta.gz\")\n",
    "\n",
    "TOKENS_PER_STEP = 2048\n",
    "MAX_STEPS = 5000\n",
    "SAVE_DIR = f\"./sae_runs/layer_{LAYER_TO_TRAIN}_32x\"\n",
    "\n",
    "# Choose layer and dims\n",
    "ESM_D_IN = 1280                # depends on your ESM checkpoint\n",
    "EXPANSION = 32\n",
    "K = EXPANSION * ESM_D_IN\n",
    "DEVICE=\"cuda\"\n",
    "\n",
    "d_in = mdl.config.hidden_size  # SAE input dimension\n",
    "\n",
    "ae = train_sae_on_esm_stream(\n",
    "    fasta_gz=fasta_path,\n",
    "    d_in=ESM_D_IN,\n",
    "    d_hidden=K,\n",
    "    tokens_per_step=TOKENS_PER_STEP,\n",
    "    max_steps=MAX_STEPS,\n",
    "    save_dir=SAVE_DIR,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 ('interplm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1106d1d489397abf5d77132595a521cf67d890f951d991cd34215b053d2a27e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
