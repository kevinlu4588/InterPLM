{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip, math, random, json, gc\n",
    "from pathlib import Path\n",
    "from typing import List, Iterable, Tuple, Dict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from utils import extract_esm_features_batch\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import time\n",
    "from utils import extract_esm_features_batch_layer_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"uniprot_sprot\"\n",
    "ESM_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "MAX_LEN = 1024 #truncate longer sequences\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "BATCH_SIZE_SEQ = 128\n",
    "LAYER_TO_TRAIN=24\n",
    "TOKENS_PER_STEP = 4096 #otken vectors per SAE update\n",
    "\n",
    "\n",
    "FASTA_GZ = Path(\"/home/ec2-user/SageMaker/InterPLM/data/uniprot/uniprot_sprot.fasta.gz\")\n",
    "OUT_DIR  = Path(f\"/home/ec2-user/SageMaker/InterPLM/data/esm2_hidden_states/{dataset_name}\")\n",
    "\n",
    "# ---------- Device ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ESM_NAME, do_lower_case=False)\n",
    "model = AutoModel.from_pretrained(ESM_NAME).eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Swissprot protein sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_swissprot_sequences(fasta_gz: Path, max_len: int = MAX_LEN):\n",
    "    with gzip.open(fasta_gz, \"rt\") as fh:\n",
    "        for rec in SeqIO.parse(fh, \"fasta\"):\n",
    "            seq = str(rec.seq)\n",
    "            if not seq:\n",
    "                continue\n",
    "            yield rec.id, (seq[:max_len] if len(seq) > max_len else seq)\n",
    "\n",
    "def batched_sequences(fasta_gz: Path, batch_size: int = BATCH_SIZE_SEQ):\n",
    "    buf = []\n",
    "    for _, seq in iter_swissprot_sequences(fasta_gz):\n",
    "        buf.append(seq)\n",
    "        if len(buf) == batch_size:\n",
    "            yield buf\n",
    "            buf = []\n",
    "    if buf:\n",
    "        yield buf #yields array of [B, L] sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP_DTYPE = torch.float16 if device.type == \"cuda\" else torch.bfloat16  # safe default\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "if device.type == \"cuda\":\n",
    "    # helps on A100/H100; no effect on CPU\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# ---------- IO helpers ----------\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_json(path: Path, obj: dict):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\") as fh:\n",
    "        json.dump(obj, fh, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smarter storing of ESM Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Saver ----------\n",
    "def save_batch_shards(\n",
    "    hs: torch.Tensor,          # [L, B, T, D]\n",
    "    attn_mask: torch.Tensor,   # [B, T]\n",
    "    out_dir: Path,\n",
    "    batch_idx: int,\n",
    "    *,\n",
    "    save_masks_once: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Saves:\n",
    "      - masks/batch_XXXXX.pt with attention mask & per-seq lengths (once per batch)\n",
    "      - layer_XX/batch_XXXXX.pt with the shard for that layer\n",
    "    Skips files that already exist.\n",
    "    \"\"\"\n",
    "    L, B, T, D = hs.shape\n",
    "    # Save the mask once per batch\n",
    "    mask_path = out_dir / \"masks\" / f\"batch_{batch_idx:05d}.pt\"\n",
    "    if save_masks_once and not mask_path.exists():\n",
    "        ensure_dir(mask_path.parent)\n",
    "        seq_lens = attn_mask.sum(dim=1).to(torch.int32).cpu()\n",
    "        torch.save(\n",
    "            {\n",
    "                \"attn_mask\": attn_mask.cpu(),\n",
    "                \"seq_lengths\": seq_lens,\n",
    "                \"batch_size\": int(B),\n",
    "                \"tokens\": int(T),\n",
    "                \"batch_index\": int(batch_idx),\n",
    "            },\n",
    "            mask_path,\n",
    "        )\n",
    "\n",
    "    # Per-layer shards\n",
    "    for l in range(L):\n",
    "        layer_dir = out_dir / f\"layer_{l:02d}\"\n",
    "        ensure_dir(layer_dir)\n",
    "        shard_path = layer_dir / f\"batch_{batch_idx:05d}.pt\"\n",
    "        if shard_path.exists():\n",
    "            continue\n",
    "        torch.save(\n",
    "            {\n",
    "                \"hidden_states\": hs[l].cpu(),  # [B, T, D]\n",
    "                \"batch_size\": int(B),\n",
    "                \"tokens\": int(T),\n",
    "                \"width\": int(D),\n",
    "                \"layer\": int(l),\n",
    "                \"batch_index\": int(batch_idx),\n",
    "            },\n",
    "            shard_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Manifest ----------\n",
    "def write_manifest(\n",
    "    out_dir: Path,\n",
    "    *,\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    max_len: int,\n",
    "    batch_size: int,\n",
    "    num_layers: int,\n",
    "    hidden_size: int,\n",
    "    total_batches: int,\n",
    "    total_sequences: int,\n",
    "):\n",
    "    manifest = {\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_name\": model_name,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"max_len\": max_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_layers\": num_layers,  # includes embedding layer\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"total_batches\": total_batches,\n",
    "        \"total_sequences\": total_sequences,\n",
    "        \"layout\": {\n",
    "            \"layers_folder\": \"layer_{:02d}\",\n",
    "            \"layer_shard\": \"batch_{:05d}.pt\",\n",
    "            \"masks_folder\": \"masks\",\n",
    "            \"mask_shard\": \"batch_{:05d}.pt\",\n",
    "            \"tensor_keys\": {\n",
    "                \"hidden_states\": \"[B, T, D]\",\n",
    "                \"attn_mask\": \"[B, T]\"\n",
    "            }\n",
    "        },\n",
    "        \"notes\": \"Per-layer shards by batch; attention mask saved once per batch under masks/.\"\n",
    "    }\n",
    "    write_json(out_dir / \"manifest.json\", manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573661\n",
      "4482\n"
     ]
    }
   ],
   "source": [
    "def count_sequences(fasta_gz: Path) -> int:\n",
    "    with gzip.open(fasta_gz, \"rt\") as fh:\n",
    "        return sum(1 for _ in SeqIO.parse(fh, \"fasta\"))\n",
    "\n",
    "n_seqs = count_sequences(FASTA_GZ)\n",
    "n_batches = math.ceil(n_seqs / BATCH_SIZE_SEQ)\n",
    "print(n_seqs)\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_fasta_to_layer_shards(\n",
    "    fasta_gz: Path = FASTA_GZ,\n",
    "    out_dir: Path = OUT_DIR,\n",
    "    *,\n",
    "    max_batches: Optional[int] = None,\n",
    "):\n",
    "\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    # Prime one mini-batch to discover (L, D)\n",
    "    probe_batch = next(batched_sequences(fasta_gz, batch_size=min(4, BATCH_SIZE_SEQ)))\n",
    "    hs_probe, attn_probe = extract_esm_features_batch_layer_all(\n",
    "        probe_batch, model=model, tokenizer=tokenizer, device=device, dtype=AMP_DTYPE\n",
    "    )\n",
    "    L, Bp, Tp, D = hs_probe.shape\n",
    "    del hs_probe, attn_probe\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Pre-create layer folders\n",
    "    for l in range(L):\n",
    "        ensure_dir(out_dir / f\"layer_{l:02d}\")\n",
    "    ensure_dir(out_dir / \"masks\")\n",
    "\n",
    "    #Main pass\n",
    "    total_sequences = 0\n",
    "    batch_counter = 0\n",
    "    for batch_seqs in tqdm(batched_sequences(fasta_gz, BATCH_SIZE_SEQ), total=n_batches, desc=\"Batches\"):\n",
    "        batch_counter += 1\n",
    "        if max_batches is not None and batch_counter > max_batches:\n",
    "            break\n",
    "\n",
    "        #skip batch if every layer shard already exists\n",
    "        already_done = True\n",
    "        for l in range(L):\n",
    "            shard_path = out_dir / f\"layer_{l:02d}\" / f\"batch_{batch_counter:05d}.pt\"\n",
    "            if not shard_path.exists():\n",
    "                already_done = False\n",
    "                break\n",
    "        mask_path = out_dir / \"masks\" / f\"batch_{batch_counter:05d}.pt\"\n",
    "        if already_done and mask_path.exists():\n",
    "            total_sequences += len(batch_seqs)\n",
    "            continue\n",
    "\n",
    "\n",
    "        #Compute\n",
    "        hs, attn_mask = extract_esm_features_batch_layer_all(batch_seqs, model=model, tokenizer=tokenizer, device=device, dtype=AMP_DTYPE)\n",
    "\n",
    "        #save\n",
    "        save_batch_shards(hs, attn_mask, out_dir, batch_counter)\n",
    "        total_sequences += len(batch_seqs)\n",
    "\n",
    "        del hs, attn_mask\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    #Finalize manifest\n",
    "    write_manifest(\n",
    "        out_dir,\n",
    "        model_name=ESM_NAME,\n",
    "        dataset_name = dataset_name,\n",
    "        max_len = MAX_LEN,\n",
    "        batch_size=BATCH_SIZE_SEQ,\n",
    "        num_layers=L,\n",
    "        hidden_size=D,\n",
    "        total_batches= batch_counter if max_batches is None else min(batch_counter, max_batches),\n",
    "        total_sequences=total_sequences,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab ESM activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 3/4482 [00:00<00:06, 727.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_all_fasta_to_layer_shards(FASTA_GZ, OUT_DIR, max_batches=3)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab ESM activations in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def activation_batches(\n",
    "#     fasta_gz: Path,\n",
    "#     tokens_per_step: int = TOKENS_PER_STEP,\n",
    "#     layer_sel=LAYER_TO_TRAIN,\n",
    "#     dtype = torch.float16,\n",
    "#     *,\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "# ) -> Iterable[torch.Tensor]:\n",
    "\n",
    "#     buf = []\n",
    "#     total = 0\n",
    "#     for seq_batch in batched_sequences(fasta_gz, BATCH_SIZE_SEQ):\n",
    "#         reps, attn = extract_esm_features_batch(\n",
    "#             sequences=seq_batch,\n",
    "#             layer_sel=layer_sel,\n",
    "#             device=DEVICE,\n",
    "#             dtype=dtype,\n",
    "#             model=model,\n",
    "#             tokenizer=tokenizer,\n",
    "#         ) #reps: [B,L,d], attn:[B, L]\n",
    "\n",
    "#         B, L, D = reps.shape #Batch, sequence length, hidden dimension = 1280\n",
    "#         for b in range(B): #for sequence in the batch\n",
    "#             valid = attn[b] # bool[L], which ones were we supposed to attend to?\n",
    "#             if valid.any():\n",
    "#                 buf.append(reps[b][valid].detach().to(\"cpu\", dtype=torch.float32)) #float32 for SAE\n",
    "#                 total += int(valid.sum())\n",
    "     \n",
    "#         # When we have enough tokens, build a randomized batch and a randomized remainder.\n",
    "#         #Yield turns the function into a generator, so the local variables are preserved between next() calls. once we drop references to it or stop iteration, it gets cleaned up\n",
    "#         if total >= tokens_per_step:\n",
    "#             all_tokens = torch.cat(buf, dim=0)                # [N, d_in]\n",
    "#             N = all_tokens.size(0)\n",
    "#             perm = torch.randperm(N)                          # random order\n",
    "#             take = perm[:tokens_per_step]\n",
    "#             keep = perm[tokens_per_step:]\n",
    "\n",
    "#             X    = all_tokens[take]                           # [tokens_per_step, d_in]\n",
    "#             rest = all_tokens[keep]                           # [N - tokens_per_step, d_in]\n",
    "\n",
    "#             # Carry forward the (already shuffled) remainder\n",
    "#             buf = ([rest] if rest.numel() else [])\n",
    "#             total = int(rest.size(0)) if rest.numel() else 0\n",
    "\n",
    "#             yield X\n",
    "\n",
    "#     # flush remainder (possibly a short batch)\n",
    "#     if buf:\n",
    "#         X = torch.cat(buf, dim=0)\n",
    "#         if X.numel() > 0:\n",
    "#             # optional: shuffle this short batch, too\n",
    "#             idx = torch.randperm(X.size(0))\n",
    "#             X = X[idx]\n",
    "#             yield X\n",
    "\n",
    "@torch.no_grad()\n",
    "def activation_batches(\n",
    "    fasta_gz: Path,\n",
    "    tokens_per_step: int = TOKENS_PER_STEP,\n",
    "    layer_sel=LAYER_TO_TRAIN,\n",
    "    dtype = torch.float16,\n",
    "    *,\n",
    "    model,\n",
    "    tokenizer,\n",
    ") -> Iterable[torch.Tensor]:\n",
    "\n",
    "    buf = []  # Keep as GPU tensors!\n",
    "    total = 0\n",
    "\n",
    "    for seq_batch in batched_sequences(fasta_gz, BATCH_SIZE_SEQ):\n",
    "        reps, attn = extract_esm_features_batch(\n",
    "            sequences=seq_batch,\n",
    "            layer_sel=layer_sel,\n",
    "            device=DEVICE,\n",
    "            dtype=dtype,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "        ) #reps: [B,L,d], attn:[B, L]\n",
    "\n",
    "        B, L, D = reps.shape\n",
    "        for b in range(B):\n",
    "            valid = attn[b]\n",
    "            if valid.any():\n",
    "                # Keep on GPU instead of moving to CPU\n",
    "                buf.append(reps[b][valid].detach())  # Stay on GPU!\n",
    "                total += int(valid.sum())\n",
    "\n",
    "        if total >= tokens_per_step:\n",
    "            # Concatenate on GPU\n",
    "            all_tokens = torch.cat(buf, dim=0)\n",
    "            N = all_tokens.size(0)\n",
    "            perm = torch.randperm(N, device=all_tokens.device)  # GPU randperm\n",
    "\n",
    "            X = all_tokens[perm[:tokens_per_step]]\n",
    "            rest = all_tokens[perm[tokens_per_step:]]\n",
    "\n",
    "            buf = ([rest] if rest.numel() else [])\n",
    "            total = int(rest.size(0)) if rest.numel() else 0\n",
    "\n",
    "            yield X  # Already on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder (ReLU activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAE(nn.Module):\n",
    "    \"\"\"\n",
    "    One-layer sparse autoencoder (untied decoder).\n",
    "\n",
    "    Design:\n",
    "      x -- (center by bias) --> encoder (Linear) --> LayerNorm? --> ReLU --> f (sparse features)\n",
    "      f -- decoder(linear) --> + bias --> xhat\n",
    "\n",
    "    Shapes (let:\n",
    "    \n",
    "        B = batch size in *tokens* (each row is one token embedding),\n",
    "        D = input dim (ESM hidden size for a given layer, e.g., 320/480/640/1280)\n",
    "        K = number of features or Dictionary size\n",
    "    ):\n",
    "        -x: [B, D]\n",
    "        -f: [B, K]\n",
    "        -x_hat: [B, D]\n",
    "        -encoder.weight: [K, D] (each row maps input -> a feature pre-activation)\n",
    "        -decoder.weight: [D, K] (each column is a dictionary atom in input space)\n",
    "    \n",
    "    Notes:\n",
    "        - The learned bias centers inputs before encoding, then is added back after decoding.\n",
    "        - Decoder is *untied* (separate matrix from encoder) - standard in mech interp saes.\n",
    "        - ReLU enforces Nonnegativity/sparsity in f.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int, #D\n",
    "        d_hidden: int, #K\n",
    "        use_layernorm: bool=False,\n",
    "        init_unitnorm_decoder: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in #D\n",
    "        self.d_hidden = d_hidden #K\n",
    "\n",
    "        #Learned centering bias b \\ in R^D\n",
    "        self.bias = nn.Parameter(torch.zeros(d_in)) # [D]\n",
    "\n",
    "        #Encoder W_e: R^D => Expanded into R^K (plus bias)\n",
    "        self.encoder = nn.Linear(d_in, d_hidden, bias=True) #weight: [K, D], bias: [K]\n",
    "\n",
    "        self.ln = nn.LayerNorm(d_hidden) if use_layernorm else nn.Identity()\n",
    "\n",
    "        #Decoder W_d: R^k => R^D (no bias; we add +b afterwards)\n",
    "        self.decoder = nn.Linear(d_hidden, d_in, bias=False) # weight: [D, K]\n",
    "\n",
    "        #Initialization\n",
    "\n",
    "        nn.init.xavier_uniform_(self.encoder.weight) #[K, D], _ represents in place operation\n",
    "        if self.encoder.bias is not None:\n",
    "            nn.init.zeros_(self.encoder.bias) #[K]\n",
    "        \n",
    "        if init_unitnorm_decoder:\n",
    "            #make decoder columns unit norm: each atom d_i = decoder.weight[:, i] / ||.||\n",
    "            with torch.no_grad():\n",
    "                W = torch.randn_like(self.decoder.weight) #[D, K]\n",
    "                W = W / (W.norm(dim=0, keepdim=True) + 1e-12)\n",
    "                self.decoder.weight.copy_(W)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.decoder.weight) #[D, K]\n",
    "    \n",
    "    #Core API\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, D]\n",
    "        returns f: [B, K] (sparse, nonnegatvie)\n",
    "        #Multiplication happens as x @ W_e.T we transpose for dimensions to work out\n",
    "        #[B, D] @ [D, K] => [B, K]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #cneter: (x - b) keeps encoder from wasting capacity ont he mean\n",
    "        f_pre = self.encoder(x - self.bias) #[B, K], encoder has weights [K, D]\n",
    "        f = F.relu(self.ln(f_pre)) #[B, K]\n",
    "        return f\n",
    "\n",
    "    def decode(self, f: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        f: [B, K]\n",
    "        returns x_hat: [B,D]\n",
    "\n",
    "        #Same thing here, where is f @ W_d.t + bias\n",
    "        so [B, K] @ [K, D]\n",
    "\n",
    "        but the decoder is actually [D, K] where the columns represent each feature\n",
    "        \"\"\"\n",
    "\n",
    "        x_hat_no_bias = self.decoder(f) #[B, D]\n",
    "        return x_hat_no_bias + self.bias #[B, D]\n",
    "\n",
    "    def forward(self, x:torch.Tensor, output_features: bool = False) -> Tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, D]\n",
    "        returns:\n",
    "            - if output_features=False: x_hat[B, D]  \n",
    "            - if output_features=True: (x_hat [B, D], f [B, K])    \n",
    "        \"\"\"\n",
    "        f = self.encode(x) #[B, K] feature activations for each dictionary vector\n",
    "        x_hat = self.decode(f) #[B, D]\n",
    "\n",
    "        return (x_hat, f) if output_features else x_hat\n",
    "    \n",
    "    #Helpers\n",
    "    @staticmethod\n",
    "    def l1(f: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Mean absolute activation (sparsity penalty).\n",
    "        f: [B,K] -> scalar\n",
    "        \n",
    "        \"\"\"\n",
    "        return f.abs().mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def l0(f: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Average number of active (nonzero) features per example.\n",
    "        f: [B, K] -> scalar\n",
    "        \"\"\"\n",
    "        return (f > eps).float().sum(dim=-1).mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def rescale_features_(self, scale: torch.Tensor, eps: float=1e-8) -> None:\n",
    "        \"\"\"\n",
    "        Per-feature rescaling that keeps reconstructions invariant but changes feature scale.\n",
    "        Use this after estimating per-feature maxima or P99 values.\n",
    "        Goal: want f' = f/s, and x_hat' = x_hat.\n",
    "\n",
    "        Achieve by:\n",
    "            encoder.weight(rows) /= s\n",
    "            encoder.bias /= s\n",
    "            decoder.weight(cols) *= s\n",
    "        Args: \n",
    "            scale: [K] positive scale per feature (clipped ot >= eps)\n",
    "        \"\"\"\n",
    "        s = torch.clamp(scale.detach().to(self.encoder.weight.device), min=eps) #[K]\n",
    "\n",
    "        #encoder.weight: [K, D] (row i corresponds to feature i)\n",
    "        self.encoder.weight.data.div_(s[:, None])\n",
    "        if self.encoder.bias is not None:\n",
    "            self.encoder.bias.data.div_(s)\n",
    "        \n",
    "        #decoder.weight: [D, K] (column i corresponds to feature i)\n",
    "        self.decoder.weight.data.mul_(s[None, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SAERunnerConfig:\n",
    "    d_in: int # ESM layer hidden size (D)\n",
    "    d_hidden: int #dictionary size (K)\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    lambda_l1: float = 0.1 #sparsity strength\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    amp_dtype: torch.dtype = torch.float16\n",
    "    use_amp: bool = True\n",
    "    device: str = \"cuda\"\n",
    "    save_dir: Optional[str] = None\n",
    "    log_every: int = 50 #Steps for printing\n",
    "    max_steps: Optional[int] = None #stops early if set\n",
    "\n",
    "class SAERunner: \n",
    "    def __init__(self, ae: nn.Module, cfg: SAERunnerConfig):\n",
    "        self.ae = ae.to(cfg.device)\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.opt = torch.optim.AdamW(self.ae.parameters(), lr = cfg.lr, weight_decay=cfg.weight_decay)\n",
    "        self.scaler = GradScaler(enabled=cfg.use_amp)\n",
    "\n",
    "        if cfg.save_dir:\n",
    "            os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "            os.makedirs(os.path.join(cfg.save_dir, \"checkpoints\"), exist_ok=True)\n",
    "        self.step = 0\n",
    "    \n",
    "    def _step(self, x_cpu: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        x_cpu: [B, D] float32 on CPU from activation_batches.\n",
    "        Moves to device and runs one optimization step.\n",
    "        \"\"\"\n",
    "        # Move batch to device (e.g. cuda:0)\n",
    "        x = x_cpu.to(self.cfg.device, non_blocking=True)  # [B, D]\n",
    "\n",
    "        # Forward + loss\n",
    "        x_hat, f = self.ae(x, output_features=True)       # x_hat: [B, D], f: [B, K]\n",
    "        mse = F.mse_loss(x_hat, x)                        # reconstruction loss\n",
    "        l1  = f.abs().mean()                              # sparsity penalty\n",
    "        loss = mse + self.cfg.lambda_l1 * l1              # total loss\n",
    "\n",
    "        # Backward + step\n",
    "        self.opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        # Compute l0 (non-differentiable, just for logging)\n",
    "        with torch.no_grad():\n",
    "            l0 = (f > 1e-12).float().sum(dim=-1).mean().item()\n",
    "\n",
    "        return {\n",
    "            \"loss\": float(loss.item()),\n",
    "            \"mse\": float(mse.item()),\n",
    "            \"l1\": float(l1.item()),\n",
    "            \"l0\": float(l0),\n",
    "        }\n",
    "\n",
    "\n",
    "    def save(self, name: str):\n",
    "        if not self.cfg.save_dir:\n",
    "            return\n",
    "        path = os.path.join(self.cfg.save_dir, \"checkpoints\", f\"{name}.pt\")\n",
    "        torch.save({\n",
    "            \"step\": self.step,\n",
    "            \"ae_state_dict\": self.ae.state_dict(),\n",
    "            \"opt_state_dict\": self.opt.state_dict(),\n",
    "            \"cfg\": self.cfg.__dict__,\n",
    "        }, path)\n",
    "\n",
    "    def train_stream(self, stream: Iterable[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        stream: yields CPU float32 tensors X of shape [N_tokens, D].\n",
    "        \"\"\"\n",
    "        total_steps = self.cfg.max_steps if self.cfg.max_steps is not None else None\n",
    "\n",
    "        with tqdm(total=total_steps, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "            for x in stream:\n",
    "                stats = self._step(x)\n",
    "                self.step += 1\n",
    "\n",
    "                # Update bar\n",
    "                pbar.set_postfix({\n",
    "                    \"loss\": f\"{stats['loss']:.4f}\",\n",
    "                    \"mse\":  f\"{stats['mse']:.4f}\",\n",
    "                    \"l1\":   f\"{stats['l1']:.4f}\",\n",
    "                    \"l0\":   f\"{stats['l0']:.1f}\",\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "                if self.cfg.save_dir and (self.step % (self.cfg.log_every * 10) == 0):\n",
    "                    self.save(f\"ae_{self.step}\")\n",
    "\n",
    "                if self.cfg.max_steps is not None and self.step >= self.cfg.max_steps:\n",
    "                    break\n",
    "\n",
    "            # Final checkpoint\n",
    "            self.save(\"final\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sae_on_esm_stream(\n",
    "    fasta_gz: Path,\n",
    "    d_in: int, #ESM layer hidden dim, e.g. 320/480/640/1280\n",
    "    d_hidden: int, #dictionary size(features), e.g. 32 * d_in\n",
    "    tokens_per_step: int,\n",
    "    max_steps: int,\n",
    "    save_dir: str,\n",
    "    *,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    layer_sel = LAYER_TO_TRAIN,\n",
    "):\n",
    "# 1) build model\n",
    "\n",
    "    ae = SAE(d_in = d_in, d_hidden=d_hidden, use_layernorm=False,init_unitnorm_decoder=True)\n",
    "\n",
    "    #runner config\n",
    "    cfg = SAERunnerConfig(\n",
    "        d_in=d_in,\n",
    "        d_hidden=d_hidden,\n",
    "        lr = 1e-3,\n",
    "        weight_decay=0.0,\n",
    "        lambda_l1 = 0.1, #tune based on desired sparsity\n",
    "        grad_clip = 1.0,\n",
    "        use_amp=False,\n",
    "        amp_dtype=torch.float16,\n",
    "        device=DEVICE, \n",
    "        save_dir = save_dir,\n",
    "        log_every=50,\n",
    "        max_steps=max_steps,\n",
    "    )\n",
    "\n",
    "    runner = SAERunner(ae, cfg)\n",
    "\n",
    "    #3) stream activations and train\n",
    "    stream = activation_batches(\n",
    "        fasta_gz = fasta_gz,\n",
    "        tokens_per_step = tokens_per_step,\n",
    "        layer_sel = layer_sel,\n",
    "        dtype=torch.float16,\n",
    "        model = model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    runner.train_stream(stream)\n",
    "    return runner.ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2065/3094311539.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(enabled=cfg.use_amp)\n",
      "Training:  17%|█▋        | 852/5000 [08:18<40:26,  1.71batch/s, loss=0.2201, mse=0.1773, l1=0.4285, l0=3230.3]     \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.49 GiB. GPU 0 has a total capacity of 79.25 GiB of which 14.76 GiB is free. Including non-PyTorch memory, this process has 64.49 GiB memory in use. Of the allocated memory 43.52 GiB is allocated by PyTorch, and 20.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m K = EXPANSION * ESM_D_IN\n\u001b[32m     11\u001b[39m DEVICE=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ae = \u001b[43mtrain_sae_on_esm_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfasta_gz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfasta_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_in\u001b[49m\u001b[43m=\u001b[49m\u001b[43mESM_D_IN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43md_hidden\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens_per_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOKENS_PER_STEP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_STEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAVE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_sae_on_esm_stream\u001b[39m\u001b[34m(fasta_gz, d_in, d_hidden, tokens_per_step, max_steps, save_dir, model, tokenizer, layer_sel)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#3) stream activations and train\u001b[39;00m\n\u001b[32m     36\u001b[39m stream = activation_batches(\n\u001b[32m     37\u001b[39m     fasta_gz = fasta_gz,\n\u001b[32m     38\u001b[39m     tokens_per_step = tokens_per_step,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     tokenizer=tokenizer\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m runner.ae\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mSAERunner.train_stream\u001b[39m\u001b[34m(self, stream)\u001b[39m\n\u001b[32m     75\u001b[39m total_steps = \u001b[38;5;28mself\u001b[39m.cfg.max_steps \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.max_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=total_steps, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, unit=\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/SageMaker/.cs/conda/envs/interplm/lib/python3.11/site-packages/torch/utils/_contextlib.py:57\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m                 response = gen.send(request)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 96\u001b[39m, in \u001b[36mactivation_batches\u001b[39m\u001b[34m(fasta_gz, tokens_per_step, layer_sel, dtype, model, tokenizer)\u001b[39m\n\u001b[32m     93\u001b[39m perm = torch.randperm(N, device=all_tokens.device)  \u001b[38;5;66;03m# GPU randperm\u001b[39;00m\n\u001b[32m     95\u001b[39m X = all_tokens[perm[:tokens_per_step]]\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m rest = \u001b[43mall_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens_per_step\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     98\u001b[39m buf = ([rest] \u001b[38;5;28;01mif\u001b[39;00m rest.numel() \u001b[38;5;28;01melse\u001b[39;00m [])\n\u001b[32m     99\u001b[39m total = \u001b[38;5;28mint\u001b[39m(rest.size(\u001b[32m0\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m rest.numel() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 16.49 GiB. GPU 0 has a total capacity of 79.25 GiB of which 14.76 GiB is free. Including non-PyTorch memory, this process has 64.49 GiB memory in use. Of the allocated memory 43.52 GiB is allocated by PyTorch, and 20.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "fasta_path = Path(\"/home/ec2-user/SageMaker/InterPLM/data/uniprot/uniprot_sprot.fasta.gz\")\n",
    "\n",
    "TOKENS_PER_STEP = 2048\n",
    "MAX_STEPS = 5000\n",
    "SAVE_DIR = f\"./sae_runs/layer_{LAYER_TO_TRAIN}_32x\"\n",
    "\n",
    "# Choose layer and dims\n",
    "ESM_D_IN = 1280                # depends on your ESM checkpoint\n",
    "EXPANSION = 32\n",
    "K = EXPANSION * ESM_D_IN\n",
    "DEVICE=\"cuda\"\n",
    "\n",
    "ae = train_sae_on_esm_stream(\n",
    "    fasta_gz=fasta_path,\n",
    "    d_in=ESM_D_IN,\n",
    "    d_hidden=K,\n",
    "    tokens_per_step=TOKENS_PER_STEP,\n",
    "    max_steps=MAX_STEPS,\n",
    "    save_dir=SAVE_DIR,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 ('interplm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1106d1d489397abf5d77132595a521cf67d890f951d991cd34215b053d2a27e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
